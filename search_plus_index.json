{"./":{"url":"./","title":"打怪之路总览","keywords":"","body":"一、前言二、学习技能三、学习音视频理论知识1.重要知识点2.书籍推荐3.实践3.1.视音频数据处理入门3.2.完整的RTMP推送小项目四、学习过程的分析工具五、重点开发工具/组件1.FFmpeg1.1.学习途径1.2.学习路线2 OpenGL3.Webrtc六、实践项目七、最后音视频学习资料为了更好的阅读请前往GitBook 一、前言 随着时代的发展，各种短视频、直播、语音客服等等走进了各行各业！而未来5g/6g技术普及的时候，音视频要成为每个项目的标配！而本人在学习和工作过程中整理了学习笔记和各方面的学习资料。 二、学习技能 技能 重要度 作用 学习建议 c/c++ ★★★★☆ 音视频开源库基本都是用c/c++写的，如：FFmpeg库用C语言写的，Webrtc底层是用c++写的。 1. 看下方视频；2. 看书：c++ primer 第5版；3. 看FFmpeg源码；4. 进阶看侯杰视频。5. 最重要自己动手敲。 cmake ★★★☆☆ 跨平台引导编译的重要语言。在CMakeList.txt文件体现。 1. 看下方视频；2. CMake 入门实战 shell ★★☆☆☆ 很多开源库都是通过shell脚本进行编译的。如ffmpeg和x264中configure。 Shell脚本 Android NDK ★★☆☆☆ 在android平台上使用，需要掌握NDK的一些知识。如：交叉编译，JNI的接入。 看下方视频 IOS ★★☆☆☆ （略） （略） 三、学习音视频理论知识 1.重要知识点 知识点 重要度 作用 YUV ★★★★★ 视频原始（裸流）数据，解码最终显示就是一帧帧YUV数据 。 PCM ★★★★★ 音频原始（裸流）数据，解码最终播放的就是PCM数据。 H.264(AVC) ★★★★★ 目前主流的视频编解码协议。 H.265(HEVC) ★★★☆☆ 基于H.264的升级版，大幅度提升了编码大小和质量。因为版权和收费问题没有普及。 AAC ★★★★★ 目前主流的音频编解码协议。 RTMP ★★★☆☆ 直播推流，看侧重点 封装格式 ★★☆☆☆ MP4、AVI、MKV、RMVB、FLV等容器，把音频、视频、字幕等通道封装成一个文件 webrtc(VP8) ★★★☆☆ P2P的音视频通话，看侧重点 OpenGL ★★★☆☆ 使用GPU渲染视频，释放宝贵的CPU资源，看侧重点 2.书籍推荐 书：音视频开发进阶指南：基于Android与iOS平台的实践(京东) ：第1章　音视频基础概念；电子书往最后翻。 书：Android 音视频开发_何俊林(京东) ：第1章　音视频基础知识；电子书往最后翻。 书：新一代视频压缩编码标准-H.264/AVC(第二版)(京东) : 讲述H.264等编解码原理实现，其中几个算法 这篇文章讲的很深刻。 也可以在这里1 或者这里2 搜索。 这系列文章通俗易懂讲述编解码的一些知识 。 3.实践 3.1.视音频数据处理入门 [总结]视音频编解码技术零基础学习方法 系列文章，介绍了视音频编解码技术大体上原理和流程，通俗易懂。包括以下文章： 视音频数据处理入门：RGB、YUV像素数据处理 ：视频就是由它们组成的。 视音频数据处理入门：PCM音频采样数据处理 ：音频就是由它们组成的。 视音频数据处理入门：H.264视频码流解析 ：视频编码技术的一种（现代音视频开发必须掌握）。 视音频数据处理入门：AAC音频码流解析 ：音频编码技术的一种（现代音视频开发必须掌握）。 视音频数据处理入门：FLV封装格式解析 ：音视频封装格式的一种。 视音频数据处理入门：UDP-RTP协议解析 ：音视频协议的一种。 3.2.完整的RTMP推送小项目 直播推流全过程：总纲 直播推流全过程：视频数据源之YUV 直播推流全过程：音频数据源之PCM 直播推流全过程：视频编码之H.264 直播推流全过程：音频编码之AAC 直播推流全过程：直播推流编码之RTMP 其他：H.264符号描述 其他：直播优化基础 四、学习过程的分析工具 工具 作用 下载地址 VideoEye 来自雷神的强大实时视频码流分析软件。 地址 Codecian H264/H265等分析工具（跨平台）。 地址 H264Visa H.264/AVC实时视频分析工具。 （略） Hxd Hex Editor 16进制查看工具。 地址 ffprobe ffmpeg中自带的分析工具，非常强大，不过上手有难度。 参考 五、重点开发工具/组件 1.FFmpeg 音视频开发是绕不开FFmpeg的，因为它是一个\"集大成者\"，里面已经包含或可集成现代几乎所有的音视频技术（库）。 1.1.学习途径 阅读官方文档 学习官方例子（源码中doc/examples/xxx） [总结]FFMPEG视音频编解码零基础学习方法 书籍（电子书往最后翻） 1.FFmpeg从入门到精通(京东) 2.FFMPEG_FFPLAY源码剖析(CSDN) 3.音视频开发进阶指南：基于Android与iOS平台的实践(京东) 4.Android 音视频开发_何俊林(京东) 1.2.学习路线 这里不推荐直接学习雷神的 [总结]FFMPEG视音频编解码零基础学习方法，建议是通过在学习FFmpeg官方例子中进行学习，避免先入为主使用了过时的API。 1.2.1.源码编译 编译ffmpeg4.2.2通过这篇文章我们基本可以编译出我们想要的FFmpeg库 1.2.2.源码阅读 源码导入：FFmpeg导入到Clion（MacOS） 、 使用Clion阅读FFmpeg源码（支持跳转） 阅读参考：FFMPEG_FFPLAY源码剖析(CSDN) 、雷神的FFmpeg源代码系列 1.2.3.学习官方例子 FFmpeg重要结构体（转载） ，因为在学习FFmpeg中，必须得知道结构体中重要参数的含义，否则举步维艰。 FFmpeg Demuxing（解封装） 对应 doc/examples/demuxing_decoding.c 中的解封装部分。 FFmpeg Muxing（封装） 对应 doc/examples/muxing.c 。 FFmpeg Remuxing（重新封装） 对应 doc/examples/remuxing.c 。 FFmpeg Decode（解码） 对应 doc/examples/decode_audio.c 和 doc/examples/decode_video.c 。 FFmpeg Encode（编码） 对应 doc/examples/encode_audio.c 和 doc/examples/encode_video.c 。 FFmpeg 简单实现转码 汇总解封装、解码、编码、封装放到一起方便理解 。 FFmpeg Filter和SDL（Video） 对应 doc/examples/filtering_video.c 。 FFmpeg Filter和SDL（Audio） 对应 doc/examples/filtering_video.c 。 FFmpeg Transcode(转码) 对应 doc/examples/transcoding.c 。 FFmpeg Swscale（图像转换） 对应 doc/examples/scaling_video.c 。 1.2.4.音视频同步 FFmpeg 音视频同步处理 1.2.5.FFmpeg相关 FFmpeg 命令使用指南：分析ffmpeg、ffprobe、ffplay工具使用文档，关联官方文档，以及滤镜、协议、视频合并、各种播放参数等相关介绍 2 OpenGL OpenGL使用GPU渲染视频，释放宝贵的CPU资源，学习它是必不可少的。但是，正如雷神所说 ：作为一个搞视频技术的人研究OpenGL，需要耗费大量时间和精力，这样学习不是很经济。所以推荐只学习有关视频渲染相关知识。 OpenGL介绍，和相关程序库 纹理有关的基础知识 、OpenGL播放RGB/YUV 、OpenGL播放YUV420P（通过Texture，使用Shader） Android OpenGL ES官方文档 LearnOpenGL-CN OpenGL电子书下载 OpenGL基础知识 AFPlayer：OpenGL ES播放RGB 3.Webrtc 看下方视频，资料等待补充... 六、实践项目 AFPlayer项目 Android实现FFmpeg、OpenSL ES、OpenGL SE、MediaCodec等，实现简单的播放器，主要体现出相关知识点的使用。 七、最后 该博客持续更新中！ 若有帮助就Star一下呗，您的鼓励是我开源的动力！ 音视频学习资料 创作皆不易，有条件的朋友请支持原版，谢谢！ 密码:lqi9 网易视频 价值几千块的音视频视频 动脑视频 C++侯捷视频 音视频开发进阶指南：基于Android与iOS平台的实践.pdf 音视频05-H265码流分析.pdf 音视频04-H265之CU TU PU划分.pdf 音视频03-H265深度解析.pdf 音视频02-H265编码与H264区别.pdf 音视频01-H265编码.pdf 新一代视频压缩编码标准-H.264_AVC(第二版).pdf 数字信号处理教程（第四版）.pdf 视频技术手册(第5版).pdf 《FFmpeg从入门到精通》.pdf 《FFmpeg_Basics(260页)》.pdf webrtc介绍.pdf video_file_format_spec_v10_1.pdf STL源码剖析简体中文完整版(清晰扫描带目录).pdf SDL2-API手册.doc rtmp规范翻译1.0.docx rtmp_specification_1.0.pdf rtmp.part3.Commands-Messages.pdf rtmp.part2.Message-Formats.pdf rtmp.part1.Chunk-Stream.pdf jni基础介绍.pdf ISO_IEC_14496-14_2003-11-15.pdf ISO_IEC-14496-3-2009.pdf hls-mpeg-ts-VB_WhitePaper_TransportStreamVSProgramStream_rd2.pdf hls-mpeg-ts-iso13818-1.pdf H.264官方中文版.pdf H.264_MPEG-4-Part-10-White-Paper.pdf H.264-AVC-ISO_IEC_14496-15.pdf H.264-AVC-ISO_IEC_14496-10.pdf ffmpeg命令大全.pdf FFmpeg命令大全.docx FFMPEG_FFPLAY源码剖析.7z CMake中文手册.pdf Cmake在Android studio Ndk使用.pdf C++ Primer(第5版)中文版.pdf C++ Primer Plus（第6版）中文版.azw3 C Primer中文版 第五版 .pdf Android 音视频开发_何俊林.pdf amf3_spec_121207.pdf amf0_spec_121207.pdf Advanced C and C++ Compiling.pdf 【重点声明】此系列仅用于学习，禁止用于非法攻击，非法传播。一切遵守《网络安全法》。且如有发现商用，必纠法律责任。如有侵权请联系我（邮箱：xhunmon@126.com）进行删除。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-10 16:48:27 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/":{"url":"直播推流全过程/","title":"直播推流全过程","keywords":"","body":"总纲总纲 本系列介绍了RTMP直播推流全过程 完整的项目地址 以下文章是针对每一个情况，介绍音视频相关知识，以及实现的原理，总共分五章： 第一章：视频数据源之YUV（1） RGB或YUV 组成一张画面，很多个的画面就可以组成一个视频，而在视频编解码领域中YUV则是这一切的基础。 第二章：音频数据源之PCM（2） 音频处理就是对声音特性采集成数字信号后进行处理，而PCM则是最原始采集到的数据，称“裸流”。 第三章：视频编码之H.264（3） 为了减少视频大小，以及改善网络传输，H.264编码在网络传输中可是非常重要。 第四章：音频编码之AAC（4） AAC编码是音频公认的主流编码。 第五章：直播推流编码之RTMP（5） 结合RTMP分块的特性，把数据较大的视频数据进行分块传输，这必定是直播界的宠儿！ 其他：H.264符号描述 其他：直播优化基础 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/1-yuv.html":{"url":"直播推流全过程/1-yuv.html","title":"视频数据源之YUV","keywords":"","body":"视频数据源之YUV（1）视频基本概念YUV基本介绍YUV初步实战参考视频数据源之YUV（1） 视频基本概念 一个视频简单点理解就是播放一张张画面。我们就从这里面把视频的相关名词扯出来：①一张画面，一张画面也就一帧画面；属性为图像的大小或尺寸称分别率；画面的成像组成的方式有：rgb和yuv；跟计算器关联起来还不是用0101的比特来表示②当画面遇上了时间，爱的结晶就出来了：比特率、帧率和刷新率。接下来我们简单介绍一下他们的作用： 视频帧： 常见有I帧（关键帧，含完整画面，所以数据量大）、P帧（前向参考帧，参考前面I帧编码的图像信息）、B帧（双向预测帧，参考前面I帧、前面P帧和后面I帧编码的图像信息）；我们网上看视频时常常会遇到拖动进度条出现回退一两秒的情况吧？因为那个位置的当前帧不是I帧，没有完整的画面。 分辨率： 图像的大小或尺寸。 RGB： 任何彩色图像可由红绿蓝组成。RGB每一个通道占8位，1个字节内存。每个像素包含一个RGB，占3个字节（如果加上透明度RGBA则占32位，总共4个字节）。如：1920 x 1080内存大小=1920 x 1080 x 3=5.9M。 YUV(YCbCr)： Y：亮度；UV：色度和饱和度；wiki YCbCr 介绍。目前大多数都是使用yuv格式来表示视频帧的裸流数据。具体详情请往下阅读。 比特率(码率)： 单位时间内播放媒体（包括视频和音频）的比特数量（bit的数量）。文件大小计算公式： 文件大小（b）= 码率（b/s）x 时间（s） 帧率(帧数)： 画面每秒传输帧数，单位：fps（frame per second）或者 “赫兹”（Hz）。对于人眼感官常用范围在15~75fps之间。 刷新率： 屏幕在每秒刷新（画面）的次数。单位：赫兹（Hz）。 YUV基本介绍 人类视觉系统（HVS）对亮度比彩色更敏感，所以把Y和UV单独抽出来，每一个像素点都有一个Y，4个Y和一组UV（UV数量不定）共同绘制4个像素点，而Y和UV的比例不一样就分了多种 取样格式。 YUV与RGB的相互转化 RGB转YUV： Y = 0.299R + 0.587G + 0.114B U = 0.564(B - Y) V = 0.713(R - Y) YUV转RGB： R = Y + 1.402V B = Y + 1.772U G = Y - 0.344U - 0.714V YUV取样格式 常见的取样格式有以下3种 4：4：4 每4个素位置都有4个YUV，内存计算：1920 x 1080 = 1920 x 1080 x 3=5.9M； 4：2：2 每4个Y像素具有2个U和2个V；内存计算：1920 x 1080 = 1920 x 1080 x (1 + 2/4 + 2/4)=3.9M； 4：2：0 每4个Y像素具有1个U和1个V，使用在视频领域中应用最广泛。内存计算：1920 x 1080 = 1920 x 1080 x (1 + 1/4 + 1/4)=2.8M； YUV数据排列格式 在这里介绍两种格式，一种是Android平台特有NV21（又称YUV420SP），另一种则是其他大部分平台同样的I420（又称YUV420P），这两个取样格式都是4:2:0，所以说他们两种的数据完全一样，只是放到内存里面的顺序变了。（我们要实现把从Android采样NV21数据转成I420然后推送到服务器。） NV21： ①先把Y数据全部排序完；②UV数据交替排序完； I420： ①先把Y数据全部排序完；②U数据排序完；③V数据排序完； YUV初步实战 手写分离YUV分量以及对其进行播放 请前往 视音频数据处理入门：RGB、YUV像素数据处理 进行学习。对“前人种树，后人乘凉”感触颇深，只是太可惜了！ YUV数据源的采集（Android端） //1.相机权限 //2.获取相机，有后置摄像头：Camera.CameraInfo.CAMERA_FACING_BACK和前置摄像头：Camera.CameraInfo.CAMERA_FACING_FRONT Camera camera = Camera.open(cameraId); //3、Parameters这里封装着当前摄像头所能提供的参数（真实宽高等） Camera.Parameters parameters = camera.getParameters(); //根据parameters.getSupportedPreviewSizes()提供的宽高尺寸挑选一个设置进去 parameters.setPreviewSize(width, height); //设置预览数据为nv21（注意：仅仅是预览的数据，通过onPreviewFrame回调的仍没有发生变化） parameters.setPreviewFormat(ImageFormat.NV21); //设置预览角度，通过WindowManager.getDefaultDisplay().getRotation()参数查看。（因为android手机厂商安装摄像头传感器方向不统一，所以数据可能是旋转过的，所以要回正） camera.setDisplayOrientation(degrees); //设置修改过的数据，使得生效 camera.setParameters(parameters); //4、设置数据监听，我们会在onPreviewFrame(byte[] data, Camera camera)处理回调的数据，这里的数据就是每一帧原始数据流。我们会先把数据按照角度回正（注意回正后的宽高可能是调换的），然后转成I420就行编码发送。 camera.setPreviewCallbackWithBuffer(this); //5、启动预览画面 camera.setPreviewDisplay(holder); camera.startPreview(); NV21数据旋转 比如NV21数据以及顺时针旋转90度后的对比： 实现顺时针和逆时针旋转90度的代码： /** *yuv_n21_rotation(\"assets/yuv_nv21_800x480_back.yuv\",800,480,90,\"output/out_nv21_480x800_back.yuv\"); * 从android摄像机获取到的nv21格式数据，进行旋转 */ int yuv_n21_rotation(const char *url_in, int width, int height, int rotation, const char *url_out) { FILE *pIn = fopen(url_in, \"rb+\"); FILE *pOut = fopen(url_out, \"wb+\"); int yuvSize = width * height * 3 / 2; unsigned char *simple = (unsigned char *) malloc(yuvSize); unsigned char *simpleOut = (unsigned char *) malloc(yuvSize); fread(simple, 1, yuvSize, pIn); //顺时针旋转90 if (rotation == 90) { //宽高取反，把竖变行 int k = 0; //宽高取反，把竖变行 //y数据 for (int w = 0; w = 0; h--) { simpleOut[k++] = simple[h * width + w]; } } //uv数据 height*width -> 3/2height*width for (int w = 0; w = 0; h--) { // *(simpleOut + k) = simple[width * height + h * width + w]; // u simpleOut[k++] = simple[width*height + width * h + w]; // v simpleOut[k++] = simple[width*height + width * h + w + 1]; } } } else if(rotation == -90){ //宽高取反，把竖变行 int k = 0; //宽高取反，把竖变行 //y数据 for (int w = width -1; w >= 0; w--) { for (int h = 0; h 3/2height*width for (int w = 0; w = 0; h--) { // *(simpleOut + k) = simple[width * height + h * width + w]; simpleOut[k++] = simple[width*height + width * h + w]; simpleOut[k++] = simple[width*height + width * h + w + 1]; } } } fwrite(simpleOut, 1, yuvSize, pOut); return 0; } NV21数据格式转I420数据格式 //nv21_to_i420(\"assets/yuv_nv21_800x480_back.yuv\",800,480,\"output/out_yuv_i420_800x480_back.yuv\"); int nv21_to_i420(const char *url_in, int width, int height, const char *url_out){ FILE *pIn = fopen(url_in, \"rb+\"); FILE *pOut = fopen(url_out, \"wb+\"); int ySize = width * height; int uvSize = ySize /2; int yuvSize = ySize * 3 / 2; unsigned char *simple = (unsigned char *) malloc(yuvSize); unsigned char *simpleOut = (unsigned char *) malloc(yuvSize); fread(simple,yuvSize,1,pIn); //y memcpy(simpleOut, simple, ySize); for (int i = 0; i 参考 wiki YCbCr 视音频数据处理入门：RGB、YUV像素数据处理 新一代视频压缩编码标准-H.264_AVC(第二版) （书） 音视频开发进阶指南：基于Android与iOS平台的实践（书） Android 音视频开发_何俊林（书） Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/2-pcm.html":{"url":"直播推流全过程/2-pcm.html","title":"音频数据源之PCM","keywords":"","body":"音频数据源之PCM（2）声音与音频音频采集与关键名词PCM数据的基本使用Android终端音频采样介绍播放pcm原始数据参考音频数据源之PCM（2） 声音与音频 声音是波，成为声波，而声波的三要素是频率、振幅和波形。频率代表音阶的高低（女高音、男低音）单位赫兹（Hz），人耳能听到的声波范围：频率在20Hz~20kHz之间；振幅代表响度（音量）；波形代表音色。而我们音频处理就是对声波采集成数字信号后进行处理。 音频采集与关键名词 音频采集的过程主要是通过设备设置采样率、采样数，将音频信号采集为pcm（Pulse-code modulation，脉冲编码调制）编码的原始数据（无损压缩），然后编码压缩成mp3、aac等封装格式的数据。音频关键知识： 采样率： 一段音频数据中单位时间内（每秒）采样的个数。 位宽： 一次最大能传递数据的宽度，可以理解成放单个采集数据的内存。常有8位和16位，而8位：代表着每个采集点的数据都使用8位（1字节）来存储；16位：代表着每个采集点的数据都使用16位（2字节）来存储。 声道数： 扬声器的个数，单声道、双声道等。每一个声道都占一个位宽。 来一张图来描述一下： 一段时间内的数据大小如何计算？ 采样率 x (位宽 / 8) x 声道数 x 时间 = 数据大小（单位：字节） 比如 2分钟的CD（采样率为：44100，位宽：16，声道数：2）的数据大小：44100 x (16 / 8) x 2 x 120 = 20671.875 Byte 约为 20.18M。 PCM数据的基本使用 我们采集到的pcm原始数据要怎么玩？首先得知道怎么这些数据都代表啥意思，然后才能入手处理。 1、pcm数据时如何组成（存储）？ 举个例子，分别使用不同的方式存储一段采集数据 0x11 0x22 0x33 0x44 0x55 0x66 0x77 0x88 总共8个字节。 8位单声道： 按照数据采集时间顺序存储，即：0x11 0x22 0x33 0x44 0x55 0x66 0x77 0x88 8位双声道： L声道-R声道-L声道-R声道形式存储，即：0x11(L) 0x22(R) 0x33(L) 0x44(R) …… 16位单声道： 首先从 维基多媒体：pcm了解到位宽大于8位时，字节的排序方式是有差别的，描述如下： When more than one byte is used to represent a PCM sample, the byte order (big endian vs. little endian) must be known. Due to the widespread use of little-endian Intel CPUs, little-endian PCM tends to be the most common byte orientation. 当使用一个以上的字节表示PCM样本时，必须知道字节顺序（大端与小端）。由于低端字节Intel CPU的广泛使用，低端字节PCM往往是最常见的字节方向。 举个栗子：当位宽为16位（2字节）存储一个采集数据时，如：0x12ab，大端和小端分别是： big-endian: 0x12 0xab； little-endian: 0xab 0x12。 所以： big-endian存储方式：0x1122 0x3344 0x5566 0x7788； little-endian存储方式：0x2211 0x4433 0x6655 0x8877。 16位双声道： L声道-R声道-L声道-R声道形式存储： big-endian：0x1122(L) 0x3344(R) 0x5566(L) 0x7788(R) little-endian: 0x2211(L) 0x4433(R) 0x66550(L) 0x8877(R) 2、pcm原始数据可以怎么玩？ 将little-endian_2_44100_16.pcm采样数据进行切割，只保留后面5秒的数据 /** * 将little-endian_2_44100_16.pcm采样数据进行切割，只保留后面5秒的数据 * 1、该类型数据5秒有多长？ * 2、从哪里开始截取？ */ int cut5second(const char *url){ FILE *in = fopen(url, \"rb+\"); FILE *out = fopen(\"./output/spit5second.pcm\", \"wb+\"); long long data5Length = 44100 * (16/8) * 2 * 5; struct stat statbuf; stat(url,&statbuf); long long fileLength = statbuf.st_size; long long start = fileLength - data5Length; char *simple = (char *)malloc(data5Length); //把指针位置移动到start位置开始读取 fseek(in,start,1); //每次从in文件中读取1组data5Length个长度数据的到simple中 fread(simple,data5Length,1,in); fwrite(simple,data5Length,1,out); fclose(in); fclose(out); return 0; } 分离各声道的数据：把各个声道的采集点数据分开存储。 /** * 将little-endian_2_44100_16.pcm 分离各声道的数据，即把各个声道的采集点数据分开存储。 */ int separateLR(const char *url){ FILE *in = fopen(url, \"rb+\"); FILE *outL = fopen(\"./output/l.pcm\", \"wb+\"); FILE *outR = fopen(\"./output/r.pcm\", \"wb+\"); int simpleLength = 16 / 8 * 2; char *simple = (char *)malloc(simpleLength); while (1){ //每次从in文件中读取1组4个长度数据的到simple中 fread(simple,4,1,in); if(feof(in)){ break; } //l(0声道)：1-2 fwrite(simple,2,1,outL); //r(1声道)：3-4 fwrite(simple+2,2,1,outR); } fclose(in); fclose(outL); fclose(outR); return 0; } 音量调节：把每个采集点数据的值 x 调节比例。注意：需要注意的是可调节范围，如：8位有无符号时最大是多少。 /** * 将little-endian_2_44100_16.pcm 调节音量 比例 * 1、little-endian排序的值是如何排序的？真正的值是多少？ * 2、little-endian转成真正的值之后再进行计算，得到的结果再反转little-endian。 * 如：原始pcm数据：0xaa 0x01(左声道采样点数据)，当scale=2： * -> 值：0x01aa * 2 = 0x0354 * -> 转回little-endian再进行存储：0x5403（缩放后的值） */ int volumeAdjustment(const char *url, float scale){ FILE *in = fopen(url, \"rb+\"); FILE *out = fopen(\"./output/volume_adjustment.pcm\", \"wb+\"); char *simple = (char *)malloc(4); while (1){ //每次从in文件中读取1组4个长度数据的到simple中 fread(simple, 4, 1, in); if(feof(in)){ break; } short *simple8bitTemp = (short *)malloc(2); //l(0声道)： simple8bitTemp[0] = (simple[0] + (simple[1] > 8; simple[2] = simple8bitTemp[1] & 0x00FF; simple[3] = simple8bitTemp[1] >> 8; for(int i=0; i 播放速度：按照比例丢弃（或插入0）采集点的数据即可。（涉及到不是整数倍不单单是这么处理，我也不懂） 参照雷神的必看项目： 视音频数据处理入门：PCM音频采样数据处理 。 Android终端音频采样介绍 1、关于采集的主要api介绍 /** * @param audioSource 音频来源{@link MediaRecorder.AudioSource}；如指定麦克风：MediaRecorder.AudioSource.MIC * @param sampleRateInHz 采样率{@link AudioFormat#SAMPLE_RATE_UNSPECIFIED}，单位Hz；安卓支持所有的设备是：44100Hz * @param channelConfig 声道数{@link AudioFormat#CHANNEL_IN_MONO}； * @param audioFormat 位宽{@link AudioFormat#ENCODING_PCM_8BIT} * @param bufferSizeInBytes 采集期间缓存区的大小 */ public AudioRecord(int audioSource, int sampleRateInHz, int channelConfig, int audioFormat, int bufferSizeInBytes) //获取最小缓存区，参数跟AudioRecord保持一致 int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat) {} 2、实现采集的伪代码 //1、申请权限 //2、获取最小缓存大小（根据api介绍，应该取要比预期大的缓冲区大小），这个大小其实也可以取①和②计算得来的大小 int minBufferSize = AudioRecord.getMinBufferSize(44100, AudioFormat.CHANNEL_IN_STEREO, AudioFormat.ENCODING_PCM_16BIT) * 2; //3、初始化AudioRecord对象 AudioRecord audioRecord = new AudioRecord(MediaRecorder.AudioSource.MIC, 44100, AudioFormat.CHANNEL_IN_STEREO, AudioFormat.ENCODING_PCM_16BIT, minBufferSize); //4、开始在子线程中进行采集数据 new Thread(new Runnable() { @Override public void run() { audioRecord.startRecording(); while(isRunning){ byte[] bytes = new byte[minBufferSize]; int len = audioRecord.read(bytes, 0, bytes.length); //这里就可以把数据直接写入到sdcard了，如：xxx.pcm；输出排序方式为：little endian。 } //5、停止录音机 audioRecord.stop(); } }).start(); //6、最后释放资源 audioRecord.release(); 播放pcm原始数据 ffmpeg： ffplay -f s16le -sample_rate 44100 -channels 2 -i xxx.pcm 其他： Adobe Audition 参考 维基多媒体：pcm 视音频数据处理入门：PCM音频采样数据处理 Android 音视频开发_何俊林（书） Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/3-h264.html":{"url":"直播推流全过程/3-h264.html","title":"视频编码之H.264","keywords":"","body":"视频编码之H.264（3）简单说说编码H.264编码SPS（序列参数集）PPS（图像参数集）其他常见名称缩写X264参考视频编码之H.264（3） 简单说说编码 当我们把摄像头采集画面直接写入到文件中时，我们会发现没一会文件已经非常大了。这导致很不适合保存和传输，所以需要编码，把画面数据进行压缩。视频编码标准有很多，而我们这里讲的是H.264编码。其他请看：视频编码标准汇总及比较 。 H.264编码 制订H.264的主要目标有两个： （1）视频编码层(VCL，全称：Video Coding Layer)：得到高的视频压缩比。 （2）网络提取层(NAL，全称：Network Abstraction Layer)：具有良好的网络亲和性，即可适用于各种传输网络。而NAL则是以NALU（NAL Unit）为单元来支持编码数据在基于包交换技术网络中传输的。 H.264编码器 上面是H264的编码器原理图，编码时进行帧内编码和帧间编码。用相机录像举例，当相机录像过程帧出现一帧帧画面，没有压缩之间都是可以单独作为一张完整的照片，经过H264编码器后会出现： 1.SPS、PPS：生成图片的一些参数信息，比如宽高等。一般是开启编码器后的第一次I帧前输出一次。 2.I帧：编码后第一个完整帧，包含一张图片的完整信息，但是也是压缩的。主要是进行帧内压缩，把一帧图片划分为很多宏块，如：16x16、8x16、8x8、4x4等等，记录宏块顶部和左侧像素点数据，以及预测的方向。 3.B帧：叫双向预测帧。编码器遇到下一帧画面与前面帧变化非常非常小（相似度在95%之内），比如在录像中的人在发呆，当遇到变化略微有点大的P帧才停止，这会出现多张B帧，为了尽可能的存储更少的信息，将参考前面的I和后面的P帧，把中间变化的数据存储下来，这样编码记录运动矢量和残差数据即可。所以编码器当遇到这些帧时，先等待P帧编码结束后才进行编码B帧，因此输出顺序在P帧之后。 4.P帧：有了I帧作为基础，P就有参考的对象。跟前面I帧的变化非常少（相似度70%之内）。 5.GOF：按上面说录像人在发呆场景时，将一组数据相差较小的图片进行编码，这就有了GOF，即：group of picture，即一组图片，也叫一个场景的一组图片。是从一个I帧开始到下一个I帧前的一组数据。接着编码就是这样的一个个GOF的轮回。 如果不追求其中的细节，把他看成一个\"黑盒\"的话，编码成了： 解码成了： 编解码器工作具体细节我们可以不用深究，但是输入和输出的结果一定要知道，因为我们根据这个可以做很多事情，比如： 直播是过多的输出B帧会有什么影响？会出现延迟！！！ 编码的输入与输出 一张张画面通过以H.264编码标准的编码器(如x264)编码后，输出一段包含N个NALU的数据，每个NALU之间通过起始码来分隔，如图： 起始码： 0x00 00 01 或者 0x00 00 00 01。 在网络传输（如RTMP）或者一些容器中（如FLV），通常会把NALU整合到视频区域的数据中。如下图的flv格式： 所以这篇文章主要学习NALU的基本知识，学会如何去分析一段NALU数据。 NALU（NAL 单元） NALU(NAL Unit，NAL 单元)的组成部分如下图。其中，f(0)占1bit，u(2)占2bit，u(5)占5bit，文中如有出现类似描述符请看H.264描述符 。 forbidden_zero_bit：禁止位，初始为0。当客户端接受到该位为1时，则会丢弃该NAL单元的解码，表示异常数据。 nal_ref_idc：优先级，越大优先级越高。比如：I帧优先级大于B帧，DPS芯片会优先解码重要性高的。 从上图可以看出来，当前NAL单元属于什么样的类型，这取决于RBSP具体是什么样的类型，而RBSP的类型是根据nal_unit_type的值来定义的。 ①当nal_unit_type为1~5时：RBSP为切片类型（有5种切片类型）；整个NAL单元类型为VCL NAL单元，VCL是上面说的视频编码层，里面有编码后画面数据。 ②当nal_unit_type为其他时：RBSP为序列参数集类型、图像参数集类型等等；整个NAL单元类型为非VCL NAL单元。 具体的nal_unit_type所对应的RBSP类型如下表所示： nal_unit_type NAL 单元和 RBSP 语法结构的内容 0 未指定 1 一个非IDR图像的编码条带slice_layer_without_partitioning_rbsp( ) 2 编码条带数据分割块Aslice_data_partition_a_layer_rbsp( ) 3 编码条带数据分割块Bslice_data_partition_b_layer_rbsp( ) 4 编码条带数据分割块Cslice_data_partition_c_layer_rbsp( ) 5 IDR图像的编码条带slice_layer_without_partitioning_rbsp( ) 6 辅助增强信息 (SEI)sei_rbsp( ) 7 序列参数集（SPS）seq_parameter_set_rbsp( ) 8 图像参数集(PPS)pic_parameter_set_rbsp( ) 9 访问单元分隔符access_unit_delimiter_rbsp( ) 10 序列结尾end_of_seq_rbsp( ) 11 流结尾end_of_stream_rbsp( ) 12 填充数据filler_data_rbsp( ) 13 序列参数集扩展seq_parameter_set_extension_rbsp( ) 14..18 保留 19 未分割的辅助编码图像的编码条带slice_layer_without_partitioning_rbsp( ) 20..23 保留 24..31 未指定 SPS（序列参数集） SPS全称 Sequence parameter set(序列参数集)，当nal_unit_type=7时，RBSP就是SPS类型，也可以说NAL单元为SPS的NAL单元。SPS主要包含的是针对一连续编码视频序列的参数，如帧数、图像尺寸等；详见下表 序列参数集RBSP 语法： 上面中的主要参数的含义： profile_idc profile_idc 档次（H.264编码标准有几个档次，等级越高视频越清晰）： 值（十进制） 表示的意义 66 Baseline（直播） 77 Main（一般场景） 88 Extended 100 High (FRExt) 110 High 10 (FRExt) 122 High 4:2:2 (FRExt) 144 High 4:4:4 (FRExt) level_idc level_idc 最大支持码流范围： 标识当前码流的Level。编码的Level定义了某种条件下的最大视频分辨率、最大视频帧率等参数，码流所遵从的level由level_idc指定。 值(十进制) 表示最大支持每秒码流大小 10 1 (supports only QCIF format and below with 380160 samples/sec) 11 1.1 (CIF and below. 768000 samples/sec) 12 1.2 (CIF and below. 1536000 samples/sec) 13 1.3 (CIF and below. 3041280 samples/sec) 20 2 (CIF and below. 3041280 samples/sec) 21 2.1 (Supports HHR formats. Enables Interlace support. 5 068 800 samples/sec) 22 2.2 (Supports SD/4CIF formats. Enables Interlace support. 5184000 samples/sec) 30 3 (Supports SD/4CIF formats. Enables Interlace support. 10368000 samples/sec) 31 3.1 (Supports 720p HD format. Enables Interlace support. 27648000 samples/sec) 32 3.2 (Supports SXGA format. Enables Interlace support. 55296000 samples/sec) 40 4 (Supports 2Kx1K format. Enables Interlace support. 62914560 samples/sec) 41 4.1 (Supports 2Kx1K format. Enables Interlace support. 62914560 samples/sec) 42 4.2 (Supports 2Kx1K format. Frame coding only. 125829120 samples/sec) 50 5 (Supports 3672x1536 format. Frame coding only. 150994944 samples/sec) 51 5.1 (Supports 4096x2304 format. Frame coding only. 251658240 samples/sec) seq_parameter_set_id seq_parameter_set_id 标识符，本序列的id号，会被PPS引用。 chroma_format_idc chroma_format_idc 与亮度取样对应的色度取样 chroma_format_idc 的值应该在 0到 3的范围内（包括 0和 3）。当 chroma_format_idc不存在时，应推断其值为 1（4：2：0的色度格式）。 色度采样结构 chroma_format_idc 色彩格式 SubWidthC SubHeightC 0 单色 - - 1 4:2:0 2 2 2 4:2:2 2 1 3 4:4:4 1 1 num_ref_frames 规定了可能在视频序列中任何图像帧间预测的解码过程中用到的短期参考帧和长期参考 帧、互补参考场对以及不成对的参考场的最大数量。num_ref_frames 字段也决定了 8.2.5.3 节规定的滑动窗口操作 的大小。num_ref_frames 的取值范围应该在 0 到 MaxDpbSize （参见 A.3.1 或 A.3.2 节的定义）范围内，包括 0 和 MaxDpbSize 。 pic_width_in_mbs_minus1 pic_width_in_mbs_minus1 加1是指以宏块为单元的每个解码图像的宽度。 本句法元素加 1 后指明图像宽度，以宏块为单位：PicWidthInMbs = pic_width_in_mbs_minus1 + 1 通过这个句法元素解码器可以计算得到亮度分量以像素为单位的图像宽度： PicWidthInSamplesL = PicWidthInMbs * 16 从而也可以得到色度分量以像素为单位的图像宽度： PicWidthInSamplesC = PicWidthInMbs * 8 以上变量 PicWidthInSamplesL、 PicWidthInSamplesC 分别表示图像的亮度、色度分量以像素为单位的宽。 H.264 将图像的大小在序列参数集中定义，意味着可以在通信过程中随着序列参数集动态地改变图像的大小，甚至可以将传送的图像剪裁后输出。 frame_width = 16 × (pic_width_in_mbs_minus1 + 1); pic_height_in_map_units_minus1 pic_height_in_map_units_minus1 加1表示以条带组映射为单位的一个解码帧或场的高度。 本句法元素加 1 后指明图像高度： PicHeightInMapUnits = pic_height_in_map_units_minus1 + 1 PicSizeInMapUnits = PicWidthInMbs * PicHeightInMapUnits 图像的高度的计算要比宽度的计算复杂，因为一个图像可以是帧也可以是场，从这个句法元素可以在帧模式和场模式下分别计算出出亮度、色度的高。值得注意的是，这里以 map_unit 为单位， map_unit的含义由后文叙述。 PicHeightInMapUnits = pic_height_in_map_units_minus1 + 1; frame_height = 16 × (pic_height_in_map_units_minus1 + 1); 手撕SPS 下面为从一个只放h.264视频编码文件的一段（SPS）： ue(v)和se(v)的计算公式见 H.264描述符 。 # 00000001 6764001E ACD940A0 2FF96100 00030001 00000300 320F162D 96 00000001 #起始码 #NAL单元头---0x67 0110 0111 -------------- 0... .... # forbidden_zero_bit -->u(1) .11. .... # nal_ref_idc -->u(2) -->HIGHEST ...0 0111 # nal_unit_type -->u(5) -->SPS 64 # profile_idc=103 -->u(8) #---0x00 0000 0000 -------------- 0... .... #constraint_set0_flag .0.. .... #constraint_set1_flag ..0. .... #constraint_set2_flag ...0 .... #constraint_set3_flag .... 0000 #reserved_zero_4bits 1E # level_idc -->u(8) --> 30 #-----------0xAC 1010 1100 -------------- 1... .... # seq_parameter_set_id --> ue(v) --> 0 .010 .... # log2_max_frame_num_minus4 --> ue(v) --> 1 .... 1... # pic_order_cnt_type --> ue(v) -->1 执行else if( pic_order_cnt_type == 1 ) .... .1..#delta_pic_order_always_zero_flag -->u(1) #----------0xACD9 (1010 11)00 1101 1001 ====== 括号里面的bit上面已使用 .... ..00 110. .... #offset_for_non_ref_pic -->se(v)->codeNum=5->value=3 .... .... ...1 .... #offset_for_top_to_bottom_field -->se(v)->codeNum=0->value=0 .... .... .... 1... #num_ref_frames_in_pic_order_cnt_cycle -->ue(v)->0 #----------0xD940 (1101 1)001 0100 0000 ====== 括号里面的bit上面已使用 .... .001 01.. .... #num_ref_frames -->ue(v)->4 .... .... ..0. .... #gaps_in_frame_num_value_allowed_flag -->u(1)->0 #----------0x40A0 (010)0 0000 1010 0000 ====== 括号里面的bit上面已使用 ...0 0000 1010 00.. #pic_width_in_mbs_minus1 -->ue(v)->32-1+8=39 --> 视频宽 = (宏块 + 1) X 16 = (39 + 1)X16 = 640 #----------0xA02F (1010 00)00 0010 1111 ====== 括号里面的bit上面已使用 .... ..00 0010 111. #pic_height_in_map_units_minus1 -->ue(v)->16-1+7=22--> 视频高 = (宏块 + 1) X 16 = (22 + 1)X16 = 368 …………就到这里了，偷个懒，有兴趣大家自己分析下去，哈哈 PPS（图像参数集） PPS全称picture parameter set(图像参数集)，当nal_unit_type=8时，RBSP就是PPS类型，也可以说NAL单元为SPS的NAL单元。一个序列中某一幅图像或者某几幅图像，其参数如标识符pic_parameter_set_id、可选的seq_parameter_set_id、熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系数调整标识等；详见下表 图像参数集RBSP 语法： 其他常见名称缩写 名词缩写 中文含义 CABAC 基于上下文的自适应二进制算术编码 CAVLC 基于上下文的自适应变长编码 CBR 恒定比特率 CPB 编码图像缓存区 DPB 解码图像缓存区 DUT 被测解码器 FIFO 先进先出 HRD 假想参考解码器 HSS 假想码流调度器 IDR 即时解码刷新（I帧） LSB 最低有效位 MB 宏块 MBAFF 宏块自适应帧－场编码 MSB 最高有效位 NAL 网络抽象层 RBSP 原始字节序列载荷 SEI 补充的增强信息 SODB 数据比特串 UUID 通用唯一性标识符 VBR 可变比特率 VCL 视频编码层 VLC 变长编码 X264 这是国际好评的H.264协议标准的编码工具，这里简单介绍一下如何使用。 （1）下载：https://www.videolan.org/developers/x264.html （2）编译（android的交叉编译，平台：Mac） #!/bin/sh ##########脚本忘记是参考哪位大神的了########## #ndk的路径 NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c API=17 #最低支持Android版本 #编译平台darwin-x86_64为mac，linux-x86_64为linux HOST_PLATFORM=darwin-x86_64 function build_x264 { OUTPUT=$(pwd)/\"android\"/\"$CPU\" ./configure \\ --prefix=$OUTPUT \\ --cross-prefix=$CROSS_PREFIX \\ --sysroot=$SYSROOT \\ --host=$HOST \\ --disable-asm \\ --disable-shared \\ --enable-static \\ --disable-opencl \\ --enable-pic \\ --disable-cli \\ --extra-cflags=\"$EXTRA_CFLAGS\" \\ --extra-ldflags=\"$EXTRA_LDFLAGS\" make clean make install echo \"编译结束，路径如下：\" echo \"$OUTPUT\" } CPU=\"armeabi-v7a\" CROSS_PREFIX=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi- SYSROOT=$NDK/platforms/android-$API/arch-arm/ EXTRA_CFLAGS=\"-D__ANDROID_API__=$API -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" EXTRA_LDFLAGS=\"-marm\" HOST=arm-linux build_x264 （3）把编译生成的静态库移入android studio 在CMakeList.txt文件中添加： ... include_directories(include) ... target_link_libraries( ... x264 ) （4）API简单使用 /** 关键步骤，来自雷神：https://blog.csdn.net/leixiaohua1020/article/details/42078645 x264_param_default()：设置参数集结构体x264_param_t的缺省值。 x264_picture_alloc()：为图像结构体x264_picture_t分配内存。 x264_encoder_open()：打开编码器。 x264_encoder_encode()：编码一帧图像。 x264_encoder_close()：关闭编码器。 x264_picture_clean()：释放x264_picture_alloc()申请的资源。 存储数据的结构体如下所示。 x264_picture_t：存储压缩编码前的像素数据。 x264_nal_t：存储压缩编码后的码流数据。 */ int X264Rtmp::encode(const char *url, int width, int height, int bitRate, int fps) { //计算一帧等信息 int ySize = width * height; int uvSize = ySize / 4; FILE *fp_src = fopen(url, \"rb\"); //初始化VLC图片编码层的参数 x264_picture_t *pic_in = (x264_picture_t *) malloc(sizeof(x264_picture_t)); x264_t *videoCodec = 0; x264_param_t param; x264_param_default(&param); //根据应用场景设置编码速度，以及编码质量。2：x264_preset_names，3：x264_tune_names x264_param_default_preset(&param, x264_preset_names[0], x264_tune_names[7]); //输入数据格式， yuv 4:2:0 param.i_csp = X264_CSP_I420; param.i_width = width; param.i_height = height; //base_line 3.2 编码规格，影响网络带宽，图像分辨率等。 -- https://en.wikipedia.org/wiki/Advanced_Video_Coding param.i_level_idc = 32; //两张参考图片间b帧的数量 param.i_bframe = 0; //参数i_rc_method表示码率控制，CQP(恒定质量)，CRF(恒定码率)，ABR(平均码率) param.rc.i_rc_method = X264_RC_ABR; //比特率(码率, 单位Kbps) param.rc.i_bitrate = bitRate / 1000; //瞬时最大码率 param.rc.i_vbv_max_bitrate = bitRate / 1000 * 1.2; //设置了i_vbv_max_bitrate必须设置此参数，码率控制区大小,单位kbps param.rc.i_vbv_buffer_size = bitRate / 1000; //帧率（每秒显示多少张画面） param.i_fps_num = fps; param.i_fps_den = 1; param.i_timebase_den = param.i_fps_num; param.i_timebase_num = param.i_fps_den; // param.pf_log = x264_log_default2; //用fps而不是时间戳来计算帧间距离 param.b_vfr_input = 0; //帧距离(关键帧) 2s一个关键帧 param.i_keyint_max = fps * 2; // 是否复制sps和pps放在每个关键帧的前面 该参数设置是让每个关键帧(I帧)都附带sps/pps。 param.b_repeat_headers = 1; //多线程 param.i_threads = 1; x264_param_apply_profile(&param, \"baseline\"); //打开编码器 videoCodec = x264_encoder_open(&param); x264_picture_alloc(pic_in, X264_CSP_I420, width, height); //编码：h264码流 while (!feof(fp_src)) { //y数据 fread(pic_in->img.plane[0], ySize, 1, fp_src); //Y fread(pic_in->img.plane[1], uvSize, 1, fp_src); //U fread(pic_in->img.plane[2], uvSize, 1, fp_src); //V //编码出来的数据 （帧数据） x264_nal_t *pp_nal; //编码出来有几个数据 （多少帧） int pi_nal; x264_picture_t pic_out; x264_encoder_encode(videoCodec, &pp_nal, &pi_nal, pic_in, &pic_out); //如果是关键帧 3 int sps_len; int pps_len; uint8_t sps[100]; uint8_t pps[100]; // chroma_format_idc for (int i = 0; i 参考 视频编码标准汇总及比较 H.264-AVC-ISO_IEC_14496-10 新一代视频压缩编码标准-H.264_AVC(第二版) H.264官方中文版.pdf https://stackoverflow.com/questions/28421375/usage-of-start-code-for-h264-video/29103276 https://blog.csdn.net/engineer_james/article/details/81750864 x264流程 x264参数注释 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-10 16:09:31 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/4-aac.html":{"url":"直播推流全过程/4-aac.html","title":"音频编码之AAC","keywords":"","body":"音频编码之AAC（4）回顾简述AAC的音频文件格式faac开源库参考音频编码之AAC（4） 回顾 还记得我们采集到的PCM原始数据流（俗称裸流）吗？由于PCM裸流过大，不便于储存与传输，于是就出现了针对于PCM裸流的压缩编码标准，包含AAC，MP3，AC-3 等等（wiki audio file format）；而AAC则是当前的主流。这里的AAC指的是一套编码标准（协议），而faac是一个开源的AAC编解码工具。 简述 AAC：高级音频编码(Advanced Audio Coding)，基于MPEG-2的音频编码技术，目的是取代MP3格式。2000年，MPEG-4标准出现后，AAC重新集成了其特性，为了区别于传统的MPEG-2 AAC又称为MPEG-4 AAC。 AAC的音频文件格式 AAC的音频文件格式有两种ADIF和ADTS。这两种格式主要区别：ADIF只有一个文件头，ADTS每个包前面有一个文件头。而我们重点讲解的是ADTS格式。 ADIF Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不需进行在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。故这种格式常用在磁盘文件中。编码格式如下： 在MPEG-2 AAC中ADIF语法规则如下： ADTS Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始。也就是说ADTS的每一帧都有一个header和aac音频数据，这可以在网络传输的时候进行实时解码。 下图为ADTS的组成部分以及在MPEG-2 AAC的语法结构： 下图为ADTS的组成部分以及在MPEG-4 AAC的语法结构： 注：ES：全称elementary stream，这里意为编码后的音频数据。 adts_fixed_header关键参数如下： syncword 恒为 '1111 1111 1111'，也就是0xFFF。作为每个adts_freme的分割。 ID 使用那个MPEG版本。0：MPEG-4，1：MPEG-2。 layer 应该恒 为‘00’。 protection_absent 是否使用error_check()。0：使用，1：不使用。 profile(MPEG-4:profile_ObjectType) 见下表（左边是MPEG-2版本；右边是MPEG-4版本，Profile_ObjectType的值）： sampling_frequency_index 采样率的数组下标，即：sampling frequeny[sampling_frequency_index] ： private_bit 私有位，编码时设置为0，解码时忽略。 channel_configuration 声道数。 original_copy 编码时设置为0，解码时忽略。 home 编码时设置为0，解码时忽略。 adts_variable_header关键参数如下： copyright_identification_bit 72位版权标识字段中的一位。 copyright_identification_start 一位表示 该音频帧中的copyright_identification_bit是 72位版权标识的第一位。如果不 版权标识已传输，此位应 保持为'0'。'0'在 此音频帧“ 1”开始在 此音频帧。 frame_length(MPEG-4:aac_frame_length) 帧的长度，包括header和以字节为单位的error_check。 adts_buffer_fullness 固定0x7FF，表示比特流是可变速率比特流。 number_of_raw_data_blocks_in_frame 在ADTS帧中有number_of_raw_data_blocks_in_frame + 1个AAC原始数据块。number_of_raw_data_blocks_in_frame == 0 表示说ADTS帧中有一个AAC原始数据块。 下图为一个ADTS格式的文件开头部分 我们来数一个第一个frame的header ####adts_fixed_header() #-------FFF1--------- 1111 1111 1111 .... #0xFFF; syncword的值 .... .... .... 0... #ID=0;使用MPEG-4 .... .... .... .00. #layer .... .... .... ...0 #protection_absent，不使用error_check() #-------4C80--------- 01.. .... .... .... #Profile_ObjectType=AAC MAIN ..00 11.. .... .... #sampling_frequency_index=3,采样率为48000 .... ..0. .... .... #private_bit .... ...0 10.. .... #channel_configuration=2（声道数） .... .... ..0. .... #original_copy .... .... ...0 .... #home ####adts_variable_header() #-------(4C8)0--------- .... .... .... 0... #copyright_identification_bit .... .... .... .0.. #copyright_identification_start #-------(4C8)0 223F-------- ..00 0010 0010 001. .... #aac_frame_length=0x111=273字节-->下一帧到上图的FFF14C #-------(22)3F FC-------- ...1 1111 1111 11.. #adts_buffer_fullness=0x7FF #-------(F)C-------- ..00 #number_of_raw_data_blocks_in_frame faac开源库 （1）下载 https://nchc.dl.sourceforge.net/project/faac/faac-src/faac-1.29/faac-1.29.9.2.tar.gz （2）编译生成静态库（这里是android的交叉编译脚本，ndk21，平台mac） #!/bin/bash PREFIX=`pwd`/android/armeabi-v7a NDK_ROOT=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 # 注意：Mac为darwin-x86_64，linux为linux-x86_64；一定要确保路径真实有效 TOOLCHAIN=$NDK_ROOT/toolchains/llvm/prebuilt/darwin-x86_64 CROSS_COMPILE=$TOOLCHAIN/bin/arm-linux-androideabi #在android studio中新建一个NDK项目，并且保持NDK版本与这里的一致。该FLAGS从build.ninja文件中拷贝。 FLAGS=\"-g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -march=armv7-a -mthumb -Wformat -Werror=format-security -Oz -DNDEBUG -fPIC\" export CC=$TOOLCHAIN/bin/armv7a-linux-androideabi21-clang export CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi21-clang++ export CFLAGS=\"$FLAGS\" export PATH=$PATH:$TOOLCHAIN/bin ./configure \\ --prefix=$PREFIX \\ --host=arm-linux-androideabi \\ --with-pic \\ --enable-shared=no make clean make install （3）移入项目中 并且配置CMakeList.txt文件 ... include_directories(include) ... target_link_libraries( ... faac ) （4）API简单使用 //1.打开编码器，获取inputSamples和maxOutputBytes的值，用于后面编码 //1：采样率；2：声道数；3：单次输入的样本数；4：输出数据最大字节数 faacEncOpen(unsigned long sampleRate,unsigned int numChannels,unsigned long *inputSamples,unsigned long *maxOutputBytes); //2.设置编码器参数 faacEncConfigurationPtr config = faacEncGetCurrentConfiguration(faacEncHandle hEncoder); //指定mpeg4编码标准 config->mpegVersion = MPEG4; //config->mpegVersion = MPEG2; //lc 标准 config->aacObjectType = LOW; //16位 config->inputFormat = FAAC_INPUT_16BIT; // 编码出原始数据；0 = Raw; 1 = ADTS config->outputFormat = 1; faacEncSetConfiguration(faacEncHandle hEncoder, config); //3.进行编码 //1：FAAC的handle；2：采集的pcm的原始数据；3：从faacEncOpen获取的inputSamples；4：至少有从faacEncOpen获取maxOutputBytes大小的缓冲区；5：从faacEncOpen获取maxOutputBytes //返回值为编码后数据字节的长度 int encodeLenght = faacEncEncode(faacEncHandle hEncoder, int32_t * inputBuffer, unsigned int samplesInput, unsigned char *outputBuffer, unsigned int bufferSize); 参考 AAC格式简介 wiki audio file format https://csclub.uwaterloo.ca/~ehashman/ISO14496-3-2009.pdf Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/5-rtmp.html":{"url":"直播推流全过程/5-rtmp.html","title":"直播推流编码之RTMP","keywords":"","body":"直播推流编码之RTMP（5）简述Handshake Diagram（握手流程）分块RTMPDump参考直播推流编码之RTMP（5） 简述 Adobe 公司的实时消息传输协议 (RTMP) 通过一个可靠地流传输提供了一个双向多通道消息服务，意图在通信端之间传递带有时间信息的视频、音频和数据消息流。 Handshake Diagram（握手流程） Uninitialized (未初始化)： 客户端发送C0包(1 字节，版本信息)，如果服务器支持这个版本会响应S0和S1，否则终止连接。 Version Sent (版本已发送)： 当服务器接收到版本号后(已发送S0和S1)，客户端等S1，服务器等C1，当都接收后，客户端发送C2，服务器发送S2，然后两者状态变成Ack Sent。 Ack Sent (确认已发送)： 客户端和服务器分别等待 S2 和 C2。 Handshake Done (握手结束)： 客户端和服务器可以开始交换消息了。 分块 网络传输过程中，每一个块（每个rtmp包）必须被完全发送才可以发送下一块，而在接收端，这些块被根据 chunk stream ID 被组装成消息。分块允许上层协议将大的消息分解为更小的消息，例如，防止体积大的但优先级小的消息 (比如视频) 阻碍体积较小但优先级高的消息 (比如音频或者控制命令)。分块也让我们能够使用较小开销发送小消息，因为块头包含包含在消息内部的信息压缩提示。下面块格式就是一个块的组成。（注意：当连续接收到chunk stream ID 相同时，这些快是同一个消息，需要合并。） Chunk Format（块格式） 下图大致的概括了一块的组成，从当前块的 第一个字节 大致能分析出该块头的组成信息。 Extended Timestamp 和 Chunk Data具体计算在下面介绍。 Basic Header（块基本头） cs id保留0和1的值，而2的值保留用于下层协议控制消息和命令；具体如下： 第一个字节低6bit值>1： Basic Header 为1byte；fmt=2bit；cs id =6bit，范围：2-63（也就是6bit最大能支持的范围）。 fmt cs id 高2bit 低6bit 如：0100 1010->fmt：01.. ....=2；cs id：..00 1010=10 第一个字节低6bit值=0： Basic Header 为2byte；fmt=2bit；此时第一个字节中后6bit的值为0，cs id=第2个byte，范围：64 ~ 319（也就是第2个byte的值 + 64）。 fmt 0 cs id - 64 高2bit 低6bit byte 如：1000 0000 0001 0001->fmt：10.. ....=2；0：..00 0000；cs id：(0001 0001)+64=17+64=81 第一个字节低6bit值=1： Basic Header 为3byte；fmt=2bit；此时第一个字节中后6bit的值为0，cs id=第2个byte，范围：64~65599（(第3个byte) * 256 + (第2个byte) + 64）。 fmt 1 cs id - 64 高2bit 低6bit 2byte 如：1100 0000 0001 0001 0010 0010->fmt：11.. ....=2；1：..00 0001；cs id：(0010 0010)x256 + (0001 0001)+64=8704 + 17 + 64=8785 Message Header（块消息头） 根据块基本头中的 fmt 的值来区分块消息头，从0—3共4种，上图标的很明确了，具体如下： fmt = 0： 块消息头为11字节，当前消息的 timestamp 在这表示(此时 Extended Timestamp 辅助用)，如果用户设置时间戳>=0xFFFFFF时(3字节容不下了)时，timestamp 字段就固定为0xFFFFFF。message length 是指 Chunk Data 的大小。Extended Timestamp 字段用4字节表示； timestamp message length message type id message stream id 3byte 3byte 1byte 4byte fmt = 1： 块消息头为7字节，少了 message stream id ，这一块使用前一块一样的流 ID。 timestamp delta message length message type id 3byte 3byte 1byte fmt = 2： 块消息头为3字节，只有 timestamp delta ，计算方式同 fmt = 0 时一样处理；流 ID与 fmt = 1 相同。 fmt = 3： 无块消息头；流 ID与 fmt = 1 相同；当一个消息被切割成多块时，除第一块外，其他都应 fmt = 3。 Extended Timestamp（扩展时间戳） 当块消息头中的 timestamp 或者 timestamp delta 字段(3字节)容不下时(fmt = 0，1或2)，Extended Timestamp 才会被使用。 RTMPDump tmpdump 是一个用来处理 RTMP 流媒体的工具包，支持 rtmp://, rtmpt://, rtmpe://, rtmpte://, and rtmps:// 等。源码详细api以及流程图请看雷神的RTMPdump 源代码分析 1： main()函数 ，这里只是简单介绍，集成到android中使用。因为源码很少，所以直接在as中进行编译生成静态库。 （1）下载：http://rtmpdump.mplayerhq.hu/download/rtmpdump-2.3.tgz （2）把源码导入到as中，如下图所示： librtmp/CMakeLists.txt文件配置： cmake_minimum_required(VERSION 3.4.1) #预编译宏 set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DNO_CRYPTO\" ) #所有源文件放入 rtmp_source 变量 file(GLOB rtmp_source *.c) #编译静态库 add_library(rtmp STATIC ${rtmp_source} ) 项目的CMakeLists.txt文件引用编译生成的静态库 ... # 引入指定目录下的CMakeLists.txt add_subdirectory(${CMAKE_SOURCE_DIR}/librtmp) ... #4、链接到库文件，jni/c/c++可以引入链接到 target_link_libraries( rtmp ...) 参考 （1）rtmp_specification_1.0 （2）https://www.cnblogs.com/Kingfans/p/7083100.html （3）https://blog.csdn.net/commshare/article/details/103393461 （4）https://blog.csdn.net/leixiaohua1020/article/details/12952977 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/h264-descriptor.html":{"url":"直播推流全过程/h264-descriptor.html","title":"其他：H.264符号描述","keywords":"","body":"H.264描述符简述浅谈哥伦布编码ue(v)se(v)me(v)te(v)其他参考H.264描述符 简述 H264/AVC文档中存在着大量元素描述符ue(v)，me(v)，se(v)，te(v)等，编码为ue(v)，me(v)，或者 se(v) 的语法元素是指数哥伦布编码，而编码为te(v)的语法元素是舍位指数哥伦布编码。这些语法元素的解析过程是由比特流当前位置比特开始读取，包括非0 比特，直至leading_bits 的数量为0。由于解码过程中需要计算出每个元素的占位(bit数量)，所以我们就不得不理解这些描述符了。 浅谈哥伦布编码 在说哥伦布编码之前首先先谈谈什么是定长编码？什么是可变长度编码？ 定长编码 即固定长度的编码，如：对汉字进行编码时，每个汉字都是2两个字节。 好处是只要读取该固定的字节长度就能取出值。 坏处是会浪费很多不必要的内存，比如用1个字节能存储的值，却必须要2个字节，浪费了一个字节。 可变长编码 长度不固定的编码，随内容大小而改变编码长度。 好处是有多少内容，就占用多少内存。 但是因为内容不固定，所以得在内存中是如何知道这些内容的长度呢？所以一般都是在内容前面加上内容的长度。 为何H264中采用哥伦布编码 在H264编码中，有大量的短数据（在255以内），如：宏块大小，各种标志位等。为了减少码流的长度，把这些短数据使用可变长度编码，而在众多可变长度编码的算法中，哥伦布编码尤其合适H264编码。因为数据的值小，在其二进制的值上+1再补长度个数的0即可，这样就可以把每个bit用到，不会产生多余的浪费。 而为何只是用于短数据呢？因为多少个0代码数据长度，也就是说数据大的，就会产生很多很多0，会造成更大的浪费。具体看下面原理。 ue(v) 无符号整数指数哥伦布码编码的语法元素，左位在先。计算公式： codeNum = 2leadingZeroBits2^{leadingZeroBits}2​leadingZeroBits​​ − 1 + read_bits( leadingZeroBits ) codeNum： 占的位数（bit数量） leadingZeroBits： 遇到第一个1前面0的个数；如：0010 1011，leadingZeroBits的值为2； read_bits( leadingZeroBits )： 遇到第一个1后面leadingZeroBits个组成的无符号二进制值；如：0010 1011，值为...0 1...，即01，即1； 举两个栗子： （1）0001 1001 =>leadingZeroBits== 000. ....= 3，read_bits( 3 )==.... 100.=4，所以：codeNum=232^32​3​​-1+4=13 （2）0000 0101 0000 0000 => leadingZeroBits=5，read_bits( 5 ) = .... ..01 000. ....=8，codeNum=252^52​5​​-1+8=39 se(v) 有符号整数指数哥伦布码编码的语法元素位在先。在按照上面ue(v)公式计算出codeNum后，然后使用该计算公式： value = (−1)k+1(-1)^{k+1}(−1)​k+1​​ Ceil( k÷2 ) Ceil： 返回大于或者等于指定表达式的最小整数，如： Ceil(1.5)= 2 如上例（1）0001 1001 => codeNum=232^32​3​​-1+4=13，value = (−1)13+1(-1)^{13+1}(−1)​13+1​​Ceil(13÷2)=Ceil(6.5)=7 me(v) 映射的指数哥伦布码编码的语法元素，左位在先。在按照上面ue(v)公式计算出codeNum后，然后查表。（在H.264-AVC-ISO_IEC_14496-10的9.1.2 章节中表9-4） te(v) 舍位指数哥伦布码编码语法元素，左位在先。明确取值范围在0-x；当x>1时，te(v)就是ue(v)；否则(x=1)，b = read_bits( 1 )，codeNum = !b 其他 ae(v)： 上下文自适应算术熵编码语法元素 b(8)： 任意形式的8比特字节。 f(n)： n位固定模式比特串（由左至右），左位在先。 i(n)： 使用n比特的有符号整数。 u(n)： n位无符号整数。 参考 (1).H.264-AVC-ISO_IEC_14496-10 (2).https://blog.csdn.net/lizhijian21/article/details/80982403 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-05 09:25:42 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"直播推流全过程/6-optimize.html":{"url":"直播推流全过程/6-optimize.html","title":"其他：直播优化基础","keywords":"","body":"直播优化基础简单介绍学习参考优化重要知识点直播优化基础 简单介绍 当前直播系统涉及到的范畴是非常多的，从主播推流到观众播放，涉及到编解码、推流拉流、CND网络、粉丝间互动、给主播刷礼物等等，这系统非常庞大，还涉及到人力物力的安排，框架的选择，跨平台的处理…… 我在这方面没有过多的经验，全是通过\"大牛\"们了解到的，所以这里只是给出学习参考。 学习参考 《音视频开发进阶指南：基于Android与iOS平台的实践》 \"第11章　直播应用的构建\"：介绍直播系统所涉及到的范畴；\"第12章　直播应用中的关键处理\"：推流和拉流过程中的优化有理方案。 《Android 音视频开发_何俊林》\"第9章 直播技术\"：涉及到真个直播流程的介绍、处理以及优化。 音视频播放过程中的问题解决(播放质量优化) ：直播过程异常情况及原因简介。 优化重要知识点 H.264协议：目前还是占主流位置，需要根据懂得协议才能设置优化编码解码参数。 硬编：1：加快编码速度；2：解放CPU。 OpenGL：使用GPU播放视频，解放CPU。 注：由于H.265所涉及到的专利非常多且收费不明确等原因，导致现在很多企业都没有使用。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/":{"url":"AFPlayer/","title":"AFPlayer","keywords":"","body":"总纲简述总纲 简述 AFPlayer是指在Android上使用FFmpeg做的播放器。Android 上使用FFmpeg、OpenSL ES、OpenGL SE、MediaCodec等，实现简单的播放器，主要体现出相关知识点的使用。 项目地址：AFPlayer 第一章：OpenSL ES播放PCM OpenSL ES全称 Open Sound Library for Embedded Systems，即嵌入式系统的开放音频库。是无授权费、跨平台、硬件加速的C语言音频API，用于2D和3D音频。 第二章：OpenGL ES播放RGB OpenGL（英语：Open Graphics Library，译名：开放图形库或者“开放式图形库”）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序编程接口（API）。而OpenGL ES （OpenGL for Embedded Systems）是三维图形应用程序接口OpenGL的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，如Android 。 第三章：FFmpeg实现MediaCodec解码 MediaCodec是Android平台的硬件编解码，FFmpeg从3.1版本开始支持，但是，目前支持实现解码功能。官网硬件编解码支持结束 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/01_opensl_es.html":{"url":"AFPlayer/01_opensl_es.html","title":"OpenSL ES播放PCM","keywords":"","body":"OpenSL ES播放PCM简述API基本介绍对象和接口重要接口播放流程代码实现编辑CMakeList.txt进行链接动态库关键流程参考OpenSL ES播放PCM 简述 OpenSL ES全称 Open Sound Library for Embedded Systems，即嵌入式系统的开放音频库。是无授权费、跨平台、硬件加速的C语言音频API，用于2D和3D音频。 API基本介绍 我们可以从Android官网中了解到一些基本的API介绍。 对象和接口 对象：SLObjectItf，理解成只有存储能力的JavaBean，没有调用其他函数（方法）的途径。所有对象的名称使用这个，而创建的对象都需要调用Realize进行初始化。 接口：SLEngineItf，接口是跟每一个SLObjectItf进行绑定，可以有多个，拥有调用其他函数（方法）的途径。获取的流程如下：流程图 +----------------------------+ | 创建SLObjectItf（实例化过程） | +----------------------------+ | V +----------------------------+ | Realize（初始化过程） | +----------------------------+ | V +-----------------------------+ | 获取GetInterface（绑定过程） | +-----------------------------+ 重要接口 SLEngineItf：OpenSL ES引擎接口，全局唯一。用于创建混音器对象，播放器对象等。 SLPlayItf：播放器接口。用于获取播放状态，设置播放状态等。 SLAndroidSimpleBufferQueueItf：数据队列接口。 播放流程 通过上图可知： 只有接口拥有调用其他函数的功能；即：SLEngineItf调用了CreateOutputMix和CreateAudioPlayer。 一个对象可以生成（绑定）多个接口；即：\"播放器对象\"生成了SLPlayItf和SLAndroidSimpleBufferQueueItf。 代码实现 有个比较关键的点是：OpenEL SE 不支持浮点类型的值，所以需要转换成整形再使用。本文数据来源的形式是通过读取本地Sdcard文件来完成的。如果是想通过网络或则asset目录，请查看Android官方项目：native-audio 。 编辑CMakeList.txt进行链接动态库 target_link_libraries( native-lib log android OpenSLES) 关键流程 void bqPlayerCallback(SLAndroidSimpleBufferQueueItf bq, void *ctx) { (static_cast(ctx))->ProcessSLCallback(bq); } AudioPlayer::AudioPlayer(const char *filename, uint32_t sampleRate, uint8_t channels, uint32_t bitPerChannel) { //一帧的大小 frameSize = sampleRate * channels * bitPerChannel / 8; inFile = fopen(filename, \"rb+\"); SLresult result; //1.1）创建引擎对象。引擎对象是OpenSL ES提供API的唯一入 result = slCreateEngine(&slEngineObj_, 0, nullptr, 0, NULL, NULL); SLASSERT(result);//断言，用于调试，能快速定位问题 //1.2）实例化引擎对象，需要通过在第1步得到的引擎对象接口来实例化(在ELSE中，任何对象都需要使用接口来进行实例化) result = (*slEngineObj_)->Realize(slEngineObj_, SL_BOOLEAN_FALSE); SLASSERT(result); //1.3）获取这个引擎对象的方法接口，通过GetInterface方法，使用第2步已经实例化好了的对象 result = (*slEngineObj_)->GetInterface(slEngineObj_, SL_IID_ENGINE, &slEngineItf_); SLASSERT(result); //2.创建混音器对象 result = (*slEngineItf_)->CreateOutputMix(slEngineItf_, &outputMixObjectItf_, 0, NULL, NULL); SLASSERT(result); result = (*outputMixObjectItf_)->Realize(outputMixObjectItf_, SL_BOOLEAN_FALSE); SLASSERT(result); //3、创建播放器 // numBuffers：设置2个缓冲数据 SLDataLocator_AndroidSimpleBufferQueue android_queue = {SL_DATALOCATOR_ANDROIDSIMPLEBUFFERQUEUE, 2}; SLDataFormat_PCM pcm = { SL_DATAFORMAT_PCM,//播放pcm格式的数据 channels,//声道数（立体声） sampleRate * 1000,//44100hz -> 44100000 的频率；参考：SL_SAMPLINGRATE_44_1 bitPerChannel == 32 ? SL_PCMSAMPLEFORMAT_FIXED_32 : SL_PCMSAMPLEFORMAT_FIXED_16,//位数 32位 bitPerChannel == 32 ? SL_PCMSAMPLEFORMAT_FIXED_32 : SL_PCMSAMPLEFORMAT_FIXED_16,//和位数一致就行 SL_SPEAKER_FRONT_LEFT | SL_SPEAKER_FRONT_RIGHT,//立体声（前左前右） SL_BYTEORDER_LITTLEENDIAN//小端排序 }; SLDataSource slDataSource = {&android_queue, &pcm}; SLDataLocator_OutputMix outputMix = {SL_DATALOCATOR_OUTPUTMIX, outputMixObjectItf_}; SLDataSink audioSnk = {&outputMix, NULL}; const SLInterfaceID ids[3] = {SL_IID_BUFFERQUEUE, SL_IID_EFFECTSEND, SL_IID_VOLUME}; const SLboolean req[3] = {SL_BOOLEAN_TRUE, SL_BOOLEAN_TRUE, SL_BOOLEAN_TRUE}; result = (*slEngineItf_)->CreateAudioPlayer(slEngineItf_, &playerObjectItf_, &slDataSource, &audioSnk, 3, ids, req); SLASSERT(result); result = (*playerObjectItf_)->Realize(playerObjectItf_, SL_BOOLEAN_FALSE); SLASSERT(result); result = (*playerObjectItf_)->GetInterface(playerObjectItf_, SL_IID_PLAY, &playItf_); SLASSERT(result); //4、获取播放器对象的数据队列接口 result = (*playerObjectItf_) ->GetInterface(playerObjectItf_, SL_IID_BUFFERQUEUE, &playBufferQueueItf_); SLASSERT(result); //5. 设置回调函数 result = (*playBufferQueueItf_) ->RegisterCallback(playBufferQueueItf_, bqPlayerCallback, this); SLASSERT(result); //6. 获取播放状态接口 result = (*playItf_)->SetPlayState(playItf_, SL_PLAYSTATE_PLAYING); SLASSERT(result); //7. 主动调用回调函数开始工作 ProcessSLCallback(playBufferQueueItf_); } void AudioPlayer::ProcessSLCallback(SLAndroidSimpleBufferQueueItf bq) { void *buffer; getPcmData(&buffer); if (NULL != buffer) { SLresult result; result = (*bq)->Enqueue(bq, buffer, frameSize); SLASSERT(result); } } void AudioPlayer::getPcmData(void **pcm) { outBuf = new uint32_t[frameSize]; while (!feof(inFile)) { //读取一帧数据 memset(outBuf, 0, frameSize); fread(outBuf, frameSize, 1, inFile); if (outBuf == nullptr) {//读取结束 break; } *pcm = outBuf; goto end; } __android_log_print(ANDROID_LOG_DEBUG, \"audio_play\", \"red finish?\"); (*playItf_)->SetPlayState(playItf_, SL_PLAYSTATE_STOPPED); end: __android_log_print(ANDROID_LOG_DEBUG, \"audio_play\", \"playing？\"); } Demo地址：OpenSLESDemo 参考 https://en.wikipedia.org/wiki/OpenSL_ES https://blog.csdn.net/ywl5320/article/details/78503768 https://github.com/android/ndk-samples/tree/main/audio-echo Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/02_opengl_es.html":{"url":"AFPlayer/02_opengl_es.html","title":"OpenGL ES播放RGB","keywords":"","body":"OpenGL ES播放RGB简述屏幕图像显示流程OpengGL相关处理OpenGL渲染管线着色器几个重要概念纹理OpenGL小结EGL（Android）实现显示RGB24（Android）参考OpenGL ES播放RGB 简述 OpenGL（英语：Open Graphics Library，译名：开放图形库或者“开放式图形库”）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序编程接口（API）。而OpenGL ES （OpenGL for Embedded Systems）是三维图形应用程序接口OpenGL的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，如Android 。 屏幕图像显示流程 下图所示为常见的 CPU、GPU、显示器工作方式（网上找的）。CPU 计算好显示内容提交至 GPU，GPU 渲染完成后将渲染结果存入帧缓冲区，视频控制器会按照 VSync 信号逐帧读取帧缓冲区的数据，经过数据转换后最终由显示器进行显示。 所以在本节涉及到的内容主要有： CPU：获取本地数据（图像）并计算好作为GPU的输入数据。 GPU：通过OpengGL ES处理后的数据传递给Monitor进行显示。 Monitor：在Android中需要通过EGL作为桥梁衔接屏幕进行显示（因为OpenGL不负责窗口管理及上下文环境管理，而EGL承担了这一职责）。 OpengGL相关处理 OpenGL渲染管线 图形渲染管线（Graphics Pipeline）： 大多译为管线，实际上指的是一堆原始图形数据（顶点信息(坐标、法向量等)和像素信息(图像、纹理等)）途经一个输送管道，期间经过各种变化处理最终出现在屏幕的过程。 图形渲染管线的主要工作可以被划分为两个部分 1）把 3D 坐标转换为 2D 坐标（在OpenGL中，任何事物都在3D空间中，而屏幕和窗口却是2D像素数组）。 2）把 2D 坐标转变为实际的有颜色的像素 图形渲染管线的具体实现可分为六个阶段 注：蓝色部分代表着色器的部分。 第一阶段，顶点着色器： 顶点数据（Vertex Data） 数据（一系列顶点的集合）由3D 坐标转为另一种 3D 坐标，同时顶点着色器可以对顶点属性进行一些基本处理。（下面详细介绍） 第二阶段，形状（图元）装配： 该阶段将顶点着色器输出的所有顶点作为输入，并将所有的点装配成指定图元的形状。图中则是一个三角形。图元（Primitive） 用于表示如何渲染顶点数据，如：点、线、三角形。 第三阶段，几何着色器： 该阶段把图元形式的一系列顶点的集合作为输入，它可以通过产生新顶点构造出新的（或是其它的）图元来生成其他形状。例子中，它生成了另一个三角形。（这个阶段可以省略） 第四阶段，光栅化： 该阶段会把图元映射为最终屏幕上相应的像素，生成片段。片段（Fragment） 是渲染一个像素所需要的所有数据（一个矩形数组，包含了颜色、深度、线宽、点的大小等信息）。 第五阶段，片段着色器： 该阶段首先会对输入的片段进行 裁切（Clipping）。裁切会丢弃超出视图以外的所有像素，用来提升执行效率。（下面详细介绍） 第六阶段，测试与混合 该阶段会检测片段的对应的深度值（z 坐标），判断这个像素位于其它物体的前面还是后面，决定是否应该丢弃。此外，该阶段还会检查 alpha 值（ alpha 值定义了一个物体的透明度），从而对物体进行混合。因此，即使在片段着色器中计算出来了一个像素输出的颜色，在渲染多个三角形的时候最后的像素颜色也可能完全不同。 这是OpenGL内部需要处理的流程，而我们程序处理OpenGL的则是以下两大模块： 1）创建着色器（至少是顶点着色器和片段着色器），设置相关参数\"告诉\"OpenGL如何进行处理，以及绑定相关程序。 2）生成以及编译纹理，把纹理输入到着色器中，并且设置相关参数\"告诉\"OpenGL如何进行处理。 着色器 着色器(Shader)： 是指GPU上为每一个（渲染管线）阶段运行各自的小程序。而OpenGL着色器是用OpenGL着色器语言(OpenGL Shading Language, GLSL)写成的，这是语法简单介绍 。上面就介绍了3种着色器。 如果我们打算做渲染的话，在现代OpenGL中，我们必须定义至少一个顶点着色器和一个片段着色器（因为GPU中没有默认的顶点/片段着色器）。以下将要对两者做详细的介绍。 顶点着色器 主要的目的是把3D坐标转为另一种3D坐标(标准化设备坐标)，以及顶点着色器允许我们对顶点属性进行一些基本处理。 标准化设备坐标（就是下文纹理说的OpenGL物体表面坐标）是一个x、y和z值在-1.0到1.0的一小段空间。任何落在范围外的坐标都会被丢弃/裁剪，不会显示在你的屏幕上。下面你会看到我们定义的在标准化设备坐标中的三角形(忽略z轴)： 与通常的屏幕坐标不同，y轴正方向为向上，(0, 0)坐标是这个图像的中心，而不是左上角。最终你希望所有(变换过的)坐标都在这个坐标空间中，否则它们就不可见了。你的标准化设备坐标接着会变换为屏幕空间坐标(Screen-space Coordinates)，这是使用你通过glViewport函数提供的数据，进行视口变换(Viewport Transform)完成的。所得的屏幕空间坐标又会被变换为片段输入到片段着色器中。 顶点着色器允许我们指定任何以顶点属性为形式的输入。这使其具有很强的灵活性的同时，它还的确意味着我们必须手动指定输入数据的哪一个部分对应顶点着色器的哪一个顶点属性。所以，我们必须在渲染前指定OpenGL该如何解释顶点数据。 使用示例： 1）编写顶点着色器的源代码（根c语言基本一样）： const GLchar* VERTEX_SHADER = \"#version 330 core\\n\" \"layout (location = 0) in vec3 position;\\n\" \"layout (location = 1) in vec3 color;\\n\" \"layout (location = 2) in vec2 texCoord;\\n\" \"out vec3 ourColor;\\n\" \"out vec2 TexCoord;\\n\" \"void main(){\\n\" \" gl_Position = vec4(position, 1.0f);\\n\" \" ourColor = color;\\n\" \" TexCoord = texCoord;\\n\" \"}\\n\"; #version 330 core：表示需要使用OpenGL的版本。 layout (location = 0) in vec3 position;（第三、四行同理）： 1) layout (location = 0)：设定了输入变量position的位置值为0，跟glVertexAttribPointer配合使用，用来告诉OpenGL该如何解析顶点数据。下面纹理还会具体介绍。 2) in：表示position变量是接收外界出入的数据。 3）vec3：表示值为float类型，有3分量的容器，同理有：ivecn包含n个int分量的向量；uvecn包含n个unsigned int分量的向量等等。 out vec3 ourColor;： 表示ourColor变量的数据将作为输出，传递到\"片段着色器\"中作为输入被使用。 gl_Position = vec4(position, 1.0f);： 1) gl_Position：原始的顶点数据在顶点着色器中经过平移、旋转、缩放等数学变换后，生成新的顶点位置（一个四维 (vec4) 变量，包含顶点的 x、y、z 和 w 值）。新的顶点位置通过在顶点着色器中写入gl_Position传递到渲染管线的后继阶段继续处理。 2) vec4(position, 1.0f)：把3个分量的容器position再一个分量1.0f创建一个4个分量的容器。 2）编译顶点着色器（为了能够让OpenGL使用它）： GLuint vertexShader; vertexShader = glCreateShader(GL_VERTEX_SHADER); //创建顶点着色器 glShaderSource(vertexShader, 1, VERTEX_SHADER, NULL); //关联上面的源代码 glCompileShader(vertexShader); //在线编译 //以下为异常情况处理，打印编译错误（如果有的话） GLint success; GLchar infoLog[512]; glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(vertex, 512, NULL, infoLog); std::cout 3）链接到一个用来渲染的着色器程序，着色器程序 ，已激活着色器程序的着色器将在我们发送渲染调用的时候被使用。 //1）着色器程序 GLuint shaderProgram; shaderProgram = glCreateProgram(); //创建一个程序对象 glAttachShader(shaderProgram, vertexShader); //把顶点着色器附加到程序对象上 glAttachShader(shaderProgram, fragmentShader); //把片段着色器附加到程序对象上 glLinkProgram(shaderProgram); //链接它们 glUseProgram(shaderProgram); //激活这个程序对象，调用后每个着色器调用和渲染调用都会使用这个程序对象 // 打印连接错误（如果有的话） glGetProgramiv(shaderProgram, GL_LINK_STATUS, &success); if(!success){ glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog); std::cout 片段着色器 片段着色器(Fragment Shader)全是关于计算你的像素最后的颜色输出。在计算机图形中颜色被表示为有4个元素的数组：红色、绿色、蓝色和alpha(透明度)分量，通常缩写为RGBA。当在OpenGL或GLSL中定义一个颜色的时候，我们把颜色每个分量的强度设置在0.0到1.0之间。比如说我们设置红为1.0f，绿为1.0f，我们会得到两个颜色的混合色，即黄色。 使用示例： 1）编写片段着色器的源代码： const GLchar* FRAGMENT_SHADER = \"#version 330 core\\n\" \"in vec3 ourColor;\\n\" \"in vec2 TexCoord;\\n\" \"out vec4 color;\\n\" \"uniform sampler2D ourTexture;\\n\" \"void main(){\\n\" \" color = texture(ourTexture, TexCoord);\\n\" \"}\\n\"; uniform sampler2D ourTexture;：定义全局(uniform)变量ourTexture，sampler2D是指2D的纹理采样器。 color = texture(ourTexture, TexCoord);：GLSL内建的texture函数来采样纹理的颜色，它第一个参数是纹理采样器，第二个参数是对应的纹理坐标。 2）编译片段着色器（同顶点着色器，参数变了而已）： GLuint fragmentShader; vertexShader = glCreateShader(GL_FRAGMENT_SHADER); //创建顶点着色器 glShaderSource(fragmentShader, 1, FRAGMENT_SHADER, NULL); //关联上面的源代码 glCompileShader(fragmentShader); //在线编译 //以下为异常情况处理，打印编译错误（如果有的话） GLint success; GLchar infoLog[512]; glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(vertex, 512, NULL, infoLog); std::cout 3）链接到一个用来渲染的着色器程序，同顶点着色器。 几个重要概念 VBO VBO：顶点缓冲对象(Vertex Buffer Objects)；它会在GPU内存(通常被称为显存)中储存大量顶点，使用这些缓冲对象的好处是我们可以一次性的发送一大批数据到显卡上，而不是每个顶点发送一次。从CPU把数据发送到显卡相对较慢，所以只要可能我们都要尝试尽量一次性发送尽可能多的数据。当数据发送至显卡的内存中后，顶点着色器几乎能立即访问顶点，这是个非常快的过程。 使用实例： GLuint VBO; glGenBuffers(1, &VBO); glBindBuffer(GL_ARRAY_BUFFER, VBO); //此时：会生成 1 个 GL_ARRAY_BUFFER 类型的 VBO对象（数量可以一次多个） glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);//把之前定义的顶点数据复制到缓冲的内存中 //最后程序退出时： glDeleteBuffers(1, &VBO); glBufferData相关参数介绍： 参1：目标缓冲的类型：顶点缓冲对象当前绑定到GL_ARRAY_BUFFER目标上。还有GL_ELEMENT_ARRAY_BUFFER等，下面介绍。 参2：指定传输数据的大小(以字节为单位)；用一个简单的sizeof计算出顶点数据大小就行。 参3：我们希望发送的实际数据。 参4：指定了我们希望显卡如何管理给定的数据，它有三种形式： GL_STATIC_DRAW ：数据不会或几乎不会改变。 GL_DYNAMIC_DRAW：数据会被改变很多。 GL_STREAM_DRAW ：数据每次绘制时都会改变。 VAO（Android OpenGL ES3.0才支持） VAO：顶点数组对象(Vertex Array Object)；可以像顶点缓冲对象那样被绑定，任何随后的顶点属性调用都会储存在这个VAO中。这样的好处就是，当配置顶点属性指针时，你只需要将那些调用执行一次，之后再绘制物体的时候只需要绑定相应的VAO就行了。这使在不同顶点数据和属性配置之间切换变得非常简单，只需要绑定不同的VAO就行了。刚刚设置的所有状态都将存储在VAO中。 一个顶点数组对象会储存以下这些内容： glEnableVertexAttribArray和glDisableVertexAttribArray的调用。 通过glVertexAttribPointer设置的顶点属性配置。 通过glVertexAttribPointer调用进行的顶点缓冲对象与顶点属性链接。 使用实例： GLuint VAO; glGenVertexArrays(1, &VAO); //创建一个VAO和创建一个VBO很类似 // ..:: 初始化代码（只运行一次 (除非你的物体频繁改变)） ::.. // 1. 绑定VAO glBindVertexArray(VAO); // 2. 把顶点数组复制到缓冲中供OpenGL使用 glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // 3. 设置顶点属性指针 glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid*)(6 * sizeof(GLfloat))); glEnableVertexAttribArray(2);//对应glVertexAttribPointer中的第一个参数 2 //4. 解绑VAO glBindVertexArray(0); [...] // ..:: 绘制代（游戏循环中） ::.. // 5. 绘制物体 glUseProgram(shaderProgram); glBindVertexArray(VAO); //使用 glDrawElements 等进行绘制 glBindVertexArray(0); glVertexAttribPointer相关参数介绍： 参1：指定我们要配置的顶点属性。还记得我们在顶点着色器中使用layout(location = 0)定义了position顶点属性的位置值(Location)吗？它可以把顶点属性的位置值设置为2。因为我们希望把数据传递到这一个顶点属性中，所以这里我们传入2。 参2：指定顶点属性的大小。顶点属性是一个vec2，它由2个值组成，所以大小是2。 参3：指定数据的类型，这里是GL_FLOAT(GLSL中vec*都是由浮点数值组成的)。 参4：我们是否希望数据被标准化(Normalize)。如果我们设置为GL_TRUE，所有数据都会被映射到0（对于有符号型signed数据是-1）到1之间。我们把它设置为GL_FALSE。 参5：步长(Stride)，它告诉我们在连续的顶点属性组之间的间隔。由于下个组位置数据在8个GLfloat之后，我们把步长设置为8 * sizeof(GLfloat)。 参6：类型是GLvoid*，所以需要我们进行这个奇怪的强制类型转换。它表示位置数据在缓冲中起始位置的偏移量(Offset)。 该方法告诉了OpenGL如何指向（解析）不同的顶点。如上面定义的顶点着色器中的属性： \"layout (location = 0) in vec3 position;\\n\" \"layout (location = 1) in vec3 color;\\n\" \"layout (location = 2) in vec2 texCoord;\\n\" 说明一个顶点中存在了：3个向量顶点位置 + 3个向量RGB颜色 + 2个向量纹理 = 8个向量；根据下图更容易理解是如何如何计算寻找下一个顶点位置的纹理位置： EBO EBO：索引缓冲对象(Element Buffer Object，EBO，也叫Index Buffer Object，IBO)；和顶点缓冲对象一样，EBO也是一个缓冲，它专门储存索引，OpenGL调用这些顶点的索引来决定该绘制哪个顶点。因为OpenGL主要处理三角形的，所以如果绘制矩形则需要绘制两个三角形，而两个三角形有6个顶点，其中有两个顶点是重叠的，造成多余的浪费，为此，OpenGL引入了\"索引缓冲对象\"来处理，只需要4个顶点，然后画两个三角形即可；如： 原本通过6个顶点来绘制一个矩形的： GLfloat vertices[] = { // 第一个三角形 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, 0.5f, 0.0f, // 左上角 // 第二个三角形 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角 }; 现在通过EBO方法进行处理，只需要存储4个顶点，然后告诉OpenGL如何绘制即可： GLfloat vertices[] = { 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角 }; GLuint indices[] = { // 注意索引从0开始! 0, 1, 3, // 第一个三角形 1, 2, 3 // 第二个三角形 }; 下图为通过图： 下面使用索引绘制两个三角形拼成一个矩形，使用实例： GLuint EBO; glGenBuffers(1, &EBO); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); //和VBO类似， glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);//把索引复制到缓冲里 //当进行使用时 glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0);//使用当前绑定到GL_ELEMENT_ARRAY_BUFFER目标的EBO中获取索引进行绘制 glDrawElements相关参数介绍： 参1：指定绘制模式，这里三角形。 参2：绘制顶点的个数，绘制两个三角形，即6个顶点。 参3：索引的类型，GL_UNSIGNED_INT（无符号整形）。 参4：指定EBO中的偏移量，（或者传递一个索引数组，但是这是当你不在使用索引缓冲对象的时候）。 注意：VAO绑定时正在绑定的索引缓冲对象会被保存为VAO的元素缓冲对象。绑定VAO的同时也会自动绑定EBO，如图: VBO、VAO、EBO总结 VBO在GPU中缓存大量顶点数据，VAO存储顶点属性(函数)的调用，而EBO则根据索引来绘制顶点（三角形->矩形）。 纹理 纹理（Texture）是一个2D图片（甚至也有1D和3D的纹理），它可以用来添加物体的细节；在视频渲染中，只需要处理2D的纹理。这是纹理的介绍和使用 。 纹理所在流程 纹理(图片) --映射-> 纹理坐标 --传入-> 顶点着色器(进行采样) --传入-> 片段着色器(为每个片段进行纹理坐标的插值) 上面流程介绍了纹理(图片)被处理的过程，这都是在OpengGL渲染管道里面被处理的，而我们所需要做的就是如何设置参数，指引OpenGL处理，处理之前，需要介绍一下纹理的相关知识。 纹理坐标(Texture Coordinate) 这里说的纹理坐标是指OpenGL纹理坐标，除此之外，我们还需要知道另外两个坐标系，\"计算机图像纹理坐标\" 和 \"OpenGL物体表面坐标\"；通常使用是从图片等解析出来RGB格式的Byte数组是 \"计算机图像纹理坐标\" ；三者之间的关系如下： 计算机图像纹理坐标 -> OpenGL纹理坐标 -> OpenGL物体表面坐标 以下为三者的坐标系： 当我们直接把解析出来的数据（图：坐着电脑前的人）直接使用在 \"OpenGL纹理坐标\" 上，就会上下颠倒了，如下： 正确姿势如下（看\"计算机图像纹理坐标\"和\"OpenGL物体表面坐标\"对应的坐标与图像的变化）： 坐标理解非常重要，处理不好导致的效果就是歪画面。这里纹理还需要对应着顶点的坐标，见下面【纹理的使用】中定义的vertices纹理坐标。 采样(Sampling) 使用纹理坐标获取纹理颜色。 片段插值(Fragment Interpolation) 在片段中（假如无限放大后，纹理在物体表面中的片段），某些坐标点没有颜色，这时候需要对这些坐标进行插值。 纹理的使用 先看示例代码 /*****************************************/ /***** 1）生成纹理： ************/ /*****************************************/ GLuint texture; glGenTextures(1, &texture); //生成\"1\"个纹理 glBindTexture(GL_TEXTURE_2D, texture); //绑定为2D类型 //将image图片数据生成纹理，这里的image数据会根据下面vertices中- 纹理坐标 -传入到顶点着色器中进行处理 glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, image); glGenerateMipmap(GL_TEXTURE_2D); //这会为当前绑定的纹理自动生成所有需要的多级渐远纹理。 glBindTexture(GL_TEXTURE_2D, 0); //生成了纹理和相应的多级渐远纹理后，释放图像的内存并解绑纹理对象 /*****************************************/ /***** 2）应用纹理： ************/ /*****************************************/ //使用纹理坐标更新顶点数据： GLfloat vertices[] = { // ---- 顶点位置 ---- ---- 颜色 ---- - 纹理坐标 - 1.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 0.0f, // 右上 1.0f, -1.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 1.0f, // 右下 -1.0f, -1.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, // 左下 -1.0f, 1.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, 0.0f // 左上 }; //告诉OpenGL应该如何去找新的顶点纹理属性数据 glVertexAttribPointer(2, 2, GL_FLOAT,GL_FALSE, 8 * sizeof(GLfloat), (GLvoid*)(6 * sizeof(GLfloat))); glEnableVertexAttribArray(2); //调整顶点着色器使其能够接受顶点坐标为一个顶点属性，并把坐标传给片段着色器： const GLchar* VERTEX_SHADER = \"见 顶点着色器 源码\"; //片段着色器：接收顶点着色器传递过来的纹理对象，并使用sampler2D采集器进行采集 const GLchar* FRAGMENT_SHADER = \"见 片段着色器 源码\"; //这里省略着色器以及关联程序流程 //绑定纹理，并进行绘制 glBindTexture(GL_TEXTURE_2D, texture); glBindVertexArray(VAO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); glBindVertexArray(0); OpenGL小结 上面是直接使用RGB24格式的纹理进行渲染，而OpenGL是支持矩阵计算的(矩阵使用在这) ，也就是可以支持输入YUV格式数据，通过GPU在顶点着色器进行矩阵计算，转换成RGB后再进行使用。但这也会导致更复杂的计算以及适配问题（如各种yuv或其他格式的rgb数据转换为RGB时），所以我们可以选择牺牲一点CPU使用FFmpeg进行格式转换后再进行渲染。 上面的内容已经把OpenGL中纹理绘制讲解完了，然后在Mac上写了一个Demo，实现绘制一张rgb24格式的裸数据 Demo地址：OpenGLMacTextureDemo 。接下来要在Android中实现。 EGL（Android） 开头就已经说了EGL的作用了，就是作为OpenGL的输出与设备的屏幕之间架接起的桥梁，EGL是双缓冲的工作模式，即有一个Back Frame Buffer和一个Front Frame Buffer，正常绘制操作的目标都是Back Frame Buffer，操作完毕之后，调用eglSwapBuffer这个API，将绘制完毕的FrameBuffer交换到Front Frame Buffer并显示出来。而在Android平台上，使用的是EGL这一套机制，EGL承担了为OpenGL提供上下文环境以及窗口管理的职责。 下面直接来看代码： //_surface是从JAVA层传递过来的 ANativeWindow *window = ANativeWindow_fromSurface(env, _surface); // 1 EGL dispaly创建和初始化 EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); if(display == EGL_NO_DISPLAY){ LOGE(\"eglGetDisplay failed\"); return; } //初始化这个显示设备，该方法会返回一个布尔型变量来代表执行状态，后面两个参数则代表Major和Minor的版本。 if (EGL_FALSE == eglInitialize(display, 0, 0)) { LOGE(\"NativeEngine: failed to init display, error %d\", eglGetError()); return; } //一旦EGL有了Display之后，它就可以将OpenGL ES的输出和设备的屏幕桥接起来，但是需要指定一些配置项，类似于色彩格式、像素格式、RGBA的表示以及SurfaceType等 //2 surface窗口配置 EGLConfig config; EGLint configNum; EGLint attribs[] = { EGL_SURFACE_TYPE,EGL_WINDOW_BIT, EGL_BUFFER_SIZE, 24, EGL_RED_SIZE,8, EGL_GREEN_SIZE, 8, EGL_BLUE_SIZE, 8, EGL_DEPTH_SIZE, 16, EGL_NONE }; if(EGL_TRUE != eglChooseConfig(display, attribs, &config, 1, &configNum)){ LOGE(\"eglChooseConfig failed!\"); return; } //3. 创建surface，将EGL和设备的屏幕连接起来 EGLSurface surface = NULL; EGLint format; if (!eglGetConfigAttrib(display, config, EGL_NATIVE_VISUAL_ID, &format)) { LOGE(\"eglGetConfigAttrib() returned error %d\", eglGetError()); return; } ANativeWindow_setBuffersGeometry(window, 0, 0, format); if (!(surface = eglCreateWindowSurface(display, config, window, 0))) { LOGE(\"eglCreateWindowSurface() returned error %d\", eglGetError()); } //4 context 创建关联的上下文 const EGLint ctxAttr[] = { EGL_CONTEXT_CLIENT_VERSION, 2, EGL_NONE }; EGLContext context = eglCreateContext(display, config, EGL_NO_CONTEXT, ctxAttr); if(context == EGL_NO_CONTEXT){ LOGE(\"eglCreateContext failed!\"); return; } //5. 进行绑定 if(EGL_TRUE != eglMakeCurrent(display, surface, surface, context)){ LOGE(\"eglMakeCurrent failed！\"); return; } //此处为处理以上OpenGL相关 //在OpenGL执行渲染之后，最后通知window进行显示： eglSwapBuffers(display,surface); 实现显示RGB24（Android） 1）在CMakeList.txt引入相关库: target_link_libraries( native-lib android log EGL GLESv3) 2）申请读取文件权限；（因为本demo需要把文件拷贝到手机的Download目录下）。 3）在java层通过SurfaceView.getHolder().getSurface()函数把Surface传递给底层，开启子线程。 4）在C++处理OpenGL以及窗口显示等： VideoPlayer::VideoPlayer(ANativeWindow* window,const char *filename) { // 1 EGL dispaly创建和初始化 EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); if(display == EGL_NO_DISPLAY){ LOGE(\"eglGetDisplay failed\"); return; } //初始化这个显示设备，该方法会返回一个布尔型变量来代表执行状态，后面两个参数则代表Major和Minor的版本。 if (EGL_FALSE == eglInitialize(display, 0, 0)) { LOGE(\"NativeEngine: failed to init display, error %d\", eglGetError()); return; } //一旦EGL有了Display之后，它就可以将OpenGL ES的输出和设备的屏幕桥接起来，但是需要指定一些配置项，类似于色彩格式、像素格式、RGBA的表示以及SurfaceType等 //2 surface窗口配置 EGLConfig config; EGLint configNum; EGLint attribs[] = { EGL_SURFACE_TYPE,EGL_WINDOW_BIT, EGL_BUFFER_SIZE, 24, EGL_RED_SIZE,8, EGL_GREEN_SIZE, 8, EGL_BLUE_SIZE, 8, EGL_DEPTH_SIZE, 16, EGL_NONE }; if(EGL_TRUE != eglChooseConfig(display, attribs, &config, 1, &configNum)){ LOGE(\"eglChooseConfig failed!\"); return; } //3. 创建surface，将EGL和设备的屏幕连接起来 EGLSurface surface = NULL; EGLint format; if (!eglGetConfigAttrib(display, config, EGL_NATIVE_VISUAL_ID, &format)) { LOGE(\"eglGetConfigAttrib() returned error %d\", eglGetError()); return; } ANativeWindow_setBuffersGeometry(window, 0, 0, format); if (!(surface = eglCreateWindowSurface(display, config, window, 0))) { LOGE(\"eglCreateWindowSurface() returned error %d\", eglGetError()); } //4 context 创建关联的上下文 const EGLint ctxAttr[] = { EGL_CONTEXT_CLIENT_VERSION, 2, EGL_NONE }; EGLContext context = eglCreateContext(display, config, EGL_NO_CONTEXT, ctxAttr); if(context == EGL_NO_CONTEXT){ LOGE(\"eglCreateContext failed!\"); return; } //5. 进行绑定 if(EGL_TRUE != eglMakeCurrent(display, surface, surface, context)){ LOGE(\"eglMakeCurrent failed！\"); return; } LOGD(\"NativeEngine: EGL finish, and creating OpenGL.\"); /*********************************************************/ /******************* OpenGL ********************/ /*********************************************************/ // 版本查看NDK Demo const GLchar *VERTEX_SHADER = \"#version 300 es\\n\" \"layout (location = 0) in vec3 position;\\n\" \"layout (location = 1) in vec3 color;\\n\" \"layout (location = 2) in vec2 texCoord;\\n\" \"out vec3 ourColor;\\n\" \"out vec2 TexCoord;\\n\" \"void main(){\\n\" \" gl_Position = vec4(position, 1.0f);\\n\" \" ourColor = color;\\n\" \" TexCoord = texCoord;\\n\" \"}\\n\"; //precision mediump float; 是从报错日志查出来的 const GLchar *FRAGMENT_SHADER = \"#version 300 es\\n\" \"precision mediump float;\\n\" \"in vec3 ourColor;\\n\" \"in vec2 TexCoord;\\n\" \"out vec4 color;\\n\" \"uniform sampler2D ourTexture;\\n\" \"void main(){\\n\" \" color = texture(ourTexture, TexCoord);\\n\" \"}\\n\"; const int pixel_w = 384, pixel_h = 216; unsigned char *buffer = new unsigned char[pixel_w * pixel_h * 3]; FILE *fp = fopen(filename, \"rb+\"); fread(buffer, pixel_w * pixel_h * 3, 1, fp); // Define the viewport dimensions //此处应该感觉屏幕实际大小进行处理 glViewport(80, 300, pixel_w*2, pixel_h*2); // Build and compile our shader program GLuint vertexShader = glCreateShader(GL_VERTEX_SHADER); glShaderSource(vertexShader, 1, &VERTEX_SHADER, NULL); glCompileShader(vertexShader); //以下为异常情况处理，打印编译错误（如果有的话） GLint success; GLchar infoLog[512]; glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(vertexShader, 512, NULL, infoLog); LOGE(\"ERROR::SHADER::VERTEX::COMPILATION_FAILED1 %s\\n\",infoLog); } GLuint fragmentShader = glCreateShader(GL_FRAGMENT_SHADER); glShaderSource(fragmentShader, 1, &FRAGMENT_SHADER, NULL); glCompileShader(fragmentShader); //以下为异常情况处理，打印编译错误（如果有的话） glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(fragmentShader, 512, NULL, infoLog); LOGE(\"ERROR::SHADER::VERTEX::COMPILATION_FAILED2 %s\\n\",infoLog); } GLuint shaderProgram = glCreateProgram(); glAttachShader(shaderProgram, vertexShader); glAttachShader(shaderProgram, fragmentShader); glLinkProgram(shaderProgram); glDeleteShader(vertexShader); glDeleteShader(fragmentShader); // Set up vertex data (and buffer(s)) and attribute pointers GLfloat vertices[] = { // ---- 位置 ---- ---- 颜色 ---- - 纹理坐标 - 1.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 0.0f, // 右上 1.0f, -1.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 1.0f, // 右下 -1.0f, -1.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, // 左下 -1.0f, 1.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, 0.0f // 左上 }; //从0开始，画两个三角形，组成矩形（从0开始，画两个三角形的索引即可） GLuint indices[] = { // Note that we start from 0! 0, 1, 3, // First Triangle 1, 2, 3 // Second Triangle }; GLuint VBO, VAO, EBO; // 1. 绑定顶点数组对象 glGenVertexArrays(1, &VAO); glGenBuffers(1, &VBO); glGenBuffers(1, &EBO); glBindVertexArray(VAO); glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW); // Position attribute glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid *) 0); glEnableVertexAttribArray(0); // Color attribute glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid *) (3 * sizeof(GLfloat))); glEnableVertexAttribArray(1); // TexCoord attribute glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid *) (6 * sizeof(GLfloat))); glEnableVertexAttribArray(2); glBindVertexArray(0); // Unbind VAO // Load and create a texture GLuint texture; glGenTextures(1, &texture); glBindTexture(GL_TEXTURE_2D, texture); // All upcoming GL_TEXTURE_2D operations now have effect on this texture object // Set the texture wrapping parameters glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); // Set texture wrapping to GL_REPEAT (usually basic wrapping method) glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); // Set texture filtering parameters glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); // Load image, create texture and generate mipmaps //就让他一直显示吧 while (1){ // Render // Clear the colorbuffer glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // Bind Texture glBindTexture(GL_TEXTURE_2D, texture); glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, pixel_w, pixel_h, 0, GL_RGB, GL_UNSIGNED_BYTE, buffer); // Activate shader glUseProgram(shaderProgram); // Draw container glBindVertexArray(VAO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); glBindVertexArray(0); eglSwapBuffers(display,surface); sleep(0.5); } // Properly de-allocate all resources once they've outlived their purpose glDeleteVertexArrays(1, &VAO); glDeleteBuffers(1, &VBO); glDeleteBuffers(1, &EBO); //省略释放内存等..... } Android项目地址：OpenGLESDemo Mac项目地址：OpenGLMacTextureDemo 参考 https://en.wikipedia.org/wiki/OpenSL_ES https://blog.csdn.net/leixiaohua1020/article/details/40379845 https://github.com/android/ndk-samples/tree/main/audio-echo https://blog.csdn.net/ywl5320/article/details/78503768 http://web.cse.ohio-state.edu/~shen.94/781/Site/Slides_files/pipeline.pdf Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-09 16:31:16 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/03_mediacodec.html":{"url":"AFPlayer/03_mediacodec.html","title":"FFmpeg实现MediaCodec解码","keywords":"","body":"FFmpeg实现MediaCodec解码简述实现流程一、编译的FFmpeg二、导入三、代码实现输出验证FFmpeg实现MediaCodec解码 简述 MediaCodec是Android平台的硬件编解码，FFmpeg从3.1版本开始支持，但是，目前支持实现解码功能。官网硬件编解码支持结束 实现流程 一、编译的FFmpeg 为了快速编译，以及减少体积，这里关闭了大部分功能。 #!/bin/bash API=21 NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 function build_android() { ./configure \\ --prefix=$PREFIX \\ --disable-opencl \\ --disable-doc \\ --disable-everything \\ --disable-htmlpages \\ --disable-podpages \\ --disable-debug \\ --disable-programs \\ --disable-demuxers \\ --disable-muxers \\ --disable-decoders \\ --disable-encoders \\ --disable-bsfs \\ --disable-indevs \\ --disable-outdevs \\ --disable-filters \\ --disable-protocols \\ --disable-hwaccels \\ --disable-avdevice \\ --disable-postproc \\ --disable-devices \\ --disable-symver \\ --disable-stripping \\ --disable-asm \\ --disable-w32threads \\ --disable-parsers \\ --disable-shared \\ --enable-static \\ --enable-swscale \\ --enable-protocol=file \\ --enable-protocol=hls \\ --enable-protocol=rtmp \\ --enable-protocol=http \\ --enable-protocol=tls \\ --enable-protocol=https \\ --enable-demuxer=aac \\ --enable-demuxer=h264 \\ --enable-demuxer=mov \\ --enable-demuxer=flv \\ --enable-demuxer=avi \\ --enable-demuxer=hevc \\ --enable-demuxer=gif \\ --enable-parser=h264 \\ --enable-parser=hevc \\ --enable-parser=aac \\ --enable-decoder=aac \\ --enable-decoder=h264 \\ --enable-decoder=flv \\ --enable-decoder=gif \\ --enable-decoder=mpeg4 \\ --enable-bsf=aac_adtstoasc \\ --enable-bsf=h264_mp4toannexb \\ --enable-jni \\ --enable-mediacodec \\ --enable-decoder=h264_mediacodec \\ --enable-decoder=hevc_mediacodec \\ --enable-decoder=mpeg4_mediacodec \\ --enable-hwaccel=h264_mediacodec \\ --enable-neon \\ --enable-gpl \\ --cross-prefix=$CROSS_PREFIX \\ --target-os=android \\ --arch=$ARCH \\ --cpu=$CPU \\ --cc=$CC \\ --cxx=$CXX \\ --enable-cross-compile \\ --sysroot=$SYSROOT \\ --extra-cflags=\"-Os -fpic $OPTIMIZE_CFLAGS\" \\ --extra-ldflags=\"$ADDI_LDFLAGS\" || exit 1 make clean make -j8 make install } #armv8-a ARCH=arm64 CPU=armv8-a CC=$TOOLCHAIN/bin/aarch64-linux-android$API-clang CXX=$TOOLCHAIN/bin/aarch64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/aarch64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU\" #build_android #armv7-a ARCH=arm CPU=armv7-a CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU \" build_android #x86 ARCH=x86 CPU=x86 CC=$TOOLCHAIN/bin/i686-linux-android$API-clang CXX=$TOOLCHAIN/bin/i686-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/i686-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=i686 -mtune=intel -mssse3 -mfpmath=sse -m32\" #build_android #x86_64 ARCH=x86_64 CPU=x86-64 CC=$TOOLCHAIN/bin/x86_64-linux-android$API-clang CXX=$TOOLCHAIN/bin/x86_64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/x86_64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU -msse4.2 -mpopcnt -m64 -mtune=intel\" #build_android 二、导入 将FFmpeg到Android Studio项目中，编辑CMakeList.txt如下： #1、添加版本 cmake_minimum_required(VERSION 3.4.1) include_directories(include) file(GLOB core_srcs ${CMAKE_SOURCE_DIR}/core/*.cpp) #2、编译库，把native-lib.cpp编译成以native-lib为名称的动态库 add_library( native-lib SHARED player_main.cpp ${core_srcs}) #3、设置C++编译参数（CMAKE_CXX_FLAGS是全局变量） #添加其他预编译库还可以使用这种方式 #使用-L指导编译时库文件的查找路径 #cxx_flags \"cxx_flags -L目录\" set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -L${CMAKE_SOURCE_DIR}/libs/${CMAKE_ANDROID_ARCH_ABI}\") #4、链接到库文件，jni/c/c++可以引入链接到 target_link_libraries( native-lib log # 先把有依赖的库，先依赖进来；注意顺序。 avformat avcodec avfilter avutil swresample swscale z ) 目录结构见底部项目Demo。 三、代码实现 1）给FFmpeg设置JavaVM extern \"C\" JNIEXPORT jint JNI_OnLoad(JavaVM *vm, void *res) { av_jni_set_java_vm(vm, 0); // 返回jni版本 return JNI_VERSION_1_4; } 2）API流程 3）关键代码 /** * description: * @author 秦城季 * @email xhunmon@126.com * @blog https://qincji.gitee.io * @date 2021/2/1 */ #include \"hw_mediacodec.h\" static AVBufferRef *hw_device_ctx = NULL; static enum AVPixelFormat hw_pix_fmt; static FILE *output_file = NULL; static int hw_decoder_init(AVCodecContext *ctx, const enum AVHWDeviceType type) { int err = 0; if ((err = av_hwdevice_ctx_create(&hw_device_ctx, type, NULL, NULL, 0)) hw_device_ctx = av_buffer_ref(hw_device_ctx); return err; } static enum AVPixelFormat get_hw_format(AVCodecContext *ctx, const enum AVPixelFormat *pix_fmts) { const enum AVPixelFormat *p; for (p = pix_fmts; *p != -1; p++) { if (*p == hw_pix_fmt) return *p; } HWLOGE(\"Failed to get HW surface format.\\n\"); return AV_PIX_FMT_NONE; } static int decode_write(AVCodecContext *avctx, AVPacket *packet) { AVFrame *frame = NULL, *sw_frame = NULL; AVFrame *tmp_frame = NULL; uint8_t *buffer = NULL; int size; int ret = 0; ret = avcodec_send_packet(avctx, packet); if (ret format == hw_pix_fmt) { if ((ret = av_hwframe_transfer_data(sw_frame, frame, 0)) (tmp_frame->format), tmp_frame->width, tmp_frame->height, 1); buffer = static_cast(av_malloc(size)); if (!buffer) { HWLOGE(\"Can not alloc buffer\\n\"); ret = AVERROR(ENOMEM); goto fail; } //本次测试所得：AV_PIX_FMT_NV12 ret = av_image_copy_to_buffer(buffer, size, (const uint8_t *const *) tmp_frame->data, (const int *) tmp_frame->linesize, static_cast(tmp_frame->format), tmp_frame->width, tmp_frame->height, 1); if (ret name, av_hwdevice_get_type_name(type)); return; } if (config->methods & AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX && config->device_type == type) { hw_pix_fmt = config->pix_fmt; break; } } if (!(decoder_ctx = avcodec_alloc_context3(decoder))) { HWLOGE(\"Cannot find a video stream in the input file%d\", AVERROR(ENOMEM)); return; } //将参数赋值给新的编码器 if ((ret = avcodec_parameters_to_context(decoder_ctx, input_ctx->streams[video_stream]->codecpar)) get_format = get_hw_format; if (hw_decoder_init(decoder_ctx, type) = 0) { if ((ret = av_read_frame(input_ctx, &packet)) 输出验证 上面顺利输出后的文件格式是nv12的，我们使用ffplay进行播放如下： ffplay -f rawvideo -video_size 384x216 -pixel_format nv12 -framerate 15 Kobe-384x216.yuv Demo地址：FFmpegMediaCodecDemo Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-09 16:31:16 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/04_build_for_android.html":{"url":"AFPlayer/04_build_for_android.html","title":"FFmpeg、x264、fdk-aac、openssl的交叉编译","keywords":"","body":"FFmpeg、x264、fdk-aac、openssl的交叉编译简述X264重要参数X264在NDK17使用gcc编译X264在NDK21使用clang编译Fdk-aac编译前的操作Fdk-aac在NDK17使用gcc编译Fdk-aac在NDK21使用clang编译OpenSSL重要参数OpenSSL在NDK17使用gcc编译OpenSSL在NDK21使用clang编译FFmpeg方案一：使用gcc进行编译FFmpeg在NDK21使用clang编译导入到Android Studio中进行验证参考- 0基础学习音视频路线，以及重磅音视频资料下载 求赞。FFmpeg、x264、fdk-aac、openssl的交叉编译 简述 以前在Android进行交叉编译都是零零散散的，内容也不是很全，现在重新整理一遍，以便以后参考和移植。本文FFmpeg将集成x264、fdk aac以及openssl第三方库，以及开启mediacodec。 Android NDK在版本r17c之后弃用了gcc编译器，改用clang编译器。所以如果ndk在r17c及以下则是使用gcc进行编译。所以以下有两种ndk版本编译： NDK17使用gcc编译（FFmpeg最低支持API=21） NDK21使用clang编译（FFmpeg最低支持API更低，推荐使用） X264 在ffmpeg中使用的h264编码通常是使用x264库内置进去，因为x264的算法更优秀。所以需要把x264编译生成静态库后进行移植。 重要参数 --prefix：安装的目录 --cross-prefix：交叉编译的前缀路径，后面会拼接gcc、ld等 --sysroot：编译过程会去这个目录下找依赖相关的类等 --enable-pic：生成与路径无关静/动态库 --extra-cflags：编译过程需要传递的参数，相关参数参考 X264在NDK17使用gcc编译 ndk在r17c及以下则是使用gcc进行编译，编译脚本如下： #!/bin/sh NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c API=17 build_x264() { ./configure \\ --prefix=${PREFIX} \\ --cross-prefix=${CROSS_PREFIX} \\ --sysroot=${SYSROOT} \\ --host=${HOST} \\ --enable-shared \\ --disable-static \\ --enable-pic \\ --disable-cli \\ --extra-cflags=\"${EXTRA_CFLAGS}\" || exit 0 make clean make install } PREFIX=$(pwd)/android_r17c/armeabi-v7a HOST=arm-linux CROSS_PREFIX=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi- SYSROOT=$NDK/platforms/android-$API/arch-arm EXTRA_CFLAGS=\"-D__ANDROID_API__=$API -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" build_x264 注意：darwin-x86_64这是平台相关，这里表示MacOS系统，如果是Linux则是linux-x86_64，自己的请看NDK路径的。 X264在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 1）修改x264库跟目录下的configure文件 a. 找到CC=\"${CC-${cross_prefix}gcc}\"修改成CC=\"${CC-$CC_PATH}\"；这里的CC_PATH是我们需要添加到环境变量的clang编译器路径，居然看编译脚本。 b. 把全局的${cross_prefix}gcc-改成${cross_prefix}-；也就是把拼接中的gcc去掉。 2）看脚本文件： #!/bin/bash NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 API=21 build_x264() { ./configure \\ --prefix=${PREFIX} \\ --cross-prefix=${CROSS_PREFIX} \\ --sysroot=$SYSROOT \\ --host=${HOST} \\ --disable-asm \\ --enable-shared \\ --disable-static \\ --disable-opencl \\ --enable-pic \\ --disable-cli \\ --extra-cflags=\"$EXTRA_CFLAGS\" || exit 0 make clean make install } #armeabi-v7a PREFIX=$(pwd)/android_r21/armeabi-v7a HOST=arm-linux #把CC_PATH设置成环境变量，然后在configure中直接读取 export CC_PATH=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- #EXTRA_CFLAGS=\"-D__ANDROID_API__=$API -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" build_x264 Fdk-aac 重要参数跟x264的差不过的。 编译前的操作 将libfdkaac/libSBRdec/src/lpp_tran.cpp中的__ANDROID__改成__ANDROID_OFF__；也就是去掉log/log.h，因为ndk中没有这玩意。 Fdk-aac在NDK17使用gcc编译 ndk在r17c及以下则是使用gcc进行编译，编译脚本如下： #!/bin/bash NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c TOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 API=17 build_aac() { ./configure \\ --prefix=${PREFIX} \\ --host=${HOST} \\ --with-pic \\ --target=android \\ --disable-debug \\ --disable-asm \\ --enable-static=no \\ --enable-shared=yes \\ || exit 0 make clean make install } #armeabi-v7a PREFIX=$(pwd)/android_r17c/armeabi-v7a HOST=arm-linux-androideabi SYSROOT=$NDK/platforms/android-${API}/arch-arm CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- #以下都是设置环境变量形式就行传递的，查看方式：configure --help export CC=${CROSS_COMPILE}gcc export CPP=${CROSS_COMPILE}cpp export CXX=${CROSS_COMPILE}g++ export CFLAGS=\"-D__ANDROID_API__=$API --sysroot=${SYSROOT} -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" #-D__ANDROID__=OFF 取消宏定义都没用，也会被覆盖 export CPPFLAGS=\"${CFLAGS}\" export CXXFLAGS=\"${CFLAGS}\" export AR=${CROSS_COMPILE}ar export AS=${CROSS_COMPILE}as export LD=${CROSS_COMPILE}ld export RANLIB=${CROSS_COMPILE}ranlib export STRIP=${CROSS_COMPILE}strip build_aac Fdk-aac在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 #!/bin/bash NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 API=21 build_aac() { ./configure \\ --prefix=${PREFIX} \\ --host=${HOST} \\ --with-pic \\ --target=android \\ --disable-debug \\ --disable-asm \\ --enable-static=no \\ --enable-shared=yes \\ CPPFLAGS=\"-fPIC\" || exit 0 make clean make install } #armeabi-v7a PREFIX=$(pwd)/android_r21/armeabi-v7a HOST=arm-linux-androideabi export AR=$TOOLCHAIN/bin/arm-linux-androideabi-ar export AS=$TOOLCHAIN/bin/arm-linux-androideabi-as export LD=$TOOLCHAIN/bin/arm-linux-androideabi-ld export RANLIB=$TOOLCHAIN/bin/arm-linux-androideabi-ranlib export STRIP=$TOOLCHAIN/bin/arm-linux-androideabi-strip export CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang export CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ build_aac OpenSSL 重要参数 --prefix：安装的目录 ANDROID_NDK：设置环境变量，名称是不能改变的 -D__ANDROID_API__=$API：指定android版本 android-arm：生成架构类型，看官网 生成动态库时需要把.so.xx中的.xx放到前面来，因为android加载动态库时不支持；操作方式：直接全局搜索.so.，然后把后面的放到前面来，如： '.so.$(SHLIB_VERSION_NUMBER)' --> '.$(SHLIB_VERSION_NUMBER).so' 注意：其他的环境变量没有过多的验证 OpenSSL在NDK17使用gcc编译 ndk在r17c及以下则是使用gcc进行编译，编译脚本如下： #!/bin/sh API=17 export ANDROID_NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c export PATH=$ANDROID_NDK/toolchains/llvm/prebuilt/darwin-x86_64/bin:$ANDROID_NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin:$PATH export SYSROOT=$ANDROID_NDK/platforms/android-${API}/arch-arm export TOOLCHAIN=$ANDROID_NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- #以下都是设置环境变量形式就行传递的，查看方式：configure --help export CC=\"${CROSS_COMPILE}gcc\" export CFLAGS=\"-D__ANDROID_API__=$API -isysroot $ANDROID_NDK/sysroot -I$ANDROID_NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" export STRIP=\"${CROSS_COMPILE}strip\" export RANLIB=\"${CROSS_COMPILE}ranlib\" export AR=\"${CROSS_COMPILE}ar\" export LD=\"${CROSS_COMPILE}ld\" export AS=\"${CROSS_COMPILE}as\" ./Configure android-arm -D__ANDROID_API__=$API --prefix=$(pwd)/android_r17c/armeabi-v7a make clean make -j8 make install OpenSSL在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 API=21 export ANDROID_NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 export PATH=$ANDROID_NDK/toolchains/llvm/prebuilt/darwin-x86_64/bin:$ANDROID_NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin:$PATH export SYSROOT=$ANDROID_NDK/platforms/android-${API}/arch-arm export TOOLCHAIN=$ANDROID_NDK/toolchains/llvm/prebuilt/darwin-x86_64/bin CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- #以下都是设置环境变量形式就行传递的，查看方式：configure --help export CC=\"${CROSS_COMPILE}gcc\" export CFLAGS=\"-D__ANDROID_API__=$API -isysroot $ANDROID_NDK/sysroot -I$ANDROID_NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" export STRIP=\"${CROSS_COMPILE}strip\" export RANLIB=\"${CROSS_COMPILE}ranlib\" export AR=\"${CROSS_COMPILE}ar\" export LD=\"${CROSS_COMPILE}ld\" export AS=\"${CROSS_COMPILE}as\" ./Configure android-arm -D__ANDROID_API__=$API --prefix=$(pwd)/android_r21/armeabi-v7a make clean make -j8 make install FFmpeg 为了减少编译的时间以及减少包的体积，这里关闭了很多的参数。 注意： 1）启用openssl时需要开启--pkg-config=pkg-config，不然会找不着库。 2）使用动态库形式进行引入，不然会报找不到第三库的文件，原因未知。 方案一：使用gcc进行编译 #!/bin/bash #报错了，哈哈 #libavcodec/v4l2_buffers.c:434:44: error: call to 'mmap' declared with attribute error: mmap is not available with _FILE_OFFSET_BITS=64 when using GCC until android-21. Either raise your minSdkVersion, disable _FILE_OFFSET_BITS=64, or switch to Clang. API=21 export NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c export SYSROOT=$NDK/platforms/android-$API/arch-arm export TOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 X264_PREFIX=$(pwd)/libx264/android_r17c/armeabi-v7a X264_INCLUDE=${X264_PREFIX}/include X264_LIB=${X264_PREFIX}/lib FDK_AAC_PREFIX=$(pwd)/libfdkaac/android_r17c/armeabi-v7a FDK_AAC_INCLUDE=${FDK_AAC_PREFIX}/include FDK_AAC_LIB=${FDK_AAC_PREFIX}/lib OPENSSL_PREFIX=$(pwd)/libopenssl/android_r17c/armeabi-v7a OPENSSL_INCLUDE=${OPENSSL_PREFIX}/include OPENSSL_LIB=${OPENSSL_PREFIX}/lib //这里参照下面即可 ARCH=arm CPU=armv7-a CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- CC=${CROSS_COMPILE}gcc CXX=${CROSS_COMPILE}g++ CROSS_PREFIX=${CROSS_COMPILE} PREFIX=$(pwd)/android_r17c/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU\" CFLAG=\"-I${X264_INCLUDE} -I${FDK_AAC_INCLUDE} -I${OPENSSL_INCLUDE} -D__ANDROID_API__=$API --sysroot=${SYSROOT} -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC $OPTIMIZE_CFLAGS\" build_android FFmpeg在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 #!/bin/bash API=21 NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 X264_PREFIX=$(pwd)/libx264/android_r21/armeabi-v7a X264_INCLUDE=${X264_PREFIX}/include X264_LIB=${X264_PREFIX}/lib FDK_AAC_PREFIX=$(pwd)/libfdkaac/android_r21/armeabi-v7a FDK_AAC_INCLUDE=${FDK_AAC_PREFIX}/include FDK_AAC_LIB=${FDK_AAC_PREFIX}/lib OPENSSL_PREFIX=$(pwd)/libopenssl/android_r21/armeabi-v7a OPENSSL_INCLUDE=${OPENSSL_PREFIX}/include OPENSSL_LIB=${OPENSSL_PREFIX}/lib function build_android() { ./configure \\ --prefix=$PREFIX \\ --disable-opencl \\ --disable-doc \\ --disable-everything \\ --disable-htmlpages \\ --disable-podpages \\ --disable-debug \\ --disable-programs \\ --disable-demuxers \\ --disable-muxers \\ --disable-decoders \\ --disable-encoders \\ --disable-bsfs \\ --disable-indevs \\ --disable-outdevs \\ --disable-filters \\ --disable-protocols \\ --disable-hwaccels \\ --disable-avdevice \\ --disable-postproc \\ --disable-devices \\ --disable-symver \\ --disable-stripping \\ --disable-asm \\ --disable-w32threads \\ --disable-parsers \\ --disable-static \\ --enable-shared \\ --enable-openssl \\ --pkg-config=pkg-config \\ --enable-gpl \\ --enable-nonfree \\ --enable-libx264 \\ --enable-encoder=libx264 \\ --enable-swscale \\ --enable-protocol=file \\ --enable-protocol=hls \\ --enable-protocol=rtmp \\ --enable-protocol=http \\ --enable-protocol=tls \\ --enable-protocol=https \\ --enable-demuxer=aac \\ --enable-demuxer=h264 \\ --enable-demuxer=mov \\ --enable-demuxer=flv \\ --enable-demuxer=avi \\ --enable-demuxer=hevc \\ --enable-demuxer=gif \\ --enable-parser=h264 \\ --enable-parser=hevc \\ --enable-parser=aac \\ --enable-decoder=aac \\ --enable-decoder=h264 \\ --enable-decoder=flv \\ --enable-decoder=gif \\ --enable-decoder=mpeg4 \\ --enable-bsf=aac_adtstoasc \\ --enable-bsf=h264_mp4toannexb \\ --enable-jni \\ --enable-mediacodec \\ --enable-decoder=h264_mediacodec \\ --enable-hwaccel=h264_mediacodec \\ --enable-decoder=hevc_mediacodec \\ --enable-decoder=mpeg4_mediacodec \\ --enable-decoder=vp8_mediacodec \\ --enable-decoder=vp9_mediacodec \\ --enable-neon \\ --enable-gpl \\ --cross-prefix=$CROSS_PREFIX \\ --target-os=android \\ --arch=$ARCH \\ --cpu=$CPU \\ --cc=$CC \\ --cxx=$CXX \\ --enable-cross-compile \\ --sysroot=$SYSROOT \\ --extra-cflags=\"-I${X264_INCLUDE} -I${FDK_AAC_INCLUDE} -I${OPENSSL_INCLUDE} -Os -fpic $OPTIMIZE_CFLAGS\" \\ --extra-ldflags=\"-L${X264_LIB} -L${FDK_AAC_LIB} -L${OPENSSL_LIB} $ADDI_LDFLAGS\" || exit 1 make clean make -j8 make install } #armv8-a ARCH=arm64 CPU=armv8-a CC=$TOOLCHAIN/bin/aarch64-linux-android$API-clang CXX=$TOOLCHAIN/bin/aarch64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/aarch64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU\" #build_android #armv7-a ARCH=arm CPU=armv7-a CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- PREFIX=$(pwd)/android-so/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU \" build_android #x86 ARCH=x86 CPU=x86 CC=$TOOLCHAIN/bin/i686-linux-android$API-clang CXX=$TOOLCHAIN/bin/i686-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/i686-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=i686 -mtune=intel -mssse3 -mfpmath=sse -m32\" #build_android #x86_64 ARCH=x86_64 CPU=x86-64 CC=$TOOLCHAIN/bin/x86_64-linux-android$API-clang CXX=$TOOLCHAIN/bin/x86_64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/x86_64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU -msse4.2 -mpopcnt -m64 -mtune=intel\" #build_android 导入到Android Studio中进行验证 1）在app级的build.gradle中设置动态库目录： android { defaultConfig { externalNativeBuild { cmake { cppFlags \"\" abiFilters 'armeabi-v7a' } } } externalNativeBuild { cmake { path \"src/main/cpp/CMakeLists.txt\" // version \"3.10.2\" //版本过高导致无日志输出 version \"3.6.0\" } } //配置第三方so路径 sourceSets { main { jniLibs.srcDirs = ['src/main/cpp/libs'] } } } dependencies { } 2）以下为CMakeList.txt导入方式： #1、添加版本 cmake_minimum_required(VERSION 3.4.1) include_directories(include) file(GLOB core_srcs ${CMAKE_SOURCE_DIR}/core/*.cpp) #2、编译库，把native-lib.cpp编译成以native-lib为名称的动态库 add_library( native-lib SHARED player_main.cpp ${core_srcs}) #3、设置C++编译参数（CMAKE_CXX_FLAGS是全局变量） #添加其他预编译库还可以使用这种方式 #使用-L指导编译时库文件的查找路径 #cxx_flags \"cxx_flags -L目录\" set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -L${CMAKE_SOURCE_DIR}/libs/${CMAKE_ANDROID_ARCH_ABI}\") #4、链接到库文件，jni/c/c++可以引入链接到 target_link_libraries( native-lib log # 先把有依赖的库，先依赖进来；注意顺序。 avformat avcodec avfilter avutil swresample swscale x264.161 fdk-aac ssl.3 crypto.3 #openssl z ) 3）在java类中进行加载 static { //动态库的操作 avformat avcodec avfilter avutil swresample swscale System.loadLibrary(\"avformat\"); System.loadLibrary(\"avcodec\"); System.loadLibrary(\"avfilter\"); System.loadLibrary(\"avutil\"); System.loadLibrary(\"swresample\"); System.loadLibrary(\"swscale\"); System.loadLibrary(\"x264.161\"); System.loadLibrary(\"fdk-aac\"); System.loadLibrary(\"ssl.3\"); System.loadLibrary(\"crypto.3\"); System.loadLibrary(\"native-lib\"); } 具体的请看Demo地址：FFmpegImportDemo 参考 https://github.com/taoliuh/ffmpeg-build-tools https://github.com/ShikinChen/android_ffmpeg https://github.com/hilive/build-script https://github.com/lllkey/android-openssl-build https://www.jianshu.com/p/f98db1d84d93 https://www.jianshu.com/p/f98db1d84d93 - 0基础学习音视频路线，以及重磅音视频资料下载 求赞。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/":{"url":"FFmpeg/","title":"FFmpeg","keywords":"","body":"FFmpeg简述FFmpeg的重要知识点FFmpeg 简述 音视频开发是绕不开FFmpeg的，因为它是一个\"集大成者\"，里面已经包含或可集成现代几乎所有的音视频技术（库）。 FFmpeg的重要知识点 Configure配置选项 (转载) Shell脚本 编译FFmpeg4.2.2 FFmpeg导入到Clion（MacOS） 使用Clion阅读FFmpeg源码（支持跳转） FFmpeg重要结构体（转载） Demuxing（解封装） Muxing（封装） Remuxing（重新封装） Decode（解码） Encode（编码） 简单实现转码 Filter和SDL（Video） Filter和SDL（Audio） Transcode(转码) 音视频同步处理 FFmpeg命令使用指南 Swscale（图像转换） Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:24:22 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/01_configure_help.html":{"url":"FFmpeg/01_configure_help.html","title":"Configure配置选项 (转载)","keywords":"","body":"Configure配置选项 (转载)帮助选项（Help options）标准选项（Standard options）许可证选项（Licensing options）配置选项（Configuration options）程序选项（Program options）文档选项（Documentation options）组件选项（Component options）个别组件选项（Individual component options）扩展库支持（External library support）硬件加速功能（hardware acceleration features）工具链选项（Toolchain options）高级选项（Advanced options）优化选项（Optimization options）开发者选项（Developer options）Configure配置选项 (转载) 帮助选项Help options 标准选项Standard options 许可证选项Licensing options 配置选项Configuration options 程序选项Program options 文档选项Documentation options 组件选项Component options 个别组件选项Individual component options 扩展库支持External library support 硬件加速功能hardware acceleration features 工具链选项Toolchain options 高级选项Advanced options 优化选项Optimization options 开发者选项Developer options 帮助选项（Help options） 用于查看ffmpeg的能力 选项 说明 –help print this message –list-decoders 显示所有可用的解码器（h264/mjpeg等） –list-encoders 显示所有可用的编码器（h264/mjpeg等） –list-hwaccels 显示所有支持的硬编解码器（h264_videotoolbox/h264_mediacodec等） –list-demuxers 显示所有支持解复用的容器（mp4/h264等） –list-muxers 显示所有支持复用的容器（mp4/h264等） –list-parsers show all available parsers –list-protocols 显示所有支持的传输协议（rtmp/rtp等） –list-bsfs 显示所有可用的格式转换（h264_mp4toannexb/aac_adtstoasc等） –list-indevs 显示所有支持的输入设备（alsa/v4l2等） –list-outdevs 显示所有支持的输出设备（alsa/opengl等） –list-filters 显示支持的所有过滤器（scale/volume/fps/allyuv等） 标准选项（Standard options） 编译配置 选项 说明 –logfile=FILE 配置过程中的log输出文件，默认输出到当前位置的ffbuild/config.log文件 –disable-logging 配置过程中不输出日志 –fatal-warnings 把配置过程中的任何警告当做致命的错误处理 –prefix=PREFIX 设定安装的跟目录，如果不指定默认是/usr –bindir=DIR 设置可执行程序的安装位置，默认是[PREFIX/bin] –datadir=DIR 设置测试程序以及数据的安装位置，默认是[PREFIX/share/ffmpeg] –docdir=DIR 设置文档的安装目录，默认是[PREFIX/share/doc/ffmpeg] –libdir=DIR 设置静态库的安装位置，默认是[PREFIX/lib] –shlibdir=DIR 设置动态库的安装位置，默认是[LIBDIR] –incdir=DIR 设置头文件的安装位置，默认是[PREFIX/include]；一般来说用于依赖此头文件来开发就够了 –mandir=DIR 设置man文件的安装目录，默认是[PREFIX/share/man] –pkgconfigdir=DIR 设置pkgconfig的安装目录，默认是[LIBDIR/pkgconfig]，只有--enable-shared使能的时候这个选项才有效 –enable-rpath use rpath to allow installing libraries in paths not part of the dynamic linker search path use rpath when linking programs [USE WITH CARE] –install-name-dir=DIR Darwin directory name for installed targets 许可证选项（Licensing options） 选择许可证，FFMPEG默认许可证LGPL 2.1，如果需要加gpl的库需要使用gpl的许可证，例如libx264就是gpl的，如果需要加入libx264则需要--enable-gpl。 选项 说明 –enable-gpl allow use of GPL code, the resulting libs and binaries will be under GPL [no] –enable-version3 upgrade (L)GPL to version 3 [no] –enable-nonfree allow use of nonfree code, the resulting libs and binaries will be unredistributable [no] 配置选项（Configuration options） 编译选项的配置 选项 说明 –disable-static 不生产静态库，默认生成静态库 –enable-shared 生成动态库，默认不生成动态库 –enable-small optimize for size instead of speed，默认开启 –disable-runtime-cpudetect disable detecting cpu capabilities at runtime (smaller binary)，默认开启 –enable-gray enable full grayscale support (slower color) –disable-swscale-alpha disable alpha channel support in swscale –disable-all 禁止编译所有库和可执行程序 –enable-raise-major 增加主版本号 程序选项（Program options） 可执行程序开启选项，默认编译ffmpeg中的所有可执行程序，包括ffmpeg、ffplay、ffprobe、ffserver，不过Mac平台默认情况下不生成ffplay，目前暂未知道啥原因。 选项 说明 –disable-programs do not build command line programs –disable-ffmpeg disable ffmpeg build –disable-ffplay disable ffplay build –disable-ffprobe disable ffprobe build –disable-ffserver disable ffserver build 文档选项（Documentation options） 离线文档选择 选项 说明 –disable-doc do not build documentation –disable-htmlpages do not build HTML documentation pages –disable-manpages do not build man documentation pages –disable-podpages do not build POD documentation pages –disable-txtpages do not build text documentation pages 组件选项（Component options） 除了avresample模块，默认编译所有模块。一般来说用于轻量化ffmpeg库的大小，可以仅仅开启指定某些组件的某些功能。 选项 说明 –disable-avdevice disable libavdevice build –disable-avcodec disable libavcodec build –disable-avformat disable libavformat build –disable-swresample disable libswresample build –disable-swscale disable libswscale build –disable-postproc disable libpostproc build –disable-avfilter disable libavfilter build –enable-avresample enable libavresample build [no] –disable-pthreads disable pthreads [autodetect] –disable-w32threads disable Win32 threads [autodetect] –disable-os2threads disable OS/2 threads [autodetect] –disable-network disable network support [no] –disable-dct disable DCT code –disable-dwt disable DWT code –disable-error-resilience disable error resilience code –disable-lsp disable LSP code –disable-lzo disable LZO decoder code –disable-mdct disable MDCT code –disable-rdft disable RDFT code –disable-fft disable FFT code –disable-faan disable floating point AAN (I)DCT code –disable-pixelutils disable pixel utils in libavutil 个别组件选项（Individual component options） 可以用于设定开启指定功能，例如禁止所有encoders，在这里可以开启特定的encoders（x264、aac等） 选项 说明 –disable-everything disable all components listed below –disable-encoder=NAME disable encoder NAME –enable-encoder=NAME enable encoder NAME –disable-encoders disable all encoders –disable-decoder=NAME disable decoder NAME –enable-decoder=NAME enable decoder NAME –disable-decoders disable all decoders –disable-hwaccel=NAME disable hwaccel NAME –enable-hwaccel=NAME enable hwaccel NAME –disable-hwaccels disable all hwaccels –disable-muxer=NAME disable muxer NAME –enable-muxer=NAME enable muxer NAME –disable-muxers disable all muxers –disable-demuxer=NAME disable demuxer NAME –enable-demuxer=NAME enable demuxer NAME –disable-demuxers disable all demuxers –enable-parser=NAME enable parser NAME –disable-parser=NAME disable parser NAME –disable-parsers disable all parsers –enable-bsf=NAME enable bitstream filter NAME –disable-bsf=NAME disable bitstream filter NAME –disable-bsfs disable all bitstream filters –enable-protocol=NAME enable protocol NAME –disable-protocol=NAME disable protocol NAME –disable-protocols disable all protocols –enable-indev=NAME enable input device NAME –disable-indev=NAME disable input device NAME –disable-indevs disable input devices –enable-outdev=NAME enable output device NAME –disable-outdev=NAME disable output device NAME –disable-outdevs disable output devices –disable-devices disable all devices –enable-filter=NAME enable filter NAME –disable-filter=NAME disable filter NAME –disable-filters disable all filters 扩展库支持（External library support） ffmpeg提供的一些功能是由其他扩展库支持的，如果需要使用需要明确声明，确定编译的第三方库的目标架构--arch相同就好了，在编译ffmpeg的时候需加入第三方库的头文和库搜索路径（通过extra-cflags和extra-ldflags指定即可），剩下的事ffmpeg都给你做好了。 libx264例子 ffmpeg集成libx264官方文档 $ git clone http://source.ffmpeg.org/git/ffmpeg.git $ cd x264 $ ./configure --prefix=$FFMPEG_PREFIX --enable-static --enable-shared $ make -j8 && make install 选项 说明 –enable-avisynth enable reading of AviSynth script files [no] –disable-bzlib disable bzlib [autodetect] –enable-chromaprint enable audio fingerprinting with chromaprint [no] –enable-frei0r enable frei0r video filtering [no] –enable-gcrypt enable gcrypt, needed for rtmp(t)e support if openssl, librtmp or gmp is not used [no] –enable-gmp enable gmp, needed for rtmp(t)e support if openssl or librtmp is not used [no] –enable-gnutls enable gnutls, needed for https support if openssl is not used [no] –disable-iconv disable iconv [autodetect] –enable-jni enable JNI support [no] –enable-ladspa enable LADSPA audio filtering [no] –enable-libass enable libass subtitles rendering, needed for subtitles and ass filter [no] –enable-libbluray enable BluRay reading using libbluray [no] –enable-libbs2b enable bs2b DSP library [no] –enable-libcaca enable textual display using libcaca [no] –enable-libcelt enable CELT decoding via libcelt [no] –enable-libcdio enable audio CD grabbing with libcdio [no] –enable-libdc1394 enable IIDC-1394 grabbing using libdc1394 and libraw1394 [no] –enable-libebur128 enable libebur128 for EBU R128 measurement, needed for loudnorm filter [no] –enable-libfdk-aac enable AAC de/encoding via libfdk-aac [no] –enable-libflite enable flite (voice synthesis) support via libflite [no] –enable-libfontconfig enable libfontconfig, useful for drawtext filter [no] –enable-libfreetype enable libfreetype, needed for drawtext filter [no] –enable-libfribidi enable libfribidi, improves drawtext filter [no] –enable-libgme enable Game Music Emu via libgme [no] –enable-libgsm enable GSM de/encoding via libgsm [no] –enable-libiec61883 enable iec61883 via libiec61883 [no] –enable-libilbc enable iLBC de/encoding via libilbc [no] –enable-libkvazaar enable HEVC encoding via libkvazaar [no] –enable-libmodplug enable ModPlug via libmodplug [no] –enable-libmp3lame enable MP3 encoding via libmp3lame [no] –enable-libnut enable NUT (de)muxing via libnut, native (de)muxer exists [no] –enable-libopencore-amrnb enable AMR-NB de/encoding via libopencore-amrnb [no] –enable-libopencore-amrwb enable AMR-WB decoding via libopencore-amrwb [no] –enable-libopencv enable video filtering via libopencv [no] –enable-libopenh264 enable H.264 encoding via OpenH264 [no] –enable-libopenjpeg enable JPEG 2000 de/encoding via OpenJPEG [no] –enable-libopenmpt enable decoding tracked files via libopenmpt [no] –enable-libopus enable Opus de/encoding via libopus [no] –enable-libpulse enable Pulseaudio input via libpulse [no] –enable-librubberband enable rubberband needed for rubberband filter [no] –enable-librtmp enable RTMP[E] support via librtmp [no] –enable-libschroedinger enable Dirac de/encoding via libschroedinger [no] –enable-libshine enable fixed-point MP3 encoding via libshine [no] –enable-libsmbclient enable Samba protocol via libsmbclient [no] –enable-libsnappy enable Snappy compression, needed for hap encoding [no] –enable-libsoxr enable Include libsoxr resampling [no] –enable-libspeex enable Speex de/encoding via libspeex [no] –enable-libssh enable SFTP protocol via libssh [no] –enable-libtesseract enable Tesseract, needed for ocr filter [no] –enable-libtheora enable Theora encoding via libtheora [no] –enable-libtwolame enable MP2 encoding via libtwolame [no] –enable-libv4l2 enable libv4l2/v4l-utils [no] –enable-libvidstab enable video stabilization using vid.stab [no] –enable-libvo-amrwbenc enable AMR-WB encoding via libvo-amrwbenc [no] –enable-libvorbis enable Vorbis en/decoding via libvorbis, native implementation exists [no] –enable-libvpx enable VP8 and VP9 de/encoding via libvpx [no] –enable-libwavpack enable wavpack encoding via libwavpack [no] –enable-libwebp enable WebP encoding via libwebp [no] –enable-libx264 enable H.264 encoding via x264 [no] –enable-libx265 enable HEVC encoding via x265 [no] –enable-libxavs enable AVS encoding via xavs [no] –enable-libxcb enable X11 grabbing using XCB [autodetect] –enable-libxcb-shm enable X11 grabbing shm communication [autodetect] –enable-libxcb-xfixes enable X11 grabbing mouse rendering [autodetect] –enable-libxcb-shape enable X11 grabbing shape rendering [autodetect] –enable-libxvid enable Xvid encoding via xvidcore, native MPEG-4/Xvid encoder exists [no] –enable-libzimg enable z.lib, needed for zscale filter [no] –enable-libzmq enable message passing via libzmq [no] –enable-libzvbi enable teletext support via libzvbi [no] –disable-lzma disable lzma [autodetect] –enable-decklink enable Blackmagic DeckLink I/O support [no] –enable-mediacodec enable Android MediaCodec support [no] –enable-netcdf enable NetCDF, needed for sofalizer filter [no] –enable-openal enable OpenAL 1.1 capture support [no] –enable-opencl enable OpenCL code –enable-opengl enable OpenGL rendering [no] –enable-openssl enable openssl, needed for https support if gnutls is not used [no] –disable-schannel disable SChannel SSP, needed for TLS support on Windows if openssl and gnutls are not used [autodetect] –disable-sdl2 disable sdl2 [autodetect] –disable-securetransport disable Secure Transport, needed for TLS support on OSX if openssl and gnutls are not used [autodetect] –enable-x11grab enable X11 grabbing (legacy) [no] –disable-xlib disable xlib [autodetect] –disable-zlib disable zlib [autodetect] 硬件加速功能（hardware acceleration features） ffmpeg默认实现了移动端（Android和IOS）的硬编解码，可以选择disable的都是默认开启的，可以关闭，可以选择enable的都是需要自己解决依赖的。 选项 说明 –disable-audiotoolbox disable Apple AudioToolbox code [autodetect] –enable-cuda enable dynamically linked Nvidia CUDA code [no] –enable-cuvid enable Nvidia CUVID support [autodetect] –disable-d3d11va disable Microsoft Direct3D 11 video acceleration code [autodetect] –disable-dxva2 disable Microsoft DirectX 9 video acceleration code [autodetect] –enable-libmfx enable Intel MediaSDK (AKA Quick Sync Video) code via libmfx [no] –enable-libnpp enable Nvidia Performance Primitives-based code [no] –enable-mmal enable Broadcom Multi-Media Abstraction Layer (Raspberry Pi) via MMAL [no] –disable-nvenc disable Nvidia video encoding code [autodetect] –enable-omx enable OpenMAX IL code [no] –enable-omx-rpi enable OpenMAX IL code for Raspberry Pi [no] –disable-vaapi disable Video Acceleration API (mainly Unix/Intel) code [autodetect] –disable-vda disable Apple Video Decode Acceleration code [autodetect] –disable-vdpau disable Nvidia Video Decode and Presentation API for Unix code [autodetect] –disable-videotoolbox disable VideoToolbox code [autodetect] 工具链选项（Toolchain options） ffmpeg代码本身是支持跨平台的，要编译不同的平台需要配置不同平台的交叉编译工具链。ffmpeg都是c代码，所以不需要配置c++的sysroot。常用的就几个arch,cpu,cross-prefix,enable-cross-compile,sysroot,target-os,extra-cflags,extra-ldflags,enable-pic。现在Android和IOS几乎没有armv5的设备了，所以如果编译这两个平台配置armv7和armv8就好了。 选项 说明 –arch=ARCH 选择目标架构[armv7a/aarch64/x86/x86_64等] –cpu=CPU 选择目标cpu[armv7-a/armv8-a/x86/x86_64] –cross-prefix=PREFIX 设定交叉编译工具链的前缀,不算gcc/nm/as命令，例如android 32位的交叉编译链$ndk_dir/toolchains/arm-linux-androideabi-$toolchain_version/prebuilt/linux-$host_arch/bin/arm-linux-androideabi- –progs-suffix=SUFFIX program name suffix [] –enable-cross-compile 如果目标平台和编译平台不同则需要使能它 –sysroot=PATH 交叉工具链的头文件和库位，例如Android 32位位置$ndk_dir/platforms/android-14/arch-arm –sysinclude=PATH location of cross-build system headers –target-os=OS 设置目标系统 –target-exec=CMD command to run executables on target –target-path=DIR path to view of build directory on target –target-samples=DIR path to samples directory on target –tempprefix=PATH force fixed dir/prefix instead of mktemp for checks –toolchain=NAME set tool defaults according to NAME –nm=NM use nm tool NM [nm -g] –ar=AR use archive tool AR [ar] –as=AS use assembler AS [] –ln_s=LN_S use symbolic link tool LN_S [ln -s -f] –strip=STRIP use strip tool STRIP [strip] –windres=WINDRES use windows resource compiler WINDRES [windres] –yasmexe=EXE use yasm-compatible assembler EXE [yasm] –cc=CC use C compiler CC [gcc] –cxx=CXX use C compiler CXX [g++] –objcc=OCC use ObjC compiler OCC [gcc] –dep-cc=DEPCC use dependency generator DEPCC [gcc] –ld=LD use linker LD [] –pkg-config=PKGCONFIG use pkg-config tool PKGCONFIG [pkg-config] –pkg-config-flags=FLAGS pass additional flags to pkgconf [] –ranlib=RANLIB use ranlib RANLIB [ranlib] –doxygen=DOXYGEN use DOXYGEN to generate API doc [doxygen] –host-cc=HOSTCC use host C compiler HOSTCC –host-cflags=HCFLAGS use HCFLAGS when compiling for host –host-cppflags=HCPPFLAGS use HCPPFLAGS when compiling for host –host-ld=HOSTLD use host linker HOSTLD –host-ldflags=HLDFLAGS use HLDFLAGS when linking for host –host-libs=HLIBS use libs HLIBS when linking for host –host-os=OS compiler host OS [] –extra-cflags=ECFLAGS 设置cflags，如果是Android平台可以根据ndk内的设定,arm-linux-androideabi-4.6/setup.mk，建议参考你当前的setup来配置 –extra-cxxflags=ECFLAGS add ECFLAGS to CXXFLAGS [] –extra-objcflags=FLAGS add FLAGS to OBJCFLAGS [] –extra-ldflags=ELDFLAGS 参考cflags –extra-ldexeflags=ELDFLAGS add ELDFLAGS to LDEXEFLAGS [] –extra-ldlibflags=ELDFLAGS add ELDFLAGS to LDLIBFLAGS [] –extra-libs=ELIBS add ELIBS [] –extra-version=STRING version string suffix [] –optflags=OPTFLAGS override optimization-related compiler flags –build-suffix=SUFFIX library name suffix [] –enable-pic build position-independent code –enable-thumb compile for Thumb instruction set –enable-lto use link-time optimization –env=”ENV=override” override the environment variables 高级选项（Advanced options） 选项 说明 –malloc-prefix=PREFIX prefix malloc and related names with PREFIX –custom-allocator=NAME use a supported custom allocator –disable-symver disable symbol versioning –enable-hardcoded-tables use hardcoded tables instead of runtime generation –disable-safe-bitstream-reader disable buffer boundary checking in bitreaders (faster, but may crash) –enable-memalign-hack emulate memalign, interferes with memory debuggers –sws-max-filter-size=N the max filter size swscale uses [256] 优化选项（Optimization options） 默认开启各个平台的汇编优化，有些嵌入式平台可能并不能完整的支持架构的所有汇编指令，所以需要关闭。（自己理解的，没有实战） 选项 说明 –disable-asm disable all assembly optimizations –disable-altivec disable AltiVec optimizations –disable-vsx disable VSX optimizations –disable-power8 disable POWER8 optimizations –disable-amd3dnow disable 3DNow! optimizations –disable-amd3dnowext disable 3DNow! extended optimizations –disable-mmx disable MMX optimizations –disable-mmxext disable MMXEXT optimizations –disable-sse disable SSE optimizations –disable-sse2 disable SSE2 optimizations –disable-sse3 disable SSE3 optimizations –disable-ssse3 disable SSSE3 optimizations –disable-sse4 disable SSE4 optimizations –disable-sse42 disable SSE4.2 optimizations –disable-avx disable AVX optimizations –disable-xop disable XOP optimizations –disable-fma3 disable FMA3 optimizations –disable-fma4 disable FMA4 optimizations –disable-avx2 disable AVX2 optimizations –disable-aesni disable AESNI optimizations –disable-armv5te disable armv5te optimizations –disable-armv6 disable armv6 optimizations –disable-armv6t2 disable armv6t2 optimizations –disable-vfp disable VFP optimizations –disable-neon disable NEON optimizations –disable-inline-asm disable use of inline assembly –disable-yasm disable use of nasm/yasm assembly –disable-mipsdsp disable MIPS DSP ASE R1 optimizations –disable-mipsdspr2 disable MIPS DSP ASE R2 optimizations –disable-msa disable MSA optimizations –disable-mipsfpu disable floating point MIPS optimizations –disable-mmi disable Loongson SIMD optimizations –disable-fast-unaligned consider unaligned accesses slow 开发者选项（Developer options） 调试用的一些开关 选项 说明 –disable-debug disable debugging symbols –enable-debug=LEVEL set the debug level [] –disable-optimizations disable compiler optimizations –enable-extra-warnings enable more compiler warnings –disable-stripping disable stripping of executables and shared libraries –assert-level=level 0(default), 1 or 2, amount of assertion testing, 2 causes a slowdown at runtime. –enable-memory-poisoning fill heap uninitialized allocated space with arbitrary data –valgrind=VALGRIND run “make fate” tests through valgrind to detect memory leaks and errors, using the specified valgrind binary. Cannot be combined with –target-exec –enable-ftrapv Trap arithmetic overflows –samples=PATH location of test samples for FATE, if not set use $FATE_SAMPLES at make invocation time. –enable-neon-clobber-test check NEON registers for clobbering (should be used only for debugging purposes) –enable-xmm-clobber-test check XMM registers for clobbering (Win64-only; should be used only for debugging purposes) –enable-random randomly enable/disable components –disable-random –enable-random=LIST randomly enable/disable specific components or –disable-random=LIST component groups. LIST is a comma-separated list of NAME[:PROB] entries where NAME is a component (group) and PROB the probability associated with –random-seed=VALUE seed value for –enable/disable-random –disable-valgrind-backtrace do not print a backtrace under Valgrind (only applies to –disable-optimizations builds) 转自：https://blog.csdn.net/momo0853/article/details/78043903 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/02_shell.html":{"url":"FFmpeg/02_shell.html","title":"Shell脚本","keywords":"","body":"Shell脚本简介Shell脚本权限Shell基本语法Shell其他关键词Shell脚本 简介 Shell脚本（Shell Script）是一种为Shell编写的脚本程序。而Shell是一个用C语言编写的程序，它是用户使用Linux的桥梁。在Linux中Shell程序又有很多种，如： sh - 即 Bourne Shell。sh 是 Unix 标准默认的 shell。 bash - 即 Bourne Again Shell。bash 是 Linux 标准默认的 shell。 …… 其中，bash由于易用和免费，被广泛使用。每种Shell程序都有其对应的解释器，在脚本文件第一行会告诉系统当前程序属于那种类型，如： #!/bin/bash echo \"这是bash类型的shell程序\" 本文主要介绍bash类型的Shell脚本的基本使用。 Shell脚本权限 Shell脚本被执行前需要可执行权限。如刚创建的test.sh脚本，需chmod +x test.sh方能执行： qincji:giteeblog mac$ ./test.sh -bash: ./test.sh: Permission denied qincji:giteeblog mac$ chmod +x test.sh qincji:giteeblog mac$ ./test.sh Hellow Shell 注：执行脚本文件的命令使用sh或.，如sh test.sh或./test.sh。 Shell基本语法 注释 单行注释 - 以 # 开头，到行尾结束。 多行注释 - 以 : 开头，到 EOF 结束。 如编辑test.sh脚本文件 #!/bin/bash echo \"Hellow Shell\" #echo \"使用#单行注释了，不能输出\" : 执行sh test.sh查看输出： qincji:giteeblog mac$ sh test.sh Hellow Shell echo输出 echo用于字符串的输出。编辑test.sh进行介绍： #!/bin/bash echo \"1-You\" #1.输出普通字符串 echo \"2-You \\\"are\\\"\" #2.输出使用\\转义 age=18 echo \"3-You \\\"are\\\" ${age}\" #3.使用${变量名}输出包含变量的字符串 echo \"4-You \\\"are\\\" \"$age\"\" #4.使用\"$变量名\"输出包含变量的字符串\" echo -e \"5-YES\\nNO\" #5.使用 -e 开启（对\\n）转义 echo \"6-test\" > test.txt #6.输出重定向至文件 执行后输出结果： qincji:giteeblog mac$ ./test.sh 1-You 2-You \"are\" 3-You \"are\" 18 4-You \"are\" 18 5-YES NO qincji:giteeblog mac$ cat test.txt 6-test 变量 Bash 中没有数据类型，bash 中的变量可以保存一个数字、一个字符、一个字符串等等。定义变量名的规则： 1.命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 2.中间不能有空格，可以使用下划线（_）。 3.不能使用标点符号。 4.不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 如，编辑test.sh如下： #!/bin/bash #有效变量名 var=\"0 字符串\" VAR=1 _var=2 _var2=3 _3var=4 #以下为无效变量名 4var=5 ?var=6 var*2=7 #使用表达式命令赋值，输出看看命令的结果 files=`du -sh .` echo ${files} #删除变量 unset var echo ${var} 执行输出结果： qincji:giteeblog mac$ ./test.sh ./test.sh: line 9: 4var=5: command not found ./test.sh: line 10: ?var=6: command not found ./test.sh: line 11: var*2=7: command not found 138M . 字符串 使用'或者\"来定义字符串，也可以不用引号。但是'中不能使用变量和转义符。所以记住用\"就行了。字符串常规操作，有： 1.字符串拼接，格式：'hello，'${串}''或\"hello，${串}\"。 2.获取字符串长度，格式：${#串}。 3.截取字符串，格式${串:2:3}，其中2为起点，3为截取的个数。 编辑test.sh如下： #!/bin/bash name=\"123456\" full1='1-hello，'${name}'' #1.单引号字符串拼接 full2=\"1-hello，${name}\" #1.双引号字符串拼接 echo ${full1} echo ${full2} echo \"2-长度：${#name}\" #2.获取字符串长度 echo \"3-截取：${name:2:3}\" #3.截取字符串 执行输出结果： qincji:giteeblog mac$ ./test.sh 1-hello，123456 1-hello，123456 2-长度：6 3-截取：345 数组 Bash只支持一维数组，下标从 0 开始，下标可以是整数或算术表达式，其值应大于或等于 0。数组常规操作，有： 1.创建数组，格式：array=(value0 value1 value2)或指定下标array=([1]=value0 [1]=value1 [2]=value2)。 2.访问元素，格式：获取单个：\"${array[下标]}\"；获取所有：\"${array[@]}\"。 3.取得个数，格式：\"${#array[@]}\"或\"${#array[*]}\"。 4.增加元素，格式：array=(value3 \"${array[@]}\" value4)。 5.删除元素，格式：unset array[0]。 编辑test.sh如下： #!/bin/bash array=([1]=1 [0]=\"a\" [3]=\"￥\") #1.创建数组 echo \"2-访问：v1=${array[1]}，v2=${array[@]}\" #2.访问元素 echo \"3-获取：len1=${#array[@]}，len2=${#array[*]}\" #3.取得个数 array=(\"NO\" \"${array[@]}\" 33) #4.增加元素 echo \"4-增加：${array[@]}\" unset array[0] #5.删除元素 echo \"5-删除：${array[@]}\" 执行输出结果： qincji:giteeblog mac$ ./test.sh 2-访问：v1=1，v2=a 1 ￥ 3-获取：len1=3，len2=3 4-增加：NO a 1 ￥ 33 5-删除：a 1 ￥ 33 运算符 算术运算符 + - / %（加减乘除取余），格式：expr value0 符号 value1，特殊符号需要转义，如`expr 3 \\ 4`结果为12。 =（赋值），格式：value=value1，把value1赋值给value。 ==（比较相等），格式：[ value1 == value2 ]，如[ 2 == 5 ]，返回false。 !=（比较不相等），格式：[ value1 != value2 ]，如[ 2 != 5 ]，返回true。 注意：[ value1 != 'value2' ]是带空格的，如错误写法：[value1!='value2']。在循环体测试中使用[]不起作用，需要用(())，如： a=1 while (($a 编辑test.sh如下： #!/bin/bash a=3 b=3 o1=`expr ${a} \\* ${b}` echo \"o1=${o1}\" if [ $a == $b ] then echo \"a 等于 b\" else echo \"a 不等于 b\" fi 执行输出结果： qincji:giteeblog mac$ ./test.sh o1=9 a 等于 b 关系运算符 通用格式为：[ value1 符号 value2 ]，用于比较结构中，同上==和!=用法一样。 -eq 是否相等，格式：[ value1 -eq value2 ]，如[ 2 -eq 5 ]，返回false。 -ne 是否不相等，格式：同上。 -gt >，格式：同上。 -lt -ge >=，格式：同上。 -le 布尔运算符 ! 非，格式：[ ! 表达式 ]。与表示式结果相反。。如[ ! 1 == 1 ]返回 false。 -o 或 (与逻辑运算符||相同)，格式：[ 表达式1 -o 表达式2 ]。有一个表达式为 true 则返回 true。如[ 1 == 1 -o 2 != 2 ]返回 true。 -a 与 (与逻辑运算符&&相同)，格式：[ 表达式1 -o 表达式2 ]。两个表达式都为 true 才返回 true。如[ 1 == 1 -a 2 != 2 ]返回 false。 逻辑运算符 ||和&&参考布尔运算符。 字符串运算符 = 字符串相等，格式：[ value1 = value2 ]，如a=\"a\",b=\"b\"，[ ${a} = ${b} ]，返回true。 != 字符串不相等，格式：[ value1 != value2 ]，如a=\"a\",b=\"b\"，[ ${a} != ${b} ]，返回false。 -z 字符串长度为0为true，格式：[ -z value ]，如a=\"a\"，[ -z ${a} ]，返回false。 -n 字符串长度不为0为true， 格式：[ -n = value ]，如a=\"a\"，[ -n = ${a} ]，返回true。 文件运算符 通用格式为：[符号 文件路径 ]，用于检查文件的属性，如何符合为true。 -d 目录存在为true，格式：[ -d filePath ]，如[ -d \"./\" ]，为true。 -f 文件存在为true，格式：[ -f filePath ]，如[ -f \"./test.sh\" ]，为true。 -r 可读为true，格式：同上。 -w 可写为true，格式：同上。 -x 可执行 可执行为true，格式：同上。 -s 文件不为空(有内容) true，格式：同上。 -e 文件/目录存在，格式：同上。 编辑test.sh如下： #!/bin/bash if [ -d \"./\" ] then echo \"1-输出为true\" else echo \"1-输出为false\" fi if [ -w \"./test.sh\" ] then echo \"2-输出为true\" else echo \"2-输出为false\" fi 执行输出结果： qincji:giteeblog mac$ ./test.sh 1-输出为true 2-输出为true 流程控制 条件语句： ①if..elif..else，使用then..fi来控制命中的范围。单行格式：if [ 条件1 ]; then 执行体1; elif [ 条件2 ]; then 执行体2; else 执行体2; fi，多行格式：直接;去掉即可。 ②case，在exec..esac范围内。格式如下 exec case 输入 in 条件1) 执行体1;; 条件2) 执行体2;; *) 其他执行体;; esac 循环语句：for、while和until。循环体在do..done范围内，在循环中遇到continue会进入下个循环，遇到break跳出循环。 编辑test.sh如下： #!/bin/bash #1.注意单行的写法需要用 ； 把语句段落隔开 if [ \"111\" = \"abc\" ]; then echo \"输出if-1\" elif [ \"abc\" = \"abc\" ]; then echo \"输出elif-1\" else echo \"输出else-1\" fi #2.注意单行的写法需要用 ； 把语句段落隔开 if [ \"111\" = \"abc\" ]; then echo \"输出if-2\"; elif [ \"abc2\" = \"abc\" ]; then echo \"输出elif-2\"; else echo \"输出else-2\"; fi #3.case语句 exec case \"4\" in \"a\") echo \"case输出a\" ;; \"b\") echo \"case输出b\" ;; *) echo \"case输出*\" ;; esac #4.for语句 for var in \"red\" 2 \"yes\"; do echo \"for语句：${var}\"; done #5.while语句 a=1 #while [ $a -le 10 ] while (($a = 6)); do echo \"until语句：$a\" let \"a++\" done 执行输出结果： qincji:giteeblog mac$ ./test.sh 输出elif-1 输出else-2 case输出* for语句：red for语句：2 for语句：yes while语句：1 while语句：continue while语句：3 while语句：break until语句：4 until语句：5 函数 编辑test.sh如下： #!/bin/bash #function关键字可省了，fun_name随便定的函数名称。 function fun_name(){ echo $0 #$0：文件名称 echo $1 #$1：传递的第一个参数 echo $2 #$2：传递的第二个参数 } fun_name 1 \"Yes\" 执行输出结果： qincji:giteeblog mac$ ./test.sh ./test.sh 1 Yes Shell其他关键词 eval 语法：eval cmdLine，eval会对后面的cmdLine进行两遍扫描，如果在第一遍扫面后cmdLine是一个普通命令，则执行此命令；如果cmdLine中含有变量的间接引用，则保证简介引用的语义。示例如下： 编辑test.sh如下： #!/bin/bash #文件传递的长度，输出最后一个数 function test() { echo \"\\{% math %}#\" #这才是我们想要的结果 eval echo \"\\{% endmath %}#\" } test 11 22 33 44 执行输出结果： qincji:giteeblog mac$ ./test.sh $4 44 shift 语法：shift number，位置参数可以用shift命令左移。比如shift 3表示原来的$4现在变成$1，原来的$5现在变成$2等等，原来的$1、$2、$3丢弃，$0不移动。不带参数的shift命令相当于shift 1。示例如下： 编辑test.sh如下： #!/bin/bash #文件传递的长度，输出最后一个数 function test() { until [ $# -eq 0 ]; do echo \"第一个参数为: $1 参数个数为: $#\" shift 2 done } test 11 22 33 44 55 66 执行输出结果： qincji:giteeblog mac$ ./test.sh 第一个参数为: 11 参数个数为: 6 第一个参数为: 33 参数个数为: 4 第一个参数为: 55 参数个数为: 2 参考 https://www.cnblogs.com/jingmoxukong/p/7867397.html Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/03_build_ffmpeg.html":{"url":"FFmpeg/03_build_ffmpeg.html","title":"编译FFmpeg4.2.2","keywords":"","body":"编译FFmpeg4.2.2前言FFmpeg编译流程FFmpeg基本组成结构选择模块进行编译准备测试1：仅仅编译出ffplay程序测试2：根据测试1中去掉支持rtmp协议测试3：集成第三方库：x264测试4：android交叉编译总结编译FFmpeg4.2.2 前言 在编译FFmpeg之前，我们得先知道FFmpeg包含了那些内容（组件），我们应该要如何查看并选择？这里我们就简单来说说FFmpeg编译的过程，以及集成x264，编译android平台所需要的动态库等。（这里的测试使用的是Mac系统） FFmpeg编译流程 编译过程主要分为两步（下图）： (1)configure：通过configure --help查看我们所能选择的配置。前往查看configure配置选项注释。这一步后会生成许多Makefile编译所需要的东西。其中在 ffbuild/config.log 可查看当前执行的日志。 (2)make install：编译生成我们所配置的东西。如果之前编译过需要make clean清除之前编译过的数据再执行make install。 注：生成的执行文件或可移植类库时根据系统会不一样，比如生成ffplay时：Unix系统会是ffplay，windows会是ffplay.exe。 FFmpeg基本组成结构 从configure配置文件中阅读可知，FFmpeg基本组成结构可以下部分（下图）： 主要由AVCodec（编解码相关）、AVDevice（输入/输出设备相关）、AVFilter（滤波处理相关）和AVFormat（数据格式处理相关）四大基本模块构成。在这四大模块下又细分了一些小模块，这里对小模块作用简单做一下说明： bsfs：格式转换。通过./configure --list-bsfs查看所有能支持转换的格式。 decoders：解码器。通过./configure --list-decoders查看所有能支持解码器。 encoders：编码器。通过./configure --list-encoders查看所有能支持编码器。 hwaccels：硬件编解码器。通过./configure --list-hwaccels查看所有能支持硬件编解码器。 parsers：解析器。通过./configure --list-parsers查看所有能支持解析器。 indevs：输入设备（如安卓摄像头）。通过./configure --list-indevs查看所有能支持输入设备。 outdevs：输出设备（如opengl）。通过./configure --list-outdevs查看所有能支持输出设备。 filters：滤镜处理器（如gblur高斯模糊）。通过./configure --list-filters查看所有能支持滤镜处理器。 muxers：封装（如把flv容器格式视频拆解出视频流、音频流等）。通过./configure --list-muxers查看所有能支持封装。 demuxers：解封装（对应封装）。通过./configure --list-demuxers查看所有能支持解封装。 protocols：协议（如rtmp）。通过./configure --list-protocols查看所有能支持的协议。 选择模块进行编译 上面简单介绍了ffmpeg的编译流程以及ffmpeg库到底包含了那些类库。知道了这些，我们就可以选择我们所需要的类库进行编译了。如果默认编译出来的ffmpeg将会是相当庞大的，像一下动态库移植，库太大就很烦恼了。而且像x264库等一些第三方库也是需要知道如何进行配置的。这里我们的就探讨一下如何选择我们所需要的类库进行编译。 准备 (1)下载ffmpeg源文件 ，根据自己的系统进行选择，下载回来后进行解压。 (2)参考官方文档 ，如：Mac需要按照Xcode等，编译也应该先看官网文档介绍。（最靠谱的方式吧？） (3)掌握Shell基础知识 ，为了方便修改与编译。 (4)下载x264 ，《测试3》需要用。 (5)下载NDK ，注意系统的类型。《测试4》需要用。 测试1：仅仅编译出ffplay程序 configure配置文件中默认是输出所有程序的（ffmpeg、ffplay和ffprobe）。但是因为ffplay需要sdl2库，所以这里讲一下如何配置单单输出ffplay程序。 (1)安装sdl2 从configure配置文件中得知，ffplay程序需要依赖sdl2库。Mac可通过执行brew install sdl2命令进行安装。 (2)编译验证ffplay 在项目的根目录编写build.sh脚本文件并授权，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash ./configure \\ --disable-programs \\ --enable-ffplay make clean make install 然后在控制台中看到输出信息，发现只有ffplay程序将要被编译。 等待十几分钟后，我们发现在项目的根目录仅仅生成ffplay可执行文件，我们在控制台执行du -sh ./ffplay命令看看生成文件的大小有20M。 qincji:ffmpeg mac$ du -sh ./ffplay 20M ./ffplay 执行命令播放一个rtmp：./ffplay -window_title \"版本1\" rtmp://slive.ytn.co.kr/dmb/ydlive_20140419_1，结果如下： 好了，我们编译的ffplay播放器能正常使用了。使用./ffplay --help查看具体用法。 (3)查找这样配置的原因 上面我们已经知道了编译结果，这里需要找一下原因，这到底为何？我们从configure文件找到一些蛛丝马迹： #略…………………… #program队列 PROGRAM_LIST=\" ffplay ffprobe ffmpeg \" #略…………………… #disable方法，将调用set_all，并传值为no，以及将要设置disable的名称($*)。 disable(){ set_all no $* } #略…………………… set_all(){ value=$1 shift for var in $*; do eval $var=$value done } : 测试2：根据测试1中去掉支持rtmp协议 在测试1中我们是把程序模块的先全部取消编译，然后再选择其中一个。这里我们选择仅仅去掉一个模块中的单个组件。我们举一个例子，取消支持protocols中的rtmp协议： (1)如何入手？ 我们知道一切编译皆来自configure配置文件，所以我们得从这个文件找我们需要的信息。我刚开始找rtmp相关信息，只是找到librtmp库的启用或取消，然后编译了一下，发现并不是！ 然后我就搜索protocols，发现有一条关键信息，如下： #单个组件配置 Individual component options: #略………… #这就是我想要的！！后面NAME是rtmp！如何知道？通过 ./configure --list-protocols 看到所有支持协议的名称 --disable-protocol=NAME disable protocol NAME --disable-protocols disable all protocols (2)编译验证结果 修改build.sh脚本，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash ./configure \\ --disable-programs \\ --enable-ffplay \\ --disable-protocol=rtmp make clean make install 然后在控制台中看到输出信息，发现Enabled protocols:中已经没有了rtmp协议了！ 等待编译完成，执行命令播放一个rtmp：./ffplay -window_title \"版本2\" rtmp://slive.ytn.co.kr/dmb/ydlive_20140419_1，结果如下： 开启或者禁用其他组件的道理也是一样哦。 测试3：集成第三方库：x264 ffmpeg官方mac编译指南集成libx264 ，但是给出的信息不是很明确（有些我不太懂），几经波折后发现，是需要指定CFLAGS和LDFLAGS（也就是--extra-cflags和--extra-ldflags）才编译通过的。这里给出编译脚本，在编写前我们把下载下来解压后的x264库改名为libx264然后放进ffmpeg项目根目录。修改build.sh脚本，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash function build_x264() { cd libx264 ./configure \\ --prefix=${X264_LIBS} \\ --enable-static make clean make install } function build_ffmpeg() { cd .. ./configure \\ --disable-programs \\ --enable-ffmpeg \\ --enable-gpl \\ --extra-cflags=\"-I${X264_LIBS}/include\" \\ --extra-ldflags=\"-L${X264_LIBS}/lib\" \\ --enable-libx264 make clean make install } #设置x264变成出来的静态库保存路径，然后编译ffmpeg时，链接进去 X264_LIBS=$(pwd)/libx264/libouput build_x264 build_ffmpeg 编译过程中，我们留意到控制台的到输出信息，发现libx264协议被启用了： 等待编译成功后，验证当前生成的ffmpeg程序是否已经集成了libx264，找个mp4视频放到当前目录，命名为input.mp4。然后执行以下命令，如果没报错说了已成功集成： ./ffmpeg -re -i input.mp4 -vcodec libx264 -an output.mp4 测试4：android交叉编译 ffmpeg官网android编译指南 ， 测试了一下，最后发现这几个参数比较关键： --enable-cross-compile : 开启交叉编译。 --cross-prefix : gcc的前缀。（如果使用clang编译则可以不给） --target-os : 指定android使用平台。 --arch : 处理器类型。 --cpu : cpu类型。 --cc : c语言编译器（给当前指定的绝对路径）。 --cxx : c++语言编译器（给当前指定的绝对路径）。 --extra-cflags : 给传递给编译器的参数。 修改build.sh脚本，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash API=21 export NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk-bundle export SYSROOT=$NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot export TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 function build_android() { ./configure \\ --prefix=$PREFIX \\ --disable-programs \\ --disable-static \\ --enable-shared \\ --cross-prefix=$CROSS_PREFIX \\ --target-os=android \\ --arch=$ARCH \\ --cpu=$CPU \\ --cc=$CC \\ --cxx=$CXX \\ --enable-cross-compile \\ --extra-cflags=\"$CFLAG\" || exit 0 make clean make install } ARCH=arm CPU=armv7-a CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU\" CFLAG=\"-Os -fpic $OPTIMIZE_CFLAGS\" build_android 等待编译完成，我们发现能成功编译出动态库： 总结 好了，ffmpeg编译已经基本讲述完了。这里总结一下我所编译过程的一些想法： 使用IDE管理项目，自己处理的方便就行。我这里用clion，github有破解方式。 configure 文件非常重要，shell语法还是要懂一点。 报错了注意查看日志ffbuild/config.log。 路径一定要看是否真实存在，如$CC可能在一些API版本上xxx-clang是没有的。 参考 https://blog.csdn.net/leixiaohua1020/article/details/44587465 https://blog.csdn.net/leixiaohua1020/article/details/44556525 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/04_import_ffmpeg.html":{"url":"FFmpeg/04_import_ffmpeg.html","title":"FFmpeg导入到Clion（MacOS）","keywords":"","body":"FFmpeg导入到Clion（MacOS）编译MacOS平台的FFmpeg动态库将生成的库导入到Clion进行验证FFmpeg导入到Clion（MacOS） 继如何编译FFmpeg后，本章也是基于前面来讲解如何把编译后的库文件移入到Clion。 在编译ffmpeg前，有一点需要提前说明的，Mac系统并不支持链接静态库，详细请看初步认识c/c++编译 这篇文章。针对于平台的差异性，CLion工具在Mac系统中使用CMakelist.txt进行交叉编译并不友好，我试过集成Android平台的ffmpeg，最终以失败而告终，主要报错原因是：无法指向NDK中的 ld 链接器（要是哪位朋友尝试编译通过还望告知）。所以后续如果是Android项目还是老老实实使用Android Studio吧。 编译MacOS平台的FFmpeg动态库 在ffmpeg项目根目录编写build.sh脚本文件如下： #!/bin/bash make clean function build_macosx() { ./configure \\ --prefix=$PREFIX \\ --disable-programs \\ --target-os=darwin \\ --disable-static \\ --pkg-config=$(which pkg-config) \\ --enable-shared || exit 0 make clean make install } PREFIX=$(pwd)/macox build_macosx 然后在控制台上执行编译脚本./build.sh，建议使用IDE，如我使用CLion导入ffmpeg源码就行操作如下： 等待编译成功后，会在当前目录的macox文件夹生成动态库，如下： 将生成的库导入到Clion （1）在Clion新建C++项目，项目名称为VAFFmpeg。 （2）将生成的include和lib复制到项目跟目录。如图下图所示： （3）编写CMakeLists.txt文件，分别引入刚刚复制过来的头文件和动态库，如下： cmake_minimum_required(VERSION 3.17) project(VAFFmpeg) set(CMAKE_CXX_STANDARD 11) set(SOURCE_FILES main.cpp) link_directories(./lib/) include_directories(./include/) add_executable(VAFFmpeg ${SOURCE_FILES}) target_link_libraries( VAFFmpeg avcodec avdevice avfilter avformat ) 进行验证 编写main.cpp文件如下： #include extern \"C\" { #include } using namespace std; int main() { cout \" \" 执行能正常输出，如下： Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/05_source.html":{"url":"FFmpeg/05_source.html","title":"使用Clion阅读FFmpeg源码（支持跳转）","keywords":"","body":"使用Clion阅读FFmpeg源码（支持跳转）前言方案一（简单）步骤1步骤2步骤3简单查看方案二（需要编译源码）步骤1（同方案一）步骤2（同方案一）步骤3步骤4使用Clion阅读FFmpeg源码（支持跳转） 前言 本方案仅仅适用于阅读FFmpeg，配置方式及其简单，能支持方法间的跳转，但由于配置原因部分无法识别或跳转，介意者勿入！！ 方案一（简单） 此方案非常简单，能阅读绝大部分源码了，但仍会缺失一些头文件，导致无法小小部分无法查阅。 步骤1 在Clion新建c++项目项目File->New Project->Create，如下图： 步骤2 将FFmpeg源码复制到根目录，如下图： 步骤3 编写CMakeLists.txt文件，如下： cmake_minimum_required(VERSION 3.17) project(SourceFFmpeg) set(CMAKE_CXX_STANDARD 11) file(GLOB EX_DIR ./*.cpp ./*.c ./*/*.c ./*/*/*.c ./*/*/*/*.c) set(INCLUDE_DIR ./ ./*/ ./*/*/ ./*/*/*/ ./*/*/*/*/) include_directories(${INCLUDE_DIR}) add_executable(SourceFFmpeg ${EX_DIR}) 简单查看 到这已经完成了，这里我们查看一下源码之间的跳转，以及一些方法的被调用，可以从doc/examples例子看看源码能不能正常跳转等，我这如下： 方案二（需要编译源码） 解决方案一中一些头文件缺失问题。 步骤1（同方案一） 步骤2（同方案一） 步骤3 编译源码生成动态库/静态库，目的需要头文件，然后把生成的头文件复制到项目中，如下： 步骤4 编写CMakeLists.txt文件，如下： cmake_minimum_required(VERSION 3.17) project(SourceFFmpeg) set(CMAKE_CXX_STANDARD 11) include_directories(./include/) file(GLOB EX_DIR ./*.cpp ./*.c ./*/*.c ./*/*/*.c ./*/*/*/*.c) add_executable(SourceFFmpeg ${EX_DIR}) 到这里方案二的也完成了，可自行查阅。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/06_struct.html":{"url":"FFmpeg/06_struct.html","title":"FFmpeg重要结构体（转载）","keywords":"","body":"FFmpeg重要结构体（转载）结构体间联系a)解协议（http,rtsp,rtmp,mms）b)解封装（flv,avi,rmvb,mp4）c)解码（h264,mpeg2,aac,mp3）d)存数据AVFrame （libavutil/frame.h）AVFormatContext （libavformat/avformat.h）AVStream （libavformat/avformat.h）AVIOContext （libavformat/avio.h）AVCodecContext （libavcodec/avcodec.h）AVCodec （libavcodec/avcodec.h）AVPacket （libavcodec/avcodec.h）FFmpeg重要结构体（转载） 本文主要整合FFMPEG中最关键的结构体之间的关系 系列文章，以便于后面学习以及资料的查阅。 结构体间联系 a)解协议（http,rtsp,rtmp,mms） AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”） b)解封装（flv,avi,rmvb,mp4） AVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装格式都对应一个AVInputFormat 结构。 c)解码（h264,mpeg2,aac,mp3） 每个AVStream存储一个视频/音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频/音频流使用解码方式的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频/音频对应的解码器。每种解码器都对应一个AVCodec结构。 d)存数据 视频的话，每个结构一般是存一帧；音频可能有好几帧; 解码前数据：AVPacket 解码后数据：AVFrame AVFrame （libavutil/frame.h） AVFrame是包含码流参数较多的结构体。AVFrame结构体一般用于存储原始数据（即非压缩数据，例如对视频来说是YUV，RGB，对音频来说是PCM），此外还包含了一些相关的信息。比如说，解码的时候存储了宏块类型表，QP表，运动矢量表等数据。编码的时候也存储了相关的数据。因此在使用FFMPEG进行码流分析的时候，AVFrame是一个很重要的结构体。 #define AV_NUM_DATA_POINTERS 8 uint8_t *data[AV_NUM_DATA_POINTERS]：解码后原始数据（对视频来说是YUV、RGB，对音频来说是PCM） int linesize[AV_NUM_DATA_POINTERS]：data中“一行”数据的大小。注意：未必等于图像的宽，一般大于图像的宽。 int width, height：视频帧宽和高（1920x1080,1280x720...） int nb_samples：音频的一个AVFrame中可能包含多个音频帧，在此标记包含了几个 int format：解码后原始数据类型（YUV420，YUV422，RGB24...），未知或未设置为-1。 int key_frame：是否是关键帧 enum AVPictureType pict_type：帧类型（I,B,P...） AVRational sample_aspect_ratio：宽高比（16:9，4:3...） int64_t pts：显示时间戳 int coded_picture_number：编码帧序号 int display_picture_number：显示帧序号 int8_t *qscale_table：QP表 int channels：音频通道数 int interlaced_frame：是否是隔行扫描 其中，sample_aspect_ratio宽高比是一个分数，AVRational结构体： typedef struct AVRational{ int num; /// QP表 qscale_table : QP表指向一块内存，里面存储的是每个宏块的QP值。宏块的标号是从左往右，一行一行的来的。每个宏块对应1个QP。qscale_table[0]就是第1行第1列宏块的QP值；qscale_table[1]就是第1行第2列宏块的QP值；qscale_table[2]就是第1行第3列宏块的QP值。以此类推... 宏块的个数用下式计算(注：宏块大小是16x16的)，每行宏块数： int mb_stride = pCodecCtx->width/16+1 宏块的总数： int mb_sum = ((pCodecCtx->height+15)>>4)*(pCodecCtx->width/16+1) AVFormatContext （libavformat/avformat.h） 在使用FFMPEG进行开发的时候，AVFormatContext是一个贯穿始终的数据结构，很多函数都要用到它作为参数。它是FFMPEG解封装（flv，mp4，rmvb，avi）功能的结构体。（在这里考虑解码的情况） AVInputFormat *iformat：输入容器格式数据 AVOutputFormat *oformat：输出容器格式数据 AVIOContext *pb：输入数据的缓存 AVIOContext *pb：输入数据的缓存 unsigned int nb_streams：视音频流的个数 AVStream **streams：视音频流 char filename[1024]：文件名 char *url：输入/输出URL int64_t duration：时长（单位：微秒us，转换为秒需要除以1000000） int bit_rate：比特率（单位bps，转换为kbps需要除以1000） int packet_size：packet的长度 AVDictionary *metadata：元数据 其中，通过av_dict_get()函数获得视频的元数据。封装在了AVDictionary和AVDictionaryEntry： struct AVDictionary { int count; AVDictionaryEntry *elems; }; typedef struct AVDictionaryEntry { char *key; char *value; } AVDictionaryEntry; AVStream （libavformat/avformat.h） AVStream是存储每一个视频/音频流信息的结构体。 int index：标识该视频/音频流(AVFormatContext中) AVCodecContext *codec：指向该视频/音频流的AVCodecContext（它们是一一对应的关系） AVRational time_base：时基。通过该值可以把PTS，DTS转化为真正的时间。FFMPEG其他结构体中也有这个字段，但是根据我的经验，只有AVStream中的time_base是可用的。PTS*time_base=真正的时间 int64_t duration：该视频/音频流长度 int64_t nb_frames：该流中已知时的帧数 AVDictionary *metadata：元数据信息 AVRational avg_frame_rate：帧率（注：对视频来说，这个挺重要的） AVPacket attached_pic：附带的图片。比如说一些MP3，AAC音频文件附带的专辑封面。 AVIOContext （libavformat/avio.h） AVIOContext是FFMPEG管理输入输出数据的结构体 unsigned char *buffer：缓存开始位置 int buffer_size：缓存大小（默认32768） unsigned char *buf_ptr：当前指针读取到的位置 unsigned char *buf_end：缓存结束的位置 void *opaque：URLContext结构体 其中，opaque指向的URLContext： typedef struct URLContext { const AVClass *av_class; /** URLContext结构体中还有一个结构体URLProtocol。注：每种协议（rtp，rtmp，file等）对应一个URLProtocol。这个结构体也不在FFMPEG提供的头文件中。从FFMPEG源代码中翻出其的定义： typedef struct URLProtocol { const char *name; int (*url_open)( URLContext *h, const char *url, int flags); int (*url_open2)(URLContext *h, const char *url, int flags, AVDictionary **options); int (*url_accept)(URLContext *s, URLContext **c); int (*url_handshake)(URLContext *c); int (*url_read)( URLContext *h, unsigned char *buf, int size); int (*url_write)(URLContext *h, const unsigned char *buf, int size); int64_t (*url_seek)( URLContext *h, int64_t pos, int whence); int (*url_close)(URLContext *h); int (*url_read_pause)(URLContext *h, int pause); int64_t (*url_read_seek)(URLContext *h, int stream_index, int64_t timestamp, int flags); int (*url_get_file_handle)(URLContext *h); int (*url_get_multi_file_handle)(URLContext *h, int **handles, int *numhandles); int (*url_get_short_seek)(URLContext *h); int (*url_shutdown)(URLContext *h, int flags); int priv_data_size; const AVClass *priv_data_class; int flags; int (*url_check)(URLContext *h, int mask); int (*url_open_dir)(URLContext *h); int (*url_read_dir)(URLContext *h, AVIODirEntry **next); int (*url_close_dir)(URLContext *h); int (*url_delete)(URLContext *h); int (*url_move)(URLContext *h_src, URLContext *h_dst); const char *default_whitelist; } URLProtocol; AVCodecContext （libavcodec/avcodec.h） AVCodecContext是包含变量较多的结构体（感觉差不多是变量最多的结构体）。本文将会大概分析一下该结构体里每个变量的含义和作用。（这里只考虑解码） enum AVMediaType codec_type：编解码器的类型（视频，音频，字幕...） struct AVCodec *codec：采用的解码器AVCodec（H.264,MPEG2...） int bit_rate：平均比特率 uint8_t *extradata; int extradata_size：针对特定编码器包含的附加信息（例如对于H.264解码器来说，存储SPS，PPS等） AVRational time_base：根据该参数，可以把PTS转化为实际的时间（单位为秒s） int width, height：如果是视频的话，代表宽和高 int refs：运动估计参考帧的个数（H.264的话会有多帧，MPEG2这类的一般就没有了） int sample_rate：采样率（音频） int channels：声道数（音频） enum AVSampleFormat sample_fmt：采样格式 int frame_size：帧中每个通道的采样率（音频） int profile：型（H.264里面就有，其他编码标准应该也有） int level：级（和profile差不太多） 其中，1.编解码器类型：codec_type enum AVMediaType { AVMEDIA_TYPE_UNKNOWN = -1, /// 2.在FFMPEG中音频采样格式：sample_fmt enum AVSampleFormat { AV_SAMPLE_FMT_NONE = -1, AV_SAMPLE_FMT_U8, /// 3.FFMPEG中型：profile #define FF_PROFILE_UNKNOWN -99 #define FF_PROFILE_RESERVED -100 #define FF_PROFILE_AAC_MAIN 0 #define FF_PROFILE_AAC_LOW 1 #define FF_PROFILE_AAC_SSR 2 #define FF_PROFILE_AAC_LTP 3 #define FF_PROFILE_AAC_HE 4 #define FF_PROFILE_AAC_HE_V2 28 #define FF_PROFILE_AAC_LD 22 #define FF_PROFILE_AAC_ELD 38 #define FF_PROFILE_DTS 20 #define FF_PROFILE_DTS_ES 30 #define FF_PROFILE_DTS_96_24 40 #define FF_PROFILE_DTS_HD_HRA 50 #define FF_PROFILE_DTS_HD_MA 60 #define FF_PROFILE_MPEG2_422 0 #define FF_PROFILE_MPEG2_HIGH 1 #define FF_PROFILE_MPEG2_SS 2 #define FF_PROFILE_MPEG2_SNR_SCALABLE 3 #define FF_PROFILE_MPEG2_MAIN 4 #define FF_PROFILE_MPEG2_SIMPLE 5 #define FF_PROFILE_H264_CONSTRAINED (1 AVCodec （libavcodec/avcodec.h） AVCodec是存储编解码器信息的结构体。 const char *name：编解码器的名字，比较短 const char *long_name：编解码器的名字，全称，比较长 enum AVMediaType type：指明了类型，是视频，音频，还是字幕 enum AVCodecID id：ID，不重复 const AVRational *supported_framerates：支持的帧率（仅视频） const enum AVPixelFormat *pix_fmts：支持的像素格式（仅视频） const int *supported_samplerates：支持的采样率（仅音频） const enum AVSampleFormat *sample_fmts：支持的采样格式（仅音频） const uint64_t *channel_layouts：支持的声道数（仅音频） int priv_data_size：私有数据的大小 其中，AVMediaType结构体： enum AVMediaType { AVMEDIA_TYPE_UNKNOWN = -1, /// AVCodecID结构体： enum AVCodecID { AV_CODEC_ID_NONE, /* video codecs */ AV_CODEC_ID_MPEG1VIDEO, AV_CODEC_ID_MPEG2VIDEO, /// AVPixelFormat结构体： enum AVPixelFormat { AV_PIX_FMT_NONE = -1, AV_PIX_FMT_YUV420P, /// AVPacket （libavcodec/avcodec.h） AVPacket是存储压缩编码数据相关信息的结构体。例如对于H.264来说。1个AVPacket的data通常对应一个NAL。注意：在这里只是对应，而不是一模一样。他们之间有微小的差别：使用FFMPEG类库分离出多媒体文件中的H.264码流。因此在使用FFMPEG进行视音频处理的时候，常常可以将得到的AVPacket的data数据直接写成文件，从而得到视音频的码流文件。 int64_t pts：显示时间戳 int64_t dts：解码时间戳 uint8_t *data：压缩编码的数据 int size：data的大小 int stream_index：标识该AVPacket所属的视频/音频流。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/08_demuxing.html":{"url":"FFmpeg/08_demuxing.html","title":"Demuxing（解封装）","keywords":"","body":"Demuxing（解封装）FFmpeg解封装流程（1）avformat_open_input（2）avformat_find_stream_info（3）av_read_frame（4）avformat_close_input例子测试文件下载地址Demuxing（解封装） FFmpeg解封装流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转自雷神） 。 （1）avformat_open_input 创建并初始化AVFormatContext结构体，并把输入文件信息赋值到AVFormatContext中。 （2）avformat_find_stream_info 检索流信息，这个过程会检查输入流中信息是否存在异常，如：AVCodecContext中的extradata是否存在。 （3）av_read_frame 从文件中读取每一帧的数据到AVPacket中，得到解封装之前的数据。有些（很多吧？）解封装后的数据输出到一个文件中并不支持播放，如FLV。因为FLV解封装后的数据并不是完整一个H264格式视频数据和AAC格式音频数据，需要重新进行封装后再输出到文件中才能正常播放。（或者使用FFmpeg中的工具） （4）avformat_close_input 关闭并释放资源。 例子 参考官方例子doc/examples/demuxing_decoding.c。 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/03 * description: FFmpeg Demuxing（解封装） */ extern \"C\" { #include } int main() { const char *src_filename = \"source/Kobe.flv\"; const char *out_filename_v = \"output/kobe3.h264\";//Output file URL const char *out_filename_a = \"output/kobe.mp3\"; AVFormatContext *fmt_ctx; /**(1)*/ if (avformat_open_input(&fmt_ctx, src_filename, NULL, NULL) streams[video_idx]->codec->extradata包含了sps和pps的数据，需要组装成NALU /**(3)*/ while (av_read_frame(fmt_ctx, &pkt) >= 0) { AVPacket orig_pkt = pkt; //针对每一个AVPacket进行处理。 if(orig_pkt.stream_index == video_idx){ //封装成 NALU 形式的 av_bitstream_filter_filter(h264bsfc, fmt_ctx->streams[video_idx]->codec, NULL, &orig_pkt.data, &orig_pkt.size, orig_pkt.data, orig_pkt.size, 0); fprintf(stderr,\"Write Video Packet. size:%d\\tpts:%lld\\n\", orig_pkt.size, orig_pkt.pts); fwrite(orig_pkt.data, 1, orig_pkt.size, fp_video); } else if(orig_pkt.stream_index == audio_idx){ fprintf(stderr,\"Write Audio Packet. size:%d\\tpts:%lld\\n\", orig_pkt.size, orig_pkt.pts); } av_packet_unref(&orig_pkt); } av_bitstream_filter_close(h264bsfc); /**(4)*/ avformat_close_input(&fmt_ctx); } 测试文件下载地址 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/09_muxing.html":{"url":"FFmpeg/09_muxing.html","title":"Muxing（封装）","keywords":"","body":"Muxing（封装）FFmpeg解封装流程（1）avformat_alloc_output_context2（2）avformat_new_stream（3）avformat_write_header（4）av_interleaved_write_frame（5）av_write_trailer代码实现测试文件下载地址: Kobe.aac 、 Kobe.h264 。Muxing（封装） 本文主要参考雷神的最简单的基于FFmpeg的封装格式处理：视音频复用器（muxer） 与官方例子doc/examples/muxing.c 。 FFmpeg解封装流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 以上步骤主要方法的简单说明。 （1）avformat_alloc_output_context2 构造输出的AVFormatContext。 （2）avformat_new_stream 将新的流添加到AVFormatContext中。 （3）avformat_write_header 写入文件头 （4）av_interleaved_write_frame 通过解封装文件（或者创建）得到的AVPacket数据写入到文件中。 （5）av_write_trailer 写入文件尾 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/04 * description: FFmpeg 封装 */ #include #define __STDC_CONSTANT_MACROS #ifdef _WIN32 //Windows extern \"C\" { #include \"libavformat/avformat.h\" }; #else //Linux... #ifdef __cplusplus extern \"C\" { #endif #include #ifdef __cplusplus }; #endif #endif /* FIX: H.264 in some container format (FLV, MP4, MKV etc.) need \"h264_mp4toannexb\" bitstream filter (BSF) *Add SPS,PPS in front of IDR frame *Add start code (\"0,0,0,1\") in front of NALU H.264 in some container (MPEG2TS) don't need this BSF. */ //'1': Use H.264 Bitstream Filter #define USE_H264BSF 0 /* FIX:AAC in some container format (FLV, MP4, MKV etc.) need \"aac_adtstoasc\" bitstream filter (BSF) */ //'1': Use AAC Bitstream Filter #define USE_AACBSF 0 int main(int argc, char *argv[]) { AVOutputFormat *ofmt = NULL; //Input AVFormatContext and Output AVFormatContext AVFormatContext *ifmt_ctx_v = NULL, *ifmt_ctx_a = NULL, *ofmt_ctx = NULL; AVPacket pkt; int ret, i; int videoindex_v = -1, videoindex_out = -1; int audioindex_a = -1, audioindex_out = -1; int frame_index = 0; int64_t cur_pts_v = 0, cur_pts_a = 0; int writing_v = 1, writing_a = 1; const char *in_filename_v = \"source/kobe.h264\"; const char *in_filename_a = \"source/kobe.aac\"; const char *out_filename = \"output/kobe.flv\";//Output file URL remove(out_filename); //Input if ((ret = avformat_open_input(&ifmt_ctx_v, in_filename_v, 0, 0)) oformat; for (i = 0; i nb_streams; i++) { //Create output AVStream according to input AVStream if (ifmt_ctx_v->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO) { AVStream *out_stream = avformat_new_stream(ofmt_ctx, ifmt_ctx_v->streams[i]->codec->codec); videoindex_v = i; if (!out_stream) { printf(\"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } videoindex_out = out_stream->index; //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, ifmt_ctx_v->streams[i]->codec) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; break; } } for (i = 0; i nb_streams; i++) { //Create output AVStream according to input AVStream if (ifmt_ctx_a->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO) { AVStream *out_stream = avformat_new_stream(ofmt_ctx, ifmt_ctx_a->streams[i]->codec->codec); audioindex_a = i; if (!out_stream) { printf(\"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } audioindex_out = out_stream->index; //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, ifmt_ctx_a->streams[i]->codec) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; break; } } /* open the output file, if needed */ if (!(ofmt->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", out_filename, av_err2str(ret)); goto end; } } //Write file header if (avformat_write_header(ofmt_ctx, NULL) streams[videoindex_v]->time_base, cur_pts_a, ifmt_ctx_a->streams[audioindex_a]->time_base) = 0) { do { in_stream = ifmt_ctx->streams[pkt.stream_index]; out_stream = ofmt_ctx->streams[stream_index]; if (pkt.stream_index == videoindex_v) { //FIX：No PTS (Example: Raw H.264) //Simple Write PTS if (pkt.pts == AV_NOPTS_VALUE) { //Write PTS AVRational time_base1 = in_stream->time_base; //Duration between 2 frames (us) int64_t calc_duration = (double) AV_TIME_BASE / av_q2d(in_stream->r_frame_rate); //Parameters pkt.pts = (double) (frame_index * calc_duration) / (double) (av_q2d(time_base1) * AV_TIME_BASE); pkt.dts = pkt.pts; pkt.duration = (double) calc_duration / (double) (av_q2d(time_base1) * AV_TIME_BASE); frame_index++; } cur_pts_v = pkt.pts; break; } } while (av_read_frame(ifmt_ctx, &pkt) >= 0); } else { writing_v = 0; continue; } } else { ifmt_ctx = ifmt_ctx_a; stream_index = audioindex_out; if (av_read_frame(ifmt_ctx, &pkt) >= 0) { do { in_stream = ifmt_ctx->streams[pkt.stream_index]; out_stream = ofmt_ctx->streams[stream_index]; if (pkt.stream_index == audioindex_a) { //FIX：No PTS //Simple Write PTS if (pkt.pts == AV_NOPTS_VALUE) { //Write PTS AVRational time_base1 = in_stream->time_base; //Duration between 2 frames (us) int64_t calc_duration = (double) AV_TIME_BASE / av_q2d(in_stream->r_frame_rate); //Parameters pkt.pts = (double) (frame_index * calc_duration) / (double) (av_q2d(time_base1) * AV_TIME_BASE); pkt.dts = pkt.pts; pkt.duration = (double) calc_duration / (double) (av_q2d(time_base1) * AV_TIME_BASE); frame_index++; } cur_pts_a = pkt.pts; break; } } while (av_read_frame(ifmt_ctx, &pkt) >= 0); } else { writing_a = 0; continue; } } //Convert PTS/DTS pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding) (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding) (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base); pkt.pos = -1; pkt.stream_index = stream_index; printf(\"Write 1 Packet. size:%5d\\tpts:%lld\\n\", pkt.size, pkt.pts); //Write if (av_interleaved_write_frame(ofmt_ctx, &pkt) flags & AVFMT_NOFILE)) avio_close(ofmt_ctx->pb); avformat_free_context(ofmt_ctx); if (ret 测试文件下载地址: Kobe.aac 、 Kobe.h264 。 参考 https://blog.csdn.net/leixiaohua1020/article/details/39802913 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/10_remuxing.html":{"url":"FFmpeg/10_remuxing.html","title":"Remuxing（重新封装）","keywords":"","body":"Remuxing（重新封装）FFmpeg解封装流程官方例子测试文件下载地址: Kobe.flv 。Remuxing（重新封装） 本文来自官方例子doc/examples/remuxing.c 。 FFmpeg解封装流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 重新封装的原理就是把输入文件 解封装 之后，再进行 封装 输出到新的文件中。 官方例子 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/04 * description: FFmpeg 重新封装 */ #define __STDC_CONSTANT_MACROS extern \"C\" { #include #include } static void log_packet(const AVFormatContext *fmt_ctx, const AVPacket *pkt, const char *tag) { AVRational *time_base = &fmt_ctx->streams[pkt->stream_index]->time_base; printf(\"%s: pts:%s pts_time:%s dts:%s dts_time:%s duration:%s duration_time:%s stream_index:%d\\n\", tag, av_ts2str(pkt->pts), av_ts2timestr(pkt->pts, time_base), av_ts2str(pkt->dts), av_ts2timestr(pkt->dts, time_base), av_ts2str(pkt->duration), av_ts2timestr(pkt->duration, time_base), pkt->stream_index); } int main(int argc, char **argv) { AVOutputFormat *ofmt = NULL; AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL; AVPacket pkt; const char *in_filename, *out_filename; int ret, i; int stream_index = 0; int *stream_mapping = NULL; int stream_mapping_size = 0; in_filename = \"source/Kobe.flv\"; out_filename = \"output/Kobe.mp4\"; if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) nb_streams; stream_mapping = static_cast(av_mallocz_array(stream_mapping_size, sizeof(*stream_mapping))); if (!stream_mapping) { ret = AVERROR(ENOMEM); goto end; } ofmt = ofmt_ctx->oformat; for (i = 0; i nb_streams; i++) { AVStream *out_stream; AVStream *in_stream = ifmt_ctx->streams[i]; AVCodecParameters *in_codecpar = in_stream->codecpar; if (in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO && in_codecpar->codec_type != AVMEDIA_TYPE_VIDEO && in_codecpar->codec_type != AVMEDIA_TYPE_SUBTITLE) { stream_mapping[i] = -1; continue; } stream_mapping[i] = stream_index++; out_stream = avformat_new_stream(ofmt_ctx, NULL); if (!out_stream) { fprintf(stderr, \"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar); if (ret codecpar->codec_tag = 0; } av_dump_format(ofmt_ctx, 0, out_filename, 1); if (!(ofmt->flags & AVFMT_NOFILE)) { ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE); if (ret streams[pkt.stream_index]; if (pkt.stream_index >= stream_mapping_size || stream_mapping[pkt.stream_index] streams[pkt.stream_index]; log_packet(ifmt_ctx, &pkt, \"in\"); /* copy packet */ pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, static_cast(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, static_cast(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base); pkt.pos = -1; log_packet(ofmt_ctx, &pkt, \"out\"); ret = av_interleaved_write_frame(ofmt_ctx, &pkt); if (ret flags & AVFMT_NOFILE)) avio_closep(&ofmt_ctx->pb); avformat_free_context(ofmt_ctx); av_freep(&stream_mapping); if (ret 测试文件下载地址: Kobe.flv 。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/11_decode.html":{"url":"FFmpeg/11_decode.html","title":"Decode（解码）","keywords":"","body":"Decode（解码）FFmpeg解码流程官方例子【Audio】官方例子【Video】Decode（解码） 本文来自官方例子doc/examples/decode_audio.c 和 doc/examples/decode_video.c。 FFmpeg解码流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 官方例子【Audio】 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/06 * description: 来自官方例子：doc/examples/decode_audio.c */ #include #include #include extern \"C\"{ #include #include #include } #define AUDIO_INBUF_SIZE 20480 #define AUDIO_REFILL_THRESH 4096 static void decode(AVCodecContext *dec_ctx, AVPacket *pkt, AVFrame *frame, FILE *outfile) { int i, ch; int ret, data_size; //把AVPacket数据送去解码器 ret = avcodec_send_packet(dec_ctx, pkt); if (ret = 0) { //接到解码后的，读取到AVFrame中 ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret sample_fmt); if (data_size nb_samples; i++) for (ch = 0; ch channels; ch++) fwrite(frame->data[ch] + data_size*i, 1, data_size, outfile); } } int main(int argc, char **argv) { const char *outfilename, *filename; const AVCodec *codec; AVCodecContext *c= NULL; AVCodecParserContext *parser = NULL; int len, ret; FILE *f, *outfile; uint8_t inbuf[AUDIO_INBUF_SIZE + AV_INPUT_BUFFER_PADDING_SIZE]; uint8_t *data; size_t data_size; AVPacket *pkt; AVFrame *decoded_frame = NULL; filename = \"source/Kobe.aac\"; outfilename = \"output/Kebe.pcm\"; pkt = av_packet_alloc(); //通过AVCodecID查找到相应的AVCodec，类型需要输入文件解码类型保持一致 codec = avcodec_find_decoder(AV_CODEC_ID_AAC); if (!codec) { fprintf(stderr, \"Codec not found\\n\"); exit(1); } //获取解析器上下文 parser = av_parser_init(codec->id); if (!parser) { fprintf(stderr, \"Parser not found\\n\"); exit(1); } //获取解码器器上下文 c = avcodec_alloc_context3(codec); if (!c) { fprintf(stderr, \"Could not allocate audio codec context\\n\"); exit(1); } //初始化码器器上下文 if (avcodec_open2(c, codec, NULL) 0) { if (!decoded_frame) { if (!(decoded_frame = av_frame_alloc())) { fprintf(stderr, \"Could not allocate audio frame\\n\"); exit(1); } } //把输入的文件流进行解析，输出到AVPacket中 ret = av_parser_parse2(parser, c, &pkt->data, &pkt->size, data, data_size, AV_NOPTS_VALUE, AV_NOPTS_VALUE, 0); if (ret size){ //这部分主要是把上面AVPacket数据进行解码，最后得到裸流数据输出到AVFrame中 decode(c, pkt, decoded_frame, outfile); } if (data_size 0) data_size += len; } } /* flush the decoder */ pkt->data = NULL; pkt->size = 0; decode(c, pkt, decoded_frame, outfile); fclose(outfile); fclose(f); avcodec_free_context(&c); av_parser_close(parser); av_frame_free(&decoded_frame); av_packet_free(&pkt); return 0; } 测试文件下载地址: Kobe.aac 。 解析后的pcm原始数据可以使用ffplay或者audition等工具播放，而播放前必须先知道该原始数据的初始化信息，如通道数、位宽等，可以使用ffprobe进行查看解码前的aac文件，如：qincji:usr mac$ ffprobe ./VAFFmpeg/source/Kobe.aac ffprobe version 4.2.2 Copyright (c) 2007-2019 the FFmpeg developers built with Apple clang version 11.0.0 (clang-1100.0.33.12) configuration: libavutil 56. 31.100 / 56. 31.100 libavcodec 58. 54.100 / 58. 54.100 libavformat 58. 29.100 / 58. 29.100 libavdevice 58. 8.100 / 58. 8.100 libavfilter 7. 57.100 / 7. 57.100 libswscale 5. 5.100 / 5. 5.100 libswresample 3. 5.100 / 3. 5.100 [aac @ 0x7fec2b005200] Estimating duration from bitrate, this may be inaccurate Input #0, aac, from '/Users/Qincji/Desktop/develop/android/project/va/VAFFmpeg/source/Kobe.aac': Duration: 00:00:29.90, bitrate: 34 kb/s Stream #0:0: Audio: aac (HE-AAC), 44100 Hz, stereo, fltp, 34 kb/s 使用audition输入出文件实例： 官方例子【Video】 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/06 * description: 来自官方例子：doc/examples/decode_video.c */ #include #include #include extern \"C\"{ #include } #define INBUF_SIZE 4096 static void decode(AVCodecContext *dec_ctx, AVFrame *frame, AVPacket *pkt, FILE *outfile) { int ret; ret = avcodec_send_packet(dec_ctx, pkt); if (ret = 0) { ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret frame_number); fflush(stdout); //根据自己解码出来的数据格式进行处理 fwrite(frame->data[0],1,frame->width*frame->height,outfile);//Y fwrite(frame->data[1],1,frame->width*frame->height/4,outfile);//U fwrite(frame->data[2],1,frame->width*frame->height/4,outfile);//V } } int main(int argc, char **argv) { const char *filename, *outfilename; const AVCodec *codec; AVCodecParserContext *parser; AVCodecContext *c= NULL; FILE *f; AVFrame *frame; uint8_t inbuf[INBUF_SIZE + AV_INPUT_BUFFER_PADDING_SIZE]; uint8_t *data; size_t data_size; int ret; AVPacket *pkt; filename = \"source/Kobe.h264\"; outfilename = \"output/Kobe.yuv\"; FILE *outfile = fopen(outfilename, \"wb\"); pkt = av_packet_alloc(); if (!pkt) exit(1); /* set end of buffer to 0 (this ensures that no overreading happens for damaged MPEG streams) */ memset(inbuf + INBUF_SIZE, 0, AV_INPUT_BUFFER_PADDING_SIZE); /* find the MPEG-1 video decoder */ codec = avcodec_find_decoder(AV_CODEC_ID_H264); if (!codec) { fprintf(stderr, \"Codec not found\\n\"); exit(1); } parser = av_parser_init(codec->id); if (!parser) { fprintf(stderr, \"parser not found\\n\"); exit(1); } c = avcodec_alloc_context3(codec); if (!c) { fprintf(stderr, \"Could not allocate video codec context\\n\"); exit(1); } /* For some codecs, such as msmpeg4 and mpeg4, width and height MUST be initialized there because this information is not available in the bitstream. */ /* open it */ if (avcodec_open2(c, codec, NULL) 0) { ret = av_parser_parse2(parser, c, &pkt->data, &pkt->size, data, data_size, AV_NOPTS_VALUE, AV_NOPTS_VALUE, 0); if (ret size) decode(c, frame, pkt, outfile); } } /* flush the decoder */ decode(c, frame, NULL, outfile); fclose(f); av_parser_close(parser); avcodec_free_context(&c); av_frame_free(&frame); av_packet_free(&pkt); return 0; } 测试文件下载地址: Kobe.h264 。 解析后的yuv原始数据可以使用ffplay等工具播放，而播放前必须先知道该原始数据的宽高等信息，可以使用ffprobe进行查看解码前的h264文件，如：qincji:usr mac$ ffprobe /Users/Qincji/Desktop/develop/android/project/va/VAFFmpeg/source/Kobe.h264 ffprobe version 4.2.2 Copyright (c) 2007-2019 the FFmpeg developers built with Apple clang version 11.0.0 (clang-1100.0.33.12) configuration: libavutil 56. 31.100 / 56. 31.100 libavcodec 58. 54.100 / 58. 54.100 libavformat 58. 29.100 / 58. 29.100 libavdevice 58. 8.100 / 58. 8.100 libavfilter 7. 57.100 / 7. 57.100 libswscale 5. 5.100 / 5. 5.100 libswresample 3. 5.100 / 3. 5.100 Input #0, h264, from '/Users/Qincji/Desktop/develop/android/project/va/VAFFmpeg/source/Kobe.h264': Duration: N/A, bitrate: N/A Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 15 fps, 15 tbr, 1200k tbn, 30 tbc 使用ffplay播放： ffplay -f rawvideo -video_size 384x216 output/Kobe.yuv 输入出文件实例： Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/12_encode.html":{"url":"FFmpeg/12_encode.html","title":"Encode（编码）","keywords":"","body":"Encode（编码）FFmpeg编码流程官方例子【Audio】官方例子【Video】Encode（编码） 本文来自官方例子doc/examples/encode_audio.c 和 doc/examples/encode_video.c。 FFmpeg编码流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 官方例子【Audio】 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/07 * description: 来自官方例子：doc/examples/encode_audio.c * 主要思路： * 1、初始化编码器信息 * 2、从文件（或者生成）读取裸流数据到每个AVFrame中 * 3、把每个AVFrame通过编码器生成编码后数据的AVPacket * 4、使用AVFormatContext封装到文件中进行输出 * * */ #include #include #include extern \"C\"{ #include #include #include #include #include #include } /* check that a given sample format is supported by the encoder */ static int check_sample_fmt(const AVCodec *codec, enum AVSampleFormat sample_fmt) { const enum AVSampleFormat *p = codec->sample_fmts; while (*p != AV_SAMPLE_FMT_NONE) { if (*p == sample_fmt) return 1; p++; } return 0; } /* just pick the highest supported samplerate */ static int select_sample_rate(const AVCodec *codec) { const int *p; int best_samplerate = 0; if (!codec->supported_samplerates) return 44100; p = codec->supported_samplerates; while (*p) { if (!best_samplerate || abs(44100 - *p) channel_layouts) return AV_CH_LAYOUT_STEREO; p = codec->channel_layouts; while (*p) { int nb_channels = av_get_channel_layout_nb_channels(*p); if (nb_channels > best_nb_channels) { best_ch_layout = *p; best_nb_channels = nb_channels; } p++; } return best_ch_layout; } static int f_index = 0; static void encode(AVCodecContext *ctx, AVFrame *frame, AVPacket *pkt, FILE *output,AVFormatContext *ofmt_ctx) { int ret; /* send the frame for encoding */ ret = avcodec_send_frame(ctx, frame); if (ret = 0) { ret = avcodec_receive_packet(ctx, pkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret pts = f_index *100; pkt->dts = f_index *100; pkt->pos = -1; pkt->stream_index = 0; printf(\"Write 1 Packet. size:%5d\\tpts:%lld\\n\", pkt->size, pkt->pts); //Write if (av_interleaved_write_frame(ofmt_ctx, pkt) bit_rate = 34000; c->sample_fmt = AV_SAMPLE_FMT_FLTP; //音频：检测当前编码器是否支持采样格式 if (!check_sample_fmt(codec, c->sample_fmt)) { fprintf(stderr, \"Encoder does not support sample format %s\", av_get_sample_fmt_name(c->sample_fmt)); exit(1); } /* select other audio parameters supported by the encoder */ c->sample_rate = select_sample_rate(codec); c->channel_layout = select_channel_layout(codec); c->channels = av_get_channel_layout_nb_channels(c->channel_layout); //初始化AVCodecContext if (avcodec_open2(c, codec, NULL) nb_samples = c->frame_size; frame->format = c->sample_fmt; frame->channel_layout = c->channel_layout; /* allocate the data buffers */ ret = av_frame_get_buffer(frame, 0); if (ret oformat; AVStream *out_stream = avformat_new_stream(ofmt_ctx, codec); //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, c) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; /* open the output file, if needed */ if (!(ofmt->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, outFilename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", outFilename, av_err2str(ret)); return -1; } } //Write file header //（封装——文件头） if (avformat_write_header(ofmt_ctx, NULL) sample_fmt); while (!feof(inFile)){ /* make sure the frame is writable -- makes a copy if the encoder * kept a reference internally */ ret = av_frame_make_writable(frame); if (ret frame_size; i++) for (j = 0; j channels; j++) fread(frame->data[j] + data_size*i, 1, data_size, inFile); encode(c, frame, pkt, outFile,ofmt_ctx); } /* flush the encoder */ encode(c, NULL, pkt, outFile,ofmt_ctx); printf(\"Write file trailer.\\n\"); //Write file trailer//（封装——文件尾） av_write_trailer(ofmt_ctx); fclose(outFile); av_frame_free(&frame); av_packet_free(&pkt); avcodec_free_context(&c); avformat_free_context(ofmt_ctx); return 0; } 官方例子【Video】 改动的本例子，使用x264库进行解码。所以需要集成x264库，请参考 编译ffmpeg4.2.2 。 我这里使用在MacOS上面跑的例子，以下是我本例编译ffmpeg的脚本： #!/bin/bash make clean function build_x264() { cd libx264 ./configure \\ --prefix=${X264_LIBS} \\ --disable-static \\ --enable-shared || exit 0 make clean make install } function build_macosx() { cd .. ./configure \\ --prefix=$PREFIX \\ --disable-programs \\ --enable-ffmpeg \\ --target-os=darwin \\ --disable-static \\ --enable-shared \\ --extra-cflags=\"-fPIC\" \\ --ln_s=\"ln -s\" \\ --pkg-config=$(which pkg-config) \\ --enable-gpl \\ --extra-cflags=\"-I${X264_LIBS}/include\" \\ --extra-ldflags=\"-L${X264_LIBS}/lib\" \\ --enable-libx264 || exit 0 make clean make install } PREFIX=$(pwd)/macox X264_LIBS=$(pwd)/libx264/libouput build_x264 build_macosx 编译完成后，会在项目根目录上传ffmpeg工具，可以通过以下操作查看是否正常集成： ./ffmpeg -re -i input.mp4 -vcodec libx264 -an output.mp4 编码例子： /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/07 * description: 来自官方例子：doc/examples/encode_video.c * 主要思路： * 1、初始化编码器信息 * 2、从文件（或者生成）读取裸流数据到每个AVFrame中 * 3、把每个AVFrame通过编码器生成编码后数据的AVPacket * 4、使用AVFormatContext封装到文件中进行输出 * * */ #include #include #include extern \"C\" { #include #include #include #include #include #include #include #include } static int f_index = 0; static void encode(AVCodecContext *ctx, AVFrame *frame, AVPacket *pkt, FILE *output, AVFormatContext *ofmt_ctx) { int ret; //注意：需要在送去编码器前指定 pts frame->pts = f_index++; /* send the frame for encoding */ ret = avcodec_send_frame(ctx, frame); if (ret = 0) { ret = avcodec_receive_packet(ctx, pkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret stream_index = 0; printf(\"Write 1 Packet. size:%5d\\tpts:%lld\\n\", pkt->size, pkt->pts); //Write if (av_interleaved_write_frame(ofmt_ctx, pkt) bit_rate = 120000; /* resolution must be a multiple of two */ c->width = 384; c->height = 216; /* frames per second */ c->time_base = (AVRational) {1, 15}; c->framerate = (AVRational) {15, 1}; /* emit one intra frame every ten frames * check frame pict_type before passing frame * to encoder, if frame->pict_type is AV_PICTURE_TYPE_I * then gop_size is ignored and the output of encoder * will always be I frame irrespective to gop_size */ c->gop_size = 10; c->max_b_frames = 1; c->pix_fmt = AV_PIX_FMT_YUV420P; if (codec->id == AV_CODEC_ID_H264) av_opt_set(c->priv_data, \"preset\", \"slow\", 0); //初始化AVCodecContext if (avcodec_open2(c, codec, NULL) format = c->pix_fmt; frame->width = c->width; frame->height = c->height; /* allocate the data buffers */ ret = av_frame_get_buffer(frame, 32); if (ret oformat; AVStream *out_stream = avformat_new_stream(ofmt_ctx, codec); //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, c) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; /* open the output file, if needed */ if (!(ofmt->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, outFilename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", outFilename, av_err2str(ret)); return -1; } } //Write file header //（封装——文件头） if (avformat_write_header(ofmt_ctx, NULL) data[0], 1, frame->height * frame->width, inFile); //Y fread(frame->data[1], 1, frame->height * frame->width/4, inFile); //U fread(frame->data[2], 1, frame->height * frame->width/4, inFile); //V encode(c, frame, pkt, outFile, ofmt_ctx); } /* flush the encoder */ encode(c, NULL, pkt, outFile, ofmt_ctx); printf(\"Write file trailer.\\n\"); //Write file trailer//（封装——文件尾） av_write_trailer(ofmt_ctx); fclose(outFile); av_frame_free(&frame); av_packet_free(&pkt); avcodec_free_context(&c); avformat_free_context(ofmt_ctx); return 0; } 测试的输入数据从 Decode（解码） 生成。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/13_transfer.html":{"url":"FFmpeg/13_transfer.html","title":"简单实现转码","keywords":"","body":"简单实现转码转码原理代码实现简单实现转码 本文汇总前面几篇文章，把所有流程合并到一块，简单实现转码的流程。其中有些异常不做处理。 转码原理 先看雷神的一张图： 上图描述的很明白，上完图发现已经不需要语言描述了[Dog]。 但还是画个来说明一下这块之间的联系： 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/08 * description: 转码 * 主要思路： * 格式1-->解封装--[解封装数据AVPacket]-->解码--[原始数据AVFrame]-->编码-->[编码后数据AVPacket]-->封装-->格式2 * * 2021/01/11 增加音频重采样变换 * */ #include #include extern \"C\" { #include #include #include #include #include #include #include #include #include } int main(int argc, char **argv) { // const char *in_filename = \"source/lol.mp4\"; // const char *out_filename = \"output/lol.wma\"; // const char *in_filename = \"source/Kobe.flv\"; // const char *out_filename = \"output/Kobe.avi\"; // const char *in_filename = \"source/lol.mp4\"; // const char *out_filename = \"output/lol.avi\"; const char *in_filename = \"source/Kobe.flv\"; const char *out_filename = \"output/Kobe2.avi\"; remove(out_filename); SwrContext *swr_ctx;//当有需要时，需要重采样 AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL; AVPacket *ipkt = NULL, *opkt = NULL; AVFrame *iframe = NULL, *oframe_a = NULL; AVCodecContext *ic_v = NULL, *ic_a = NULL, *oc_v = NULL, *oc_a = NULL; AVCodec *ocodec_v = NULL, *ocodec_a = NULL; int iindex_v = -1, iindex_a = -1, oindex_v = -1, oindex_a = -1, ret, dst_nb_samples; ipkt = av_packet_alloc(); opkt = av_packet_alloc(); /**（解封装 1.1）：创建并初始化AVFormatContext*/ if (avformat_open_input(&ifmt_ctx, in_filename, NULL, NULL) streams[iindex_v]->codec; if (!(ic_v->codec = avcodec_find_decoder(ic_v->codec_id))) { fprintf(stderr, \"Could find in video AVCodec\\n\"); goto end; } ic_a = ifmt_ctx->streams[iindex_a]->codec; if (!(ic_a->codec = avcodec_find_decoder(ic_a->codec_id))) { fprintf(stderr, \"Could find in audio AVCodec\\n\"); goto end; } /**（解码 2.2）：初始化码器器上下文*/ if (avcodec_open2(ic_v, ic_v->codec, NULL) codec, NULL) oformat->video_codec))) { fprintf(stderr, \"Could find out video AVCodec\\n\"); goto end; } if (!(ocodec_a = avcodec_find_encoder(ofmt_ctx->oformat->audio_codec))) { fprintf(stderr, \"Could find out audio AVCodec\\n\"); goto end; } /**（封装 4.3）：创建输出视频和音频AVStream，并与关联上AVFormatContext。（编码 3.1）：创建我们所需的编码器*/ for (int i = 0; i nb_streams; i++) { if (i != iindex_v && i != iindex_a) { continue; } AVStream *out_stream = avformat_new_stream(ofmt_ctx, NULL); if (!out_stream) { fprintf(stderr, \"Failed allocating output stream\\n\"); goto end; } out_stream->codecpar->codec_tag = 0; /*********************************************************************************************/ /** 这一块是编码器的参数信息设置，ffmpeg生产的默认编码器信息可能不是我们想要的，需要根据具体情况进行设置 **/ /*********************************************************************************************/ if (i == iindex_v) { oindex_v = out_stream->index; // oc_v = ofmt_ctx->streams[oindex_v]->codec; //这是旧版的写法 if (!(oc_v = avcodec_alloc_context3(ocodec_v))) { fprintf(stderr, \"Could not allow out AVCodecContext v\\n\"); goto end; } oc_v->height = ic_v->height; oc_v->width = ic_v->width; oc_v->sample_aspect_ratio = ic_v->sample_aspect_ratio; oc_v->pix_fmt = ocodec_v->pix_fmts[0]; oc_v->time_base = ic_v->time_base; oc_v->has_b_frames = ic_v->has_b_frames; //输出将相对于输入延迟max_b_frames + 1-->但是输入的为0！ // oc_v->max_b_frames = ic_v->max_b_frames + 1; oc_v->max_b_frames = 2; oc_v->bit_rate = ic_v->bit_rate; oc_v->codec_type = ic_v->codec_type; if ((ret = avcodec_parameters_from_context(out_stream->codecpar, oc_v)) index; // oc_a = ofmt_ctx->streams[oindex_a]->codec; if (!(oc_a = avcodec_alloc_context3(ocodec_a))) { fprintf(stderr, \"Could not allow out AVCodecContext v\\n\"); goto end; } oc_a->sample_rate = ic_a->sample_rate; oc_a->channel_layout = ic_a->channel_layout; oc_a->channels = av_get_channel_layout_nb_channels(ic_a->channel_layout); oc_a->sample_fmt = ocodec_a->sample_fmts[0]; oc_a->time_base = {1, ic_a->sample_rate}; oc_a->bit_rate = ic_a->bit_rate; oc_a->codec_type = ic_a->codec_type; if ((ret = avcodec_parameters_from_context(out_stream->codecpar, oc_a)) oformat->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", out_filename, av_err2str(ret)); goto end; } } /**（重采样 5.1）：申请内存*/ if (!(swr_ctx = swr_alloc())) { fprintf(stderr, \"Could not allocate resampler context\\n\"); goto end; } /**（重采样 5.2）：设置参数*/ av_opt_set_int(swr_ctx, \"in_channel_count\", ic_a->channels, 0); av_opt_set_int(swr_ctx, \"in_sample_rate\", ic_a->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"in_sample_fmt\", ic_a->sample_fmt, 0); av_opt_set_int(swr_ctx, \"out_channel_count\", oc_a->channels, 0); av_opt_set_int(swr_ctx, \"out_sample_rate\", oc_a->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"out_sample_fmt\", oc_a->sample_fmt, 0); /**（重采样 5.3）：初始化SwrContext*/ if ((ret = swr_init(swr_ctx)) = 0) { if (!iframe) { if (!(iframe = av_frame_alloc())) { fprintf(stderr, \"Could not allocate iframe\\n\"); goto end; } } if (!oframe_a) { if (!(oframe_a = av_frame_alloc())) { fprintf(stderr, \"Could not allocate oframe_a\\n\"); goto end; } oframe_a->format = oc_a->sample_fmt; oframe_a->channel_layout = oc_a->channel_layout; oframe_a->sample_rate = oc_a->sample_rate; oframe_a->nb_samples = oc_a->frame_size; if (oframe_a->nb_samples) { ret = av_frame_get_buffer(oframe_a, 0); if (ret stream_index == iindex_v) { ic_temp = ic_v; oc_temp = oc_v; oindex_temp = oindex_v; } else if (ipkt->stream_index == iindex_a) { ic_temp = ic_a; oc_temp = oc_a; oindex_temp = oindex_a; } else { fprintf(stderr, \"has other streams?\\n\"); continue; } in_stream = ifmt_ctx->streams[ipkt->stream_index]; out_stream = ofmt_ctx->streams[oindex_temp]; /**（解码 2.3）：将得到的AVPacket送去解码器*/ ret = avcodec_send_packet(ic_temp, ipkt); if (ret = 0) { //接到解码后的，读取到AVFrame中 /**（解码 2.4）：从解码器中得到的数据到AVFrame*/ ret = avcodec_receive_frame(ic_temp, iframe); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret stream_index == iindex_a && iframe->nb_samples != oc_temp->frame_size) { oframe_a->nb_samples = oc_temp->frame_size; // dst_nb_samples = av_rescale_rnd(swr_get_delay(swr_ctx, ic_a->sample_rate) + // ic_a->frame_size, oc_a->sample_rate, ic_a->sample_rate, AV_ROUND_UP); /**（重采样 5.4）：重新采样*/ ret = swr_convert(swr_ctx, oframe_a->data, oc_a->frame_size, (const uint8_t **) iframe->data, ic_a->frame_size); if (ret = 0) { /**（编码 3.3）：从编码器中得到编码后数据，放入AVPacket中*/ ret = avcodec_receive_packet(oc_temp, opkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break; else if (ret 从输入的信息转换成输出的信息；参考官方例子：doc/examples/remuxing.c opkt->stream_index = oindex_temp; opkt->pts = av_rescale_q_rnd(ipkt->pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); opkt->dts = av_rescale_q_rnd(ipkt->dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); opkt->duration = av_rescale_q(ipkt->duration, in_stream->time_base, out_stream->time_base); opkt->pos = -1; // printf(\"in Read 1 Packet. size:%5d\\tstream_index:% d\\tdts:%lld\\tpts:%lld\\tduration:%lld\\tcur_dts:%lld\\n\", // ipkt->size, ipkt->stream_index, ipkt->dts, ipkt->pts, ipkt->duration, // ofmt_ctx->streams[oindex_temp]->cur_dts); printf(\"out Write 1 Packet. size:%5d\\tstream_index:% d\\tdts:%lld\\tpts:%lld\\tduration:%lld\\tcur_dts:%lld\\n\", opkt->size, opkt->stream_index, opkt->dts, opkt->pts, opkt->duration, ofmt_ctx->streams[oindex_temp]->cur_dts); /**（封装 4.6）：写入数据*/ if (av_interleaved_write_frame(ofmt_ctx, opkt) 参考 https://blog.csdn.net/leixiaohua1020/article/details/26838535 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/14_filter_v.html":{"url":"FFmpeg/14_filter_v.html","title":"Filter和SDL（Video）","keywords":"","body":"Filter和SDL（Video）使用滤镜流程代码实现CMakeList配置测试文件下载地址Filter和SDL（Video） 本文主要来自官方例子doc/examples/filtering_video.c 。 滤镜官方语法 , 推荐参考《FFmpeg从入门到精通》。 使用滤镜流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/09 * description: 来自官方例子：doc/examples/filtering_video.c * */ #include #include #include #define _XOPEN_SOURCE 600 /* for usleep */ #include extern \"C\" { #include #include #include #include #include #include }; //swapuv //const char *filter_descr = \"scale=78:24,transpose=cclock\"; //const char *filter_descr = \"movie=logo.png[wm];[in][wm]overlay=30:10[out]\"; const char *filter_descr = \"scale=iw/2:ih/2[in_tmp];[in_tmp]split=4[in_1][in_2][in_3][in_4];[in_1]pad=iw*2:ih*2[a];[a][in_2]overlay=w[b];[b][in_3]overlay=0:h[d];[d][in_4]overlay=w:h[out]\"; /* other way: scale=78:24 [scl]; [scl] transpose=cclock // assumes \"[in]\" and \"[out]\" to be input output pads respectively */ static AVFormatContext *fmt_ctx; static AVCodecContext *dec_ctx; AVFilterContext *buffersink_ctx; AVFilterContext *buffersrc_ctx; AVFilterGraph *filter_graph; static int video_stream_index = -1; static int64_t last_pts = AV_NOPTS_VALUE; static int open_input_file(const char *filename) { int ret; AVCodec *dec; if ((ret = avformat_open_input(&fmt_ctx, filename, NULL, NULL)) streams[video_stream_index]->codecpar); /* init the video decoder */ if ((ret = avcodec_open2(dec_ctx, dec, NULL)) streams[video_stream_index]->time_base; enum AVPixelFormat pix_fmts[] = {AV_PIX_FMT_YUV420P, AV_PIX_FMT_NONE};//注意输入的格式类型pix_fmts[0] filter_graph = avfilter_graph_alloc(); if (!outputs || !inputs || !filter_graph) { ret = AVERROR(ENOMEM); goto end; } /* buffer video source: the decoded frames from the decoder will be inserted here. */ snprintf(args, sizeof(args), \"video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\", dec_ctx->width, dec_ctx->height, dec_ctx->pix_fmt, time_base.num, time_base.den, dec_ctx->sample_aspect_ratio.num, dec_ctx->sample_aspect_ratio.den); //创建和初始化过滤器实例并将其添加到现有图形中。【输入】 ret = avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, \"in\", args, NULL, filter_graph); if (ret name = av_strdup(\"in\"); outputs->filter_ctx = buffersrc_ctx; outputs->pad_idx = 0; outputs->next = NULL; /* * The buffer sink input must be connected to the output pad of * the last filter described by filters_descr; since the last * filter output label is not specified, it is set to \"out\" by * default. */ inputs->name = av_strdup(\"out\"); inputs->filter_ctx = buffersink_ctx; inputs->pad_idx = 0; inputs->next = NULL; //将字符串描述的图形添加到图形中。 if ((ret = avfilter_graph_parse_ptr(filter_graph, filters_descr, &inputs, &outputs, NULL)) width, dec_ctx->height, SDL_WINDOW_OPENGL ); if (!screen){ printf(\"SDL_CreateWindow() failed: %s\\n\", SDL_GetError()); goto end; } // B3. 创建SDL_Renderer // SDL_Renderer：渲染器 sdl_renderer = SDL_CreateRenderer(screen, -1, 0); // B4. 创建SDL_Texture // 一个SDL_Texture对应一帧YUV数据，同SDL 1.x中的SDL_Overlay // 此处第2个参数使用的是SDL中的像素格式，对比参考注释A7 // FFmpeg中的像素格式AV_PIX_FMT_YUV420P对应SDL中的像素格式SDL_PIXELFORMAT_IYUV sdl_texture = SDL_CreateTexture(sdl_renderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, dec_ctx->width, dec_ctx->height); sdl_rect.x = 0; sdl_rect.y = 0; sdl_rect.w = dec_ctx->width; sdl_rect.h = dec_ctx->height; ret = 1; end: return ret; } static void sdl_play(const AVFrame *frame){ // B5. 使用新的YUV像素数据更新SDL_Rect SDL_UpdateYUVTexture(sdl_texture, // sdl texture &sdl_rect, // sdl rect &sdl_rect frame->data[0], // y plane frame->linesize[0], // y pitch frame->data[1], // u plane frame->linesize[1], // u pitch frame->data[2], // v plane frame->linesize[2] // v pitch ); // B6. 使用特定颜色清空当前渲染目标 SDL_RenderClear(sdl_renderer); // B7. 使用部分图像数据(texture)更新当前渲染目标 SDL_RenderCopy(sdl_renderer, // sdl renderer sdl_texture, // sdl texture NULL, // src rect, if NULL copy texture &sdl_rect // dst rect ); // B8. 执行渲染，更新屏幕显示 SDL_RenderPresent(sdl_renderer); // B9. 控制帧率为25FPS，此处不够准确，未考虑解码消耗的时间 //Delay 40ms SDL_Event e; while (SDL_PollEvent(&e)) { SDL_Delay(40); if (e.type == SDL_QUIT) { break; } } } static void display_frame(const AVFrame *frame, AVRational time_base) { int x, y; uint8_t *p0, *p; int64_t delay; if (frame->pts != AV_NOPTS_VALUE) { if (last_pts != AV_NOPTS_VALUE) { /* sleep roughly the right amount of time; * usleep is in microseconds, just like AV_TIME_BASE. */ delay = av_rescale_q(frame->pts - last_pts, time_base, AV_TIME_BASE_Q); if (delay > 0 && delay pts; } sdl_play(frame); /* Trivial ASCII grayscale display. */ // p0 = frame->data[0]; // puts(\"\\033c\"); /*for (y = 0; y height; y++) { p = p0; for (x = 0; x width; x++) putchar(\" .-+#\"[*(p++) / 52]); putchar('\\n'); p0 += frame->linesize[0]; } fflush(stdout);*/ } int main(int argc, char **argv) { int ret; AVPacket packet; AVFrame *frame; AVFrame *filt_frame; const char *filename = \"source/Kobe.flv\"; // const char *filename = \"source/lol.mp4\"; frame = av_frame_alloc(); filt_frame = av_frame_alloc(); if (!frame || !filt_frame) { perror(\"Could not allocate frame\"); exit(1); } //获取解封装上下文（AVFormatContext）和解码器，并查找视频通道 if ((ret = open_input_file(filename)) = 0) { //获取解码后数据 ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret pts = frame->best_effort_timestamp; /**将解码后得到的数据通过滤镜处理*/ /* push the decoded frame into the filtergraph */ if (av_buffersrc_add_frame_flags(buffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF) inputs[0]->time_base); av_frame_unref(filt_frame); } av_frame_unref(frame); } } av_packet_unref(&packet); } end: SDL_Quit(); avfilter_graph_free(&filter_graph); avcodec_free_context(&dec_ctx); avformat_close_input(&fmt_ctx); av_frame_free(&frame); av_frame_free(&filt_frame); if (ret CMakeList配置 cmake_minimum_required(VERSION 3.17) project(VAFFmpeg) set(CMAKE_CXX_STANDARD 11) set(SOURCE_FILES main.cpp) link_directories(lib) include_directories(include) #引入sdl2库 find_package(SDL2 REQUIRED) include_directories(VAFFmpeg ${SDL2_INCLUDE_DIRS}) add_executable(VAFFmpeg ${SOURCE_FILES}) target_link_libraries( VAFFmpeg ${SDL2_LIBRARIES} -lavcodec -lavdevice -lavfilter -lavformat -lswresample -lswscale # -lx264 ) 测试文件下载地址 参考 https://blog.csdn.net/leixiaohua1020/article/details/29368911 https://blog.csdn.net/leixiaohua1020/article/details/40876089 FFmpeg从入门到精通 其他：忘记留地址了[尴尬] Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/15_filter_a.html":{"url":"FFmpeg/15_filter_a.html","title":"Filter和SDL（Audio）","keywords":"","body":"Filter和SDL（Audio）使用滤镜流程代码实现测试文件下载地址Filter和SDL（Audio） 本文主要来自官方例子doc/examples/filtering_video.c 。 滤镜官方语法 , 推荐参考《FFmpeg从入门到精通》。 使用滤镜流程 参考上一篇视频滤镜使用流程 。注意以下一点： 获取滤镜器的名称输入：avfilter_get_by_name(\"buffer\") -> avfilter_get_by_name(\"abuffer\") 输出：avfilter_get_by_name(\"buffersink\") -> avfilter_get_by_name(\"abuffersink\") 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/10 * description: 来自官方例子：doc/examples/filtering_audio.c * */ #include #include #include #define _XOPEN_SOURCE 600 /* for usleep */ #include extern \"C\" { #include #include #include #include #include #include }; //fltp //static const char *filter_descr = \"aresample=44100,aformat=sample_fmts=fltp:channel_layouts=mono\"; static const char *filter_descr = \"aecho=0.8:0.88:60:0.4\";//参考：http://ffmpeg.org/ffmpeg-filters.html#aecho static const char *player = \"ffplay -f s16le -ar 8000 -ac 1 -\"; static AVFormatContext *fmt_ctx; static AVCodecContext *dec_ctx; AVFilterContext *buffersink_ctx; AVFilterContext *buffersrc_ctx; AVFilterGraph *filter_graph; static int audio_stream_index = -1; static int open_input_file(const char *filename) { int ret; AVCodec *dec; if ((ret = avformat_open_input(&fmt_ctx, filename, NULL, NULL)) streams[audio_stream_index]->codecpar); /* init the audio decoder */ if ((ret = avcodec_open2(dec_ctx, dec, NULL)) streams[audio_stream_index]->time_base; filter_graph = avfilter_graph_alloc(); if (!outputs || !inputs || !filter_graph) { ret = AVERROR(ENOMEM); goto end; } /* buffer audio source: the decoded frames from the decoder will be inserted here. */ if (!dec_ctx->channel_layout) dec_ctx->channel_layout = av_get_default_channel_layout(dec_ctx->channels); snprintf(args, sizeof(args), \"time_base=%d/%d:sample_rate=%d:sample_fmt=%s:channel_layout=0x%lld\", time_base.num, time_base.den, dec_ctx->sample_rate, av_get_sample_fmt_name(dec_ctx->sample_fmt), dec_ctx->channel_layout); ret = avfilter_graph_create_filter(&buffersrc_ctx, abuffersrc, \"in\", args, NULL, filter_graph); if (ret name = av_strdup(\"in\"); outputs->filter_ctx = buffersrc_ctx; outputs->pad_idx = 0; outputs->next = NULL; /* * The buffer sink input must be connected to the output pad of * the last filter described by filters_descr; since the last * filter output label is not specified, it is set to \"out\" by * default. */ inputs->name = av_strdup(\"out\"); inputs->filter_ctx = buffersink_ctx; inputs->pad_idx = 0; inputs->next = NULL; if ((ret = avfilter_graph_parse_ptr(filter_graph, filters_descr, &inputs, &outputs, NULL)) inputs[0]; av_get_channel_layout_string(args, sizeof(args), -1, outlink->channel_layout); av_log(NULL, AV_LOG_INFO, \"Output: srate:%dHz fmt:%s chlayout:%s\\n\", (int) outlink->sample_rate, (char *) av_x_if_null(av_get_sample_fmt_name(static_cast(outlink->format)), \"?\"), args); end: avfilter_inout_free(&inputs); avfilter_inout_free(&outputs); return ret; } static Uint8 *audio_chunk; static Uint32 audio_len; static Uint8 *audio_pos; void fill_audio(void *udata, Uint8 *stream, int len) { //SDL 2.0 SDL_memset(stream, 0, len); if (audio_len == 0) return; len = (len > audio_len ? audio_len : len); SDL_MixAudio(stream, audio_pos, len, SDL_MIX_MAXVOLUME); audio_pos += len; audio_len -= len; } //https://blog.csdn.net/leixiaohua1020/article/details/40544521 static int init_sdl(AVCodecContext *dec_ctx) { int ret = -1; // B1. 初始化SDL子系统：缺省(事件处理、文件IO、线程)、视频、音频、定时器 if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) { printf(\"SDL_Init() failed: %s\\n\", SDL_GetError()); goto end; } //注意：这里设置的参数会算出 audio_chunk 所使用的长度 //audio_chunk = 采样数 * 通道数 * 位宽 SDL_AudioSpec wanted_spec; wanted_spec.freq = dec_ctx->sample_rate; // wanted_spec.format = dec_ctx->sample_fmt; wanted_spec.format = AUDIO_F32SYS;//位宽=4 wanted_spec.channels = dec_ctx->channels;//通道数 wanted_spec.silence = 0; wanted_spec.samples = dec_ctx->frame_size;//采样数 wanted_spec.callback = fill_audio; if (SDL_OpenAudio(&wanted_spec, NULL) data[0][0] == '\\0') {//没有数据？ return; } int i, ch, data_size; data_size = av_get_bytes_per_sample(dec_ctx->sample_fmt);//每一个采样点所占的字节数 Uint32 len = data_size * frame->nb_samples * dec_ctx->channels;//所有通道采样数所占字节长度（一帧大小） Uint8 *all_channels_buf = (Uint8 *) malloc(len); int index = 0; //把所有通道采样数据重新排列 for (i = 0; i nb_samples; i++) { for (ch = 0; ch channels; ch++) { memcpy(all_channels_buf + index * data_size, frame->data[ch] + data_size * i, data_size); ++index; } } //把一帧数据设置给SDL播放器 audio_chunk = all_channels_buf; audio_len = len; audio_pos = audio_chunk; while (audio_len > 0)//Wait until finish SDL_Delay(1); free(all_channels_buf); } static void print_frame(const AVFrame *frame) { sdl_play(frame); /*const int n = frame->nb_samples * av_get_channel_layout_nb_channels(frame->channel_layout); const uint16_t *p = (uint16_t *) frame->data[0]; const uint16_t *p_end = p + n; while (p > 8 & 0xff, stdout); p++; } fflush(stdout);*/ } int main(int argc, char **argv) { int ret; AVPacket packet; AVFrame *frame = av_frame_alloc(); AVFrame *filt_frame = av_frame_alloc(); if (!frame || !filt_frame) { perror(\"Could not allocate frame\"); exit(1); } const char *filename = \"source/Kobe.flv\"; if ((ret = open_input_file(filename)) = 0) { ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret = 0) { /* push the audio data from decoded frame into the filtergraph */ if (av_buffersrc_add_frame_flags(buffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF) 测试文件下载地址 参考 https://blog.csdn.net/leixiaohua1020/article/details/40544521 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/16_transcode.html":{"url":"FFmpeg/16_transcode.html","title":"Transcode(转码)","keywords":"","body":"Transcode(转码)流程与FFmpeg 简单实现转码 差异性如下：1.增加Filter（滤镜）处理2.优化SwrContext（重采样）处理3.增加AVAudioFifo缓存处理4.视频和音频同步代码实现测试文件下载地址Transcode(转码) 本文基于官方例子：doc/examples/transcoding.c 。 流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 与FFmpeg 简单实现转码 差异性如下： 1.增加Filter（滤镜）处理 具体请看Filter和SDL（Video） 和 Filter和SDL（Audio） 2.优化SwrContext（重采样）处理 关键理解是这一段： //1.swr_get_out_samples if (swr_get_out_samples(swr_ctx, 0) >= enc_ctx->frame_size) {//当上次缓存足够时，取缓存 //2.swr_convert if ((ret = swr_convert(swr_ctx, reasampling_frame->data, enc_ctx->frame_size, NULL, 0)) data, enc_ctx->frame_size, (const uint8_t **) filt_frame->data, filt_frame->nb_samples)) swr_get_out_samples 含义：获取当前采样缓存的大小。 swr_convert 含义：从采样缓存中采样enc_ctx->frame_size个长度数据到reasampling_frame->data中。 swr_convert 含义：从有个filt_frame->nb_samples数据的filt_frame->data中采样enc_ctx->frame_size个长度数据到reasampling_frame->data中。 3.增加AVAudioFifo缓存处理 主要有三个方法av_audio_fifo_size 、 av_audio_fifo_write 和av_audio_fifo_read ，详细见代码。 4.视频和音频同步 创建流时(avformat_new_stream)，初始化时间基out_stream->time_base = enc_ctx->time_base 。 送去解码前，需要重新计算pts、dts和duration，代码如下：av_packet_rescale_ts(de_pkt, ifmt_ctx->streams[stream_index]->time_base, ifmt_ctx->streams[stream_index]->codec->time_base); 送去编码前，计算pts、dts和duration，代码如下：av_packet_rescale_ts(enc_pkt, ofmt_ctx->streams[stream_index]->codec->time_base, ofmt_ctx->streams[stream_index]->time_base); 注意：当音频编码器的frame_size和需要被编码的AVFrame中的nb_samples 大小不相等时，需要使用重采样或使用AVAudioFifo缓存，使得frame_size == nb_samples后再送去编码，这种情况会导致pts、dts和duration的值与其他帧相同问题。if (stream_index == AVMEDIA_TYPE_AUDIO && filt_frame->nb_samples != enc_ctx->frame_size){ ...... } 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/12 * description: 来着官方例子：doc/examples/transcoding.c * */ extern \"C\" { #include #include #include #include #include #include #include #include \"libavutil/audio_fifo.h\" } //将每一组滤镜组合放到一起，filter_stm[i]和ifmt_ctx->streams[i]下标保持一致，表示对音频或者视频的滤镜 typedef struct FilterStream { AVFilterContext *buffersink_ctx; AVFilterContext *buffersrc_ctx; AVFilterGraph *filter_graph; } FilterStream; static FilterStream *filter_stm; static AVFormatContext *ifmt_ctx; static AVFormatContext *ofmt_ctx; static int audio_index, video_index; static SwrContext *swr_ctx;//音频：当有需要时，需要重采样 static int64_t resample_pos = 0;//重采样时，会有缓存，这时候要另外计算dts和pts static AVAudioFifo *fifo = NULL;;//重采样时，如果输入nb_sample比输出的nb_sample小时，需要缓存 int open_input_file(const char *in_filename) { int ret = -1, i; /**（解封装 1.1）：创建并初始化AVFormatContext*/ if (avformat_open_input(&ifmt_ctx, in_filename, NULL, NULL) nb_streams; i++) { AVStream *stream = ifmt_ctx->streams[i]; AVCodec *dec; AVCodecContext *dec_ctx; //dec_ctx = stream->codec; //-->通过这种形式获取的解码器上下文已过时 /**（解码 2.1）：查找解码器(AVCodec)*/ if (!(dec = avcodec_find_decoder(stream->codecpar->codec_id))) { fprintf(stderr, \"Failed to find decoder for stream #%u\\n\", i); ret = AVERROR_DECODER_NOT_FOUND; goto end; } /**（解码 2.2）：通过解码器(AVCodec)生成解码器上下文(AVCodecContext)*/ if (!(dec_ctx = avcodec_alloc_context3(dec))) { fprintf(stderr, \"Failed to allocate the decoder context for stream #%u\\n\", i); ret = AVERROR(ENOMEM); goto end; } /**（解码 2.3）：将AVCodecParameters参数赋值给AVCodecContext*/ if ((ret = avcodec_parameters_to_context(dec_ctx, stream->codecpar)) codec_type == AVMEDIA_TYPE_VIDEO || dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO) { dec_ctx->framerate = av_guess_frame_rate(ifmt_ctx, stream, NULL); video_index = i; } else if (dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { audio_index = i; } /**（解码 2.4）：初始化码器器上下文*/ if ((ret = avcodec_open2(dec_ctx, dec, NULL)) codec = dec_ctx; } //打印输出日志 // fprintf(stderr, \"---------------------------ifmt_ctx----------------\\n\"); // av_dump_format(ifmt_ctx, 0, in_filename, 0); ret = 0; end: return ret; } int open_output_file(const char *out_filename) { int ret = -1, i; AVStream *out_stream, *in_stream; AVCodecContext *dec_ctx, *enc_ctx; AVCodec *encoder; AVCodecID encodec_id; /**（封装 4.1）：根据文件格式初始化封装器上下文AVFormatContext*/ avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, out_filename); if (!ofmt_ctx) { fprintf(stderr, \"Could not create output context\\n\"); ret = AVERROR_UNKNOWN; goto end; } for (i = 0; i nb_streams; i++) { /**（封装 4.2）：创建输出视频和音频AVStream*/ if (!(out_stream = avformat_new_stream(ofmt_ctx, NULL))) { fprintf(stderr, \"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } out_stream->codecpar->codec_tag = 0; in_stream = ifmt_ctx->streams[i]; dec_ctx = in_stream->codec; if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO || dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO) { encodec_id = ofmt_ctx->oformat->video_codec; } else if (dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { encodec_id = ofmt_ctx->oformat->audio_codec; } /* in this example, we choose transcoding to same codec */ /**（编码 3.1）：获取对应的编码器AVCodec*/ if (!(encoder = avcodec_find_encoder(encodec_id))) { fprintf(stderr, \"Necessary encoder not found\\n\"); ret = AVERROR_INVALIDDATA; goto end; } /**（编码 3.2）：通过编码器(AVCodec)获取编码器上下文(AVCodecContext)*/ if (!(enc_ctx = avcodec_alloc_context3(encoder))) { fprintf(stderr, \"Failed to allocate the encoder context\\n\"); ret = AVERROR(ENOMEM); goto end; } /**给编码器初始化信息*/ /* In this example, we transcode to same properties (picture size, * sample rate etc.). These properties can be changed for output * streams easily using filters */ if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO) { enc_ctx->height = dec_ctx->height; enc_ctx->width = dec_ctx->width; enc_ctx->sample_aspect_ratio = dec_ctx->sample_aspect_ratio; //take first format from list of supported formats if (encoder->pix_fmts) enc_ctx->pix_fmt = encoder->pix_fmts[0]; else enc_ctx->pix_fmt = dec_ctx->pix_fmt; //video time_base can be set to whatever is handy and supported by encoder enc_ctx->time_base = dec_ctx->time_base; enc_ctx->has_b_frames = dec_ctx->has_b_frames; //输出将相对于输入延迟max_b_frames + 1-->但是输入的为0！ //enc_ctx->max_b_frames = dec_ctx->max_b_frames + 1; enc_ctx->max_b_frames = 2; enc_ctx->bit_rate = dec_ctx->bit_rate; enc_ctx->codec_type = dec_ctx->codec_type; //不支持B帧 if (enc_ctx->max_b_frames && enc_ctx->codec_id != AV_CODEC_ID_MPEG4 && enc_ctx->codec_id != AV_CODEC_ID_MPEG1VIDEO && enc_ctx->codec_id != AV_CODEC_ID_MPEG2VIDEO) { enc_ctx->has_b_frames = 0; enc_ctx->max_b_frames = 0; } } else { enc_ctx->sample_rate = dec_ctx->sample_rate; enc_ctx->channel_layout = dec_ctx->channel_layout; enc_ctx->channels = av_get_channel_layout_nb_channels(dec_ctx->channel_layout); enc_ctx->sample_fmt = encoder->sample_fmts[0]; enc_ctx->time_base = {1, enc_ctx->sample_rate}; enc_ctx->bit_rate = dec_ctx->bit_rate; enc_ctx->codec_type = dec_ctx->codec_type; } if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) enc_ctx->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; /**（编码 3.3）：*/ /* Third parameter can be used to pass settings to encoder */ if ((ret = avcodec_open2(enc_ctx, encoder, NULL)) codecpar, enc_ctx)) time_base = enc_ctx->time_base; //替换掉旧的 out_stream->codec = enc_ctx; } else if (dec_ctx->codec_type == AVMEDIA_TYPE_UNKNOWN) { fprintf(stderr, \"Elementary stream #%d is of unknown type, cannot proceed\\n\", i); ret = AVERROR_INVALIDDATA; goto end; } else { /* if this stream must be remuxed */ if ((ret = avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar)) time_base = in_stream->time_base; } } //打印输出日志 // fprintf(stderr, \"---------------------------ofmt_ctx----------------\\n\"); // av_dump_format(ofmt_ctx, 0, out_filename, 1); /**（封装 4.4）：初始化AVIOContext*/ if (!(ofmt_ctx->oformat->flags & AVFMT_NOFILE)) { ; if ((ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE)) streams[audio_index]->codec; AVCodecContext *enc_ctx_audio = ofmt_ctx->streams[audio_index]->codec; int ret = -1; /**（重采样 5.1）：申请内存*/ if (!(swr_ctx = swr_alloc())) { fprintf(stderr, \"Could not allocate resampler context\\n\"); return ret; } /**（重采样 5.2）：设置参数*/ av_opt_set_int(swr_ctx, \"in_channel_count\", dec_ctx_audio->channels, 0); av_opt_set_int(swr_ctx, \"in_sample_rate\", dec_ctx_audio->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"in_sample_fmt\", dec_ctx_audio->sample_fmt, 0); av_opt_set_int(swr_ctx, \"out_channel_count\", enc_ctx_audio->channels, 0); av_opt_set_int(swr_ctx, \"out_sample_rate\", enc_ctx_audio->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"out_sample_fmt\", enc_ctx_audio->sample_fmt, 0); /**（重采样 5.3）：初始化SwrContext*/ if ((ret = swr_init(swr_ctx)) codec_type == AVMEDIA_TYPE_VIDEO) { /**（滤镜 6.1）：获取输入和输出滤镜器【同音频】*/ buffersrc = avfilter_get_by_name(\"buffer\"); buffersink = avfilter_get_by_name(\"buffersink\"); if (!buffersrc || !buffersink) { fprintf(stderr, \"filtering source or sink element not found\\n\"); ret = AVERROR_UNKNOWN; goto end; } snprintf(args, sizeof(args), \"video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\", dec_ctx->width, dec_ctx->height, dec_ctx->pix_fmt, dec_ctx->time_base.num, dec_ctx->time_base.den, dec_ctx->sample_aspect_ratio.num, dec_ctx->sample_aspect_ratio.den); /**（滤镜 6.2）：创建和初始化输入和输出过滤器实例并将其添加到现有图形中*/ if ((ret = avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, \"in\", args, NULL, filter_graph)) pix_fmt, sizeof(enc_ctx->pix_fmt), AV_OPT_SEARCH_CHILDREN); if (ret codec_type == AVMEDIA_TYPE_AUDIO) { buffersrc = avfilter_get_by_name(\"abuffer\");//名称比视频的前面多一个a buffersink = avfilter_get_by_name(\"abuffersink\"); if (!buffersrc || !buffersink) { fprintf(stderr, \"filtering source or sink element not found\\n\"); ret = AVERROR_UNKNOWN; goto end; } if (!dec_ctx->channel_layout) dec_ctx->channel_layout = av_get_default_channel_layout(dec_ctx->channels); snprintf(args, sizeof(args), \"time_base=%d/%d:sample_rate=%d:sample_fmt=%s:channel_layout=0x%lld\", dec_ctx->time_base.num, dec_ctx->time_base.den, dec_ctx->sample_rate, av_get_sample_fmt_name(dec_ctx->sample_fmt), dec_ctx->channel_layout); if ((ret = avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, \"in\", args, NULL, filter_graph)) sample_fmt, sizeof(enc_ctx->sample_fmt), AV_OPT_SEARCH_CHILDREN)) channel_layout, sizeof(enc_ctx->channel_layout), AV_OPT_SEARCH_CHILDREN)) sample_rate, sizeof(enc_ctx->sample_rate), AV_OPT_SEARCH_CHILDREN)) buffersrc_ctx /* Endpoints for the filter graph. */ outputs->name = av_strdup(\"in\"); outputs->filter_ctx = buffersrc_ctx; outputs->pad_idx = 0; outputs->next = NULL; //绑定关系 out ——> buffersink_ctx inputs->name = av_strdup(\"out\"); inputs->filter_ctx = buffersink_ctx; inputs->pad_idx = 0; inputs->next = NULL; if (!outputs->name || !inputs->name) { ret = AVERROR(ENOMEM); fprintf(stderr, \"Filter av_strdup name failed\\n\"); goto end; } /**（滤镜 6.4）：将字符串描述的图形添加到图形中*/ if ((ret = avfilter_graph_parse_ptr(filter_graph, filter_spec, &inputs, &outputs, NULL)) buffersrc_ctx = buffersrc_ctx; fctx->buffersink_ctx = buffersink_ctx; fctx->filter_graph = filter_graph; end: avfilter_inout_free(&inputs); avfilter_inout_free(&outputs); return ret; } int init_filters() { const char *filter_spec; unsigned int i; int ret; filter_stm = (FilterStream *) (av_malloc_array(ifmt_ctx->nb_streams, sizeof(*filter_stm))); if (!filter_stm) return AVERROR(ENOMEM); //这里会根据音频和视频的stream_index创建对应的filter_stm组 for (i = 0; i nb_streams; i++) { filter_stm[i].buffersrc_ctx = NULL; filter_stm[i].buffersink_ctx = NULL; filter_stm[i].filter_graph = NULL; if (!(ifmt_ctx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_AUDIO || ifmt_ctx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO)) continue; if (ifmt_ctx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) filter_spec = \"null\"; /* passthrough (dummy) filter for video */ // filter_spec = \"scale=iw/2:ih/2[in_tmp];[in_tmp]split=4[in_1][in_2][in_3][in_4];[in_1]pad=iw*2:ih*2[a];[a][in_2]overlay=w[b];[b][in_3]overlay=0:h[d];[d][in_4]overlay=w:h[out]\"; /* passthrough (dummy) filter for video */ else filter_spec = \"anull\"; /* passthrough (dummy) filter for audio */ // filter_spec = \"aecho=0.8:0.88:60:0.4\"; /* passthrough (dummy) filter for audio */ ret = init_filter(&filter_stm[i], ifmt_ctx->streams[i]->codec, ofmt_ctx->streams[i]->codec, filter_spec); if (ret) return ret; } return 0; } int encode_write_frame(AVFrame *filt_frame, unsigned int stream_index) { int ret = -1; AVFrame *reasampling_frame; AVPacket *enc_pkt = av_packet_alloc(); AVCodecContext *enc_ctx = ofmt_ctx->streams[stream_index]->codec; /* encode filtered frame */ enc_pkt->data = NULL; enc_pkt->size = 0; av_init_packet(enc_pkt); if (!filt_frame) {//刷新缓存 fprintf(stderr, \"filt_frame is null , flush encode ?\\n\"); avcodec_send_frame(enc_ctx, NULL); ret = 0; goto end; } //音频的这种情况需要重采样再进行输出 if (stream_index == AVMEDIA_TYPE_AUDIO && filt_frame->nb_samples != enc_ctx->frame_size) { //输入的比输出的还小；参考官方例子：doc/examples/transcode_aac.c if (filt_frame->nb_samples frame_size) { /* Make the FIFO as large as it needs to be to hold both, * the old and the new samples. */ /*if ((ret = av_audio_fifo_realloc(fifo, av_audio_fifo_size(fifo) + filt_frame->nb_samples)) data, filt_frame->nb_samples); if (buf_size nb_samples) { fprintf(stderr, \"Could not write data to FIFO\\n\"); ret = AVERROR_EXIT; goto end; } //这一次还不够转换 int new_size = av_audio_fifo_size(fifo); if (new_size frame_size) { ret = 0; goto end; } } if (!(reasampling_frame = av_frame_alloc())) { fprintf(stderr, \"Could not allocate oframe_a\\n\"); ret = -1; goto end; } //把filt_frame参数复制给reasampling_frame if ((ret = av_frame_copy_props(reasampling_frame, filt_frame)) format = enc_ctx->sample_fmt; reasampling_frame->channel_layout = enc_ctx->channel_layout; reasampling_frame->channels = enc_ctx->channels; reasampling_frame->sample_rate = enc_ctx->sample_rate; reasampling_frame->nb_samples = enc_ctx->frame_size; if ((ret = av_frame_get_buffer(reasampling_frame, 0)) nb_samples frame_size) { if (av_audio_fifo_read(fifo, (void **) reasampling_frame->data, reasampling_frame->nb_samples) nb_samples) { fprintf(stderr, \"Could not read data from FIFO\\n\"); // av_frame_free(&output_frame); ret = AVERROR_EXIT; goto end; } fprintf(stdout, \"deal fifo\\n\"); } else { /**（重采样 5.4）：重新采样*/ if (swr_get_out_samples(swr_ctx, 0) >= enc_ctx->frame_size) {//当上次缓存足够时，取缓存 if ((ret = swr_convert(swr_ctx, reasampling_frame->data, enc_ctx->frame_size, NULL, 0)) data, enc_ctx->frame_size, (const uint8_t **) filt_frame->data, filt_frame->nb_samples)) nb_samples != enc_ctx->frame_size){ resample_pos += enc_ctx->frame_size; //编码前重新给pts和dts赋值 reasampling_frame->pts = resample_pos; reasampling_frame->pkt_dts = resample_pos; } /**（编码 3.5）：把滤镜处理后的AVFrame送去编码*/ ret = avcodec_send_frame(enc_ctx, reasampling_frame); } else { /**（编码 3.5）：把滤镜处理后的AVFrame送去编码*/ ret = avcodec_send_frame(enc_ctx, filt_frame); } if (ret = 0) { /**（编码 3.6）：从编码器中得到编码后数据，放入AVPacket中*/ ret = avcodec_receive_packet(enc_ctx, enc_pkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)//本次的ret只针对本循环，对外部为正常 break; else if (ret >>>>\" : \"v-----\", enc_pkt->size, enc_pkt->dts, enc_pkt->pts, enc_pkt->duration, ofmt_ctx->streams[stream_index]->cur_dts); //设置pts等信息 /* prepare packet for muxing */ enc_pkt->stream_index = stream_index; av_packet_rescale_ts(enc_pkt, ofmt_ctx->streams[stream_index]->codec->time_base, ofmt_ctx->streams[stream_index]->time_base); enc_pkt->pos = -1; printf(\"write2 %s Packet. size:%5d\\tdts:%5lld\\tpts:%5lld\\tduration:%5lld\\tcur_dts:%5lld\\n\", stream_index == AVMEDIA_TYPE_AUDIO ? \"a>>>>>\" : \"v-----\", enc_pkt->size, enc_pkt->dts, enc_pkt->pts, enc_pkt->duration, ofmt_ctx->streams[stream_index]->cur_dts); /**（封装 4.6）：将编码后的数据AVPacket进行封装，写入到文件*/ if ((ret = av_interleaved_write_frame(ofmt_ctx, enc_pkt)) nb_samples != enc_ctx->frame_size) { if (swr_get_out_samples(swr_ctx, 0) >= enc_ctx->frame_size) { encode_write_frame(filt_frame, stream_index); } } ret = 0; end: av_packet_free(&enc_pkt); return ret; } int filter_encode_write_frame(AVFrame *de_frame, unsigned int stream_index) { int ret; AVFrame *filt_frame = NULL; /**（滤镜 6.6）：将解码后的AVFrame送去filtergraph进行滤镜处理*/ if ((ret = av_buffersrc_add_frame_flags(filter_stm[stream_index].buffersrc_ctx, de_frame, AV_BUFFERSRC_FLAG_KEEP_REF)) pict_type = AV_PICTURE_TYPE_NONE; //然后把滤镜处理后的数据重新进行编码成你想要的格式，再封装输出 if ((ret = encode_write_frame(filt_frame, stream_index)) streams[audio_index]->codec->sample_fmt, ofmt_ctx->streams[audio_index]->codec->channels, ofmt_ctx->streams[audio_index]->codec->frame_size * 2))) { fprintf(stderr, \"Could not allocate FIFO\\n\"); ret = AVERROR(ENOMEM); goto end; } while (1) { /**（解封装 1.3）：读取解封装后数据到AVPacket中*/ if ((ret = av_read_frame(ifmt_ctx, de_pkt)) stream_index; if (!(de_frame = av_frame_alloc())) { ret = AVERROR(ENOMEM); fprintf(stderr, \"Error alloc frame\\n\"); goto end; } //注意：av_packet_rescale_ts 在解码前重新计算pts、dts和duration的值，需要与编码时对应起来，防止视频和音频出现不同步 // 注意传入的参数！AVStream和AVCodecContext的时间基。而我的AVCodecContext是通过生成的，并非原来的初始化时的。 av_packet_rescale_ts(de_pkt, ifmt_ctx->streams[stream_index]->time_base, ifmt_ctx->streams[stream_index]->codec->time_base); /**（解码 2.5）：把AVPacket送去解码*/ /* read all the output frames (in general there may be any number of them */ if ((ret = avcodec_send_packet(ifmt_ctx->streams[stream_index]->codec, de_pkt)) = 0) { /**（解码 2.6）：从解码器获取解码后的数据到AVFrame*/ ret = avcodec_receive_frame(ifmt_ctx->streams[stream_index]->codec, de_frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret pts = de_frame->best_effort_timestamp; //这是解码后的裸数据，如果可以对其进行滤镜处理 ret = filter_encode_write_frame(de_frame, stream_index); // ret = encode_write_frame(de_frame, stream_index); if (ret nb_streams; i++) { avcodec_send_packet(ifmt_ctx->streams[i]->codec, NULL); avcodec_send_frame(ofmt_ctx->streams[i]->codec, NULL); } fprintf(stdout, \"write the trailer\\n\"); /**（封装 4.7）：写入文件尾*/ av_write_trailer(ofmt_ctx); end: av_packet_free(&de_pkt); av_frame_free(&de_frame); for (i = 0; i nb_streams; i++) { avcodec_free_context(&ifmt_ctx->streams[i]->codec); if (ofmt_ctx && ofmt_ctx->nb_streams > i && ofmt_ctx->streams[i] && ofmt_ctx->streams[i]->codec) avcodec_free_context(&ofmt_ctx->streams[i]->codec); if (filter_stm && filter_stm[i].filter_graph) avfilter_graph_free(&filter_stm[i].filter_graph); } av_free(filter_stm); avformat_close_input(&ifmt_ctx); if (ofmt_ctx && !(ofmt_ctx->oformat->flags & AVFMT_NOFILE)) avio_closep(&ofmt_ctx->pb); avformat_free_context(ofmt_ctx); if (fifo) av_audio_fifo_free(fifo); if (ret 测试文件下载地址 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/17_sync.html":{"url":"FFmpeg/17_sync.html","title":"音视频同步处理","keywords":"","body":"音视频同步处理播放流程时钟同步处理时钟类型处理逻辑裸数据\"同步\"播放影响视频播放进度的因素影响音频播放进度的因素同时播放视频和音频编解码同步处理转码流程回顾1. 处理之前相关函数2. 解码前处理（红色上AVPacket）3. 裸数据处理（绿色AVFrame）4. 编码后处理（红色右AVPacket）后话音视频同步处理 播放流程 从图中我们得到一个重要的名称，叫做\"时钟\"。视频和音频是两个不相干的线程，各自进行播放，而各自播放的进度都需要参考这个\"时钟\"，才能使得播放同步。其实这里说的\"进度\"就是我们熟悉pts，所以下面主要扒编解码中pts的那些点。讲之前，先讲讲三种类型的时钟处理。 时钟同步处理 时钟类型 音频时钟：即以音频进度作为参考时钟，音频播放进度为准。由于耳朵感官上比眼睛更敏感，所以大多少采用这种形式。缺点是画面可能出现卡段或跳帧的情况。 视频时钟：即以视频进度作为参考时钟，视频播放进度为准。 外部时钟：即以外部进度（时间戳）作为参考时钟。 处理逻辑 当音频/视频帧播放的进度大于参考时钟的进度时，需要让该帧等待播放；当音频/视频帧播放的进度小于参考时钟的进度时，需要快速播放完或者丢弃该帧处理。 总结来说就三个词：\"等待\" 和 \"快速\"或\"丢弃\"。 裸数据\"同步\"播放 我们首先再去简单屡屡音视频的一些基本概念，因为分析音视频同步不同步，理论知识真的很重要！播放的都是解码后的裸数据（yuv、pcm），如果视频跟音频两个裸数据源本身播放就不同步，再怎么处理也是徒劳。 影响视频播放进度的因素 总数据大小 = 一帧大小 x 每一秒播放的帧数（fps）x 时间 当我们做视频编解码、转换格式或滤镜处理等等时，如果出现过重新封装之后播放发现时间对不上了，那很可能你对这几个基本概念还不熟悉，看看这两篇补补？直播推流全过程：视频数据源之YUV（1） 、视音频数据处理入门：RGB、YUV像素数据处理 一帧大小：根据视频数据格式计算，如rgb=width x height x 3; yuv420=width x height x 3 / 2 ; ..... 帧率: 每一秒播放帧的数量，单位：fps（frame per second）。 比特率(码率)： 就是： 一帧大小 x 每一秒播放的帧数（fps）。 影响音频播放进度的因素 数据大小（单位：字节）= 采样率 x (位宽 / 8) x 声道数 x 时间 采样率：每秒钟采样次数，如：44100。 位宽：常见有8位、16位、32位；即对应1个字节、2个字节、4个字节。 声道数：常用单声道、双声道；即对应1个声道数，2个声道数。 还不是很熟悉的朋友请往这补补？直播推流全过程：音频数据源之PCM（2） 、视音频数据处理入门：PCM音频采样数据处理 同时播放视频和音频 这里所说是\"同时\"播放，而非同步，是因为我们并没有做同步处理，只是根据理论知识让它们各自播放，效果上展示出来同步的效果。 来实践一下？ 把 Kobe.flv 这个视频解码后做些滤镜等处理后得到yuv、pcm的裸数据后进行播放。解码过程参看：FFmpeg 简单实现转码 和 FFmpeg Transcode(转码) 。 解码后的数据如下： qincji:VAFFmpeg mac$ ls -lsh source/ 109864 -rw-r--r-- 1 mac staff 53M 1 16 11:29 Kobe-384x216-15fps.yuv 20672 -rw-r--r--@ 1 mac staff 10M 1 10 11:06 Kobe-44100-2-32-33ks.pcm 进行播放 我这使用Audition播放音频，设置如下： 若使用ffplay，则使用命令： ffplay -ar 44100 -channels 2 -f f32le -i source/Kobe-44100-2-32-33ks.pcm 使用ffplay播放视频 ffplay -f rawvideo -framerate 15 -video_size 384x216 source/Kobe-384x216-15fps.yuv 播放显示的同步效果： 编解码同步处理 首先我们先了解一下FFmpeg中有关显示处理的几个重要知识点： pts：用来显示的时间戳，单位：AVStream->time_base。 dts：用来解码的时间戳，单位：AVStream->time_base。 duration：数据时长，单位：AVStream->time_base。 编解码同步处理是指：音视频转码的过程（如格式转换、视频合并等）。需要把视频解封装、解码后重新编码、封装成新的文件，这个过程需要精准的处理pts、dts和duration才能保证播放的过程同步。 转码流程回顾 上图中三个具有颜色的点，红色是转码过程中必须要处理的，而绿色则根据实际情况而定。 以下处理的代码位置在FFmpeg Transcode(转码) 中。 1. 处理之前相关函数 1) AVRational time_base ：时间基；我们只需设置编码的时间基，其他框架自动生成。设置方式： 视频：(应保持跟解码保持一致) time_base = {1, fps}; //如time_base = {1, 30} 音频： time_base = {1, sample}; //如time_base = {1, 44100} 2）AVCodecContext->frame_size：音频帧中每个通道的样本数，跟nb_samples的值一样。所以： 一帧大小(单位：字节) = frame_size x 位宽 x 声道数 这里需要特别注意的是，编码时frame_size的大小跟采样率（sample_rate）并没有关系，它是跟音频编码器类型相关的，如： avcodec_open2 -> avctx->codec->init(avctx) 这将会调用AVCodec(编码器)中的对应的init，如AV_CODEC_ID_AAC编码器： AVCodec ff_aac_encoder = { .name = \"aac\", .long_name = NULL_IF_CONFIG_SMALL(\"AAC (Advanced Audio Coding)\"), .type = AVMEDIA_TYPE_AUDIO, .id = AV_CODEC_ID_AAC, .priv_data_size = sizeof(AACEncContext), .init = aac_encode_init, .encode2 = aac_encode_frame, .close = aac_encode_end, .defaults = aac_encode_defaults, .supported_samplerates = mpeg4audio_sample_rates, .caps_internal = FF_CODEC_CAP_INIT_THREADSAFE, .capabilities = AV_CODEC_CAP_SMALL_LAST_FRAME | AV_CODEC_CAP_DELAY, .sample_fmts = (const enum AVSampleFormat[]){ AV_SAMPLE_FMT_FLTP, AV_SAMPLE_FMT_NONE }, .priv_class = &aacenc_class, }; 然后会调用：aac_encode_init -> avctx->frame_size = 1024 ，这也就是当指定编码器后，怎么也改变不了frame_size的值的原因。 3) av_packet_rescale_ts : 简单理解为，将pkt的pts、dts和duration值从原来的，以tb_dst为标准转换为现在的有效。 /** * Convert valid timing fields (timestamps / durations) in a packet from one * timebase to another. Timestamps with unknown values (AV_NOPTS_VALUE) will be * ignored. * * @param pkt packet on which the conversion will be performed * @param tb_src source timebase, in which the timing fields in pkt are * expressed * @param tb_dst destination timebase, to which the timing fields will be * converted */ void av_packet_rescale_ts(AVPacket *pkt, AVRational tb_src, AVRational tb_dst); 我们从源码上来看： void av_packet_rescale_ts(AVPacket *pkt, AVRational src_tb, AVRational dst_tb) { if (pkt->pts != AV_NOPTS_VALUE) pkt->pts = av_rescale_q(pkt->pts, src_tb, dst_tb); if (pkt->dts != AV_NOPTS_VALUE) pkt->dts = av_rescale_q(pkt->dts, src_tb, dst_tb); if (pkt->duration > 0) pkt->duration = av_rescale_q(pkt->duration, src_tb, dst_tb); //忽略以下... } 2. 解码前处理（红色上AVPacket） 解码之前需要AVPacket的pts、dts、duration的值重新转换，如下： av_packet_rescale_ts(de_pkt, ifmt_ctx->streams[stream_index]->time_base, ifmt_ctx->streams[stream_index]->codec->time_base); 编码之后这几个数据值会传递给AVFrame，进行下去。 3. 裸数据处理（绿色AVFrame） 解码之后得到的AVFrame中装的就是裸数据，当我们需要对该数据进行加工后，结果可能是出现多个AVFrame数据，当出现这种情况时，需要重新计算当前AVFrame.pkt_dts和AVFrame.pts的值后，才能送去编码(avcodec_send_frame)，否则会造成音视频的不同步。 具体要怎么设置呢？哈哈，我也在找答案中…… 不过我在某个音频编码的源码中找到一段是这么转换的pts的： int ff_af_queue_add(AudioFrameQueue *afq, const AVFrame *f) { AudioFrame *new = av_fast_realloc(afq->frames, &afq->frame_alloc, sizeof(*afq->frames)*(afq->frame_count+1)); if(!new) return AVERROR(ENOMEM); afq->frames = new; new += afq->frame_count; /* get frame parameters */ new->duration = f->nb_samples; new->duration += afq->remaining_delay; if (f->pts != AV_NOPTS_VALUE) { //转换新的pts new->pts = av_rescale_q(f->pts, afq->avctx->time_base, (AVRational){ 1, afq->avctx->sample_rate }); //然后再减去 avctx->initial_padding 的值 new->pts -= afq->remaining_delay; if(afq->frame_count && new[-1].pts >= new->pts) av_log(afq->avctx, AV_LOG_WARNING, \"Queue input is backward in time\\n\"); } else { new->pts = AV_NOPTS_VALUE; } afq->remaining_delay = 0; /* add frame sample count */ afq->remaining_samples += f->nb_samples; afq->frame_count++; return 0; } // remaining_delay的值初始化 av_cold void ff_af_queue_init(AVCodecContext *avctx, AudioFrameQueue *afq) { afq->avctx = avctx; afq->remaining_delay = avctx->initial_padding; afq->remaining_samples = avctx->initial_padding; afq->frame_count = 0; } 上面计算新的pts的方式，我测试验证了一下，确实如此。 总结一下上面的情况（音频）： 也就是说当音频处理后出现多帧（AVFrame）的情况下，在编码(avcodec_send_frame)前，需要根据当前设置编码的time_base和sample_rate重新设置pts才能保证编码后的音频才不会\"变形\"。这里给我的解决方案（代码源同上）： //当解码的帧数与送去编码的帧数有差别时，我们必须更改pts和dts的值，否者封装时av_packet_rescale_ts（计算）pts和dts有重复，会出问题 if (filt_frame->nb_samples != enc_ctx->frame_size) { resample_pos += enc_ctx->frame_size; //编码前重新给pts和dts赋值 reasampling_frame->pts = resample_pos; reasampling_frame->pkt_dts = resample_pos; } 注：上面处理方案没有针对视频流测试过，但思路是一样的，看编码前的裸数据在你给定的fps播放下，总的帧数播放完跟原来时长是否一致，再然后定义新的pts。 4. 编码后处理（红色右AVPacket） enc_pkt->stream_index = stream_index; av_packet_rescale_ts(enc_pkt, ofmt_ctx->streams[stream_index]->codec->time_base, ofmt_ctx->streams[stream_index]->time_base); enc_pkt->pos = -1; 后话 造成音视频不同步的原因有很多种，学会分析其中的原因才是我们想要的，其中最好的方式就是看别人怎么做的。如果你在做播放的过程想参考一下音视频同步处理不妨看一下fftools/ffplay.c 。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/18_command.html":{"url":"FFmpeg/18_command.html","title":"FFmpeg命令使用指南","keywords":"","body":"FFmpeg命令使用指南FFmpeg主要工具ffmpeg1）帮助文档2）指令格式3）打印某个协议的具体信息（如h264）4）滤镜命令5）协议命令6）把多个图片合并视频命令7）合并视频命令ffprobe1）帮助文档2）指令格式ffplay1）帮助文档2）指令格式3）播放pcm数据4）播放yuv数据5）播放网络资源6）播放的其他重要参数FFmpeg命令使用指南 本文目的主要为了探讨如何使用命令，以及该如何去查找我们需要的命令，最后抛砖引玉示范几个案例。 FFmpeg主要工具 FFmpeg主要工具有ffmpeg、ffprobe和ffplay，它们分别用作编解码、内容分析和播放器。它们均可通过编译或者直接下载安装获得的应用程序。 ffmpeg 官方文档 ，如果英文不太好，推荐从《FFmpeg从入门到精通》书中查找中文含义。 源码所在：fftools/ffmpeg.c 以下分析ffmpeg工具帮助文档，以便掌握使用命令的技巧 1）帮助文档 通常拿到工具的第一步看帮助文档： ffmpeg --help 当内容太多时，我们需要把打印的信息保存到文件中，再拿工具看： ffmpeg --help > ffmpeg_help.txt 内容如下： Hyper fast Audio and Video encoder usage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}... Getting help: -h -- print basic options -h long -- print more options -h full -- print all options (including all format and codec specific options, very long) -h type=name -- print all options for the named decoder/encoder/demuxer/muxer/filter/bsf See man ffmpeg for detailed description of the options. Print help / information / capabilities: -L show license -h topic show help -? topic show help -help topic show help --help topic show help -version show version -buildconf show build configuration -formats show available formats -muxers show available muxers -demuxers show available demuxers -devices show available devices -codecs show available codecs -decoders show available decoders -encoders show available encoders -bsfs show available bit stream filters -protocols show available protocols -filters show available filters -pix_fmts show available pixel formats -layouts show standard channel layouts -sample_fmts show available audio sample formats -colors show available color names -sources device list sources of the input device -sinks device list sinks of the output device -hwaccels show available HW acceleration methods Global options (affect whole program instead of just one file: -loglevel loglevel set logging level -v loglevel set logging level -report generate a report -max_alloc bytes set maximum size of a single allocated block -y overwrite output files -n never overwrite output files -ignore_unknown Ignore unknown stream types -filter_threads number of non-complex filter threads -filter_complex_threads number of threads for -filter_complex -stats print progress report during encoding -max_error_rate maximum error rate ratio of errors (0.0: no errors, 1.0: 100% errors) above which ffmpeg returns an error instead of success. -bits_per_raw_sample number set the number of bits per raw sample -vol volume change audio volume (256=normal) Per-file main options: -f fmt force format -c codec codec name -codec codec codec name -pre preset preset name -map_metadata outfile[,metadata]:infile[,metadata] set metadata information of outfile from infile -t duration record or transcode \"duration\" seconds of audio/video -to time_stop record or transcode stop time -fs limit_size set the limit file size in bytes -ss time_off set the start time offset -sseof time_off set the start time offset relative to EOF -seek_timestamp enable/disable seeking by timestamp with -ss -timestamp time set the recording timestamp ('now' to set the current time) -metadata string=string add metadata -program title=string:st=number... add program with specified streams -target type specify target file type (\"vcd\", \"svcd\", \"dvd\", \"dv\" or \"dv50\" with optional prefixes \"pal-\", \"ntsc-\" or \"film-\") -apad audio pad -frames number set the number of frames to output -filter filter_graph set stream filtergraph -filter_script filename read stream filtergraph description from a file -reinit_filter reinit filtergraph on input parameter changes -discard discard -disposition disposition Video options: -vframes number set the number of video frames to output -r rate set frame rate (Hz value, fraction or abbreviation) -s size set frame size (WxH or abbreviation) -aspect aspect set aspect ratio (4:3, 16:9 or 1.3333, 1.7777) -bits_per_raw_sample number set the number of bits per raw sample -vn disable video -vcodec codec force video codec ('copy' to copy stream) -timecode hh:mm:ss[:;.]ff set initial TimeCode value. -pass n select the pass number (1 to 3) -vf filter_graph set video filters -ab bitrate audio bitrate (please use -b:a) -b bitrate video bitrate (please use -b:v) -dn disable data Audio options: -aframes number set the number of audio frames to output -aq quality set audio quality (codec-specific) -ar rate set audio sampling rate (in Hz) -ac channels set number of audio channels -an disable audio -acodec codec force audio codec ('copy' to copy stream) -vol volume change audio volume (256=normal) -af filter_graph set audio filters Subtitle options: -s size set frame size (WxH or abbreviation) -sn disable subtitle -scodec codec force subtitle codec ('copy' to copy stream) -stag fourcc/tag force subtitle tag/fourcc -fix_sub_duration fix subtitles duration -canvas_size size set canvas size (WxH or abbreviation) -spre preset set the subtitle options to the indicated preset 2）指令格式 usage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}... 这就是ffmpeg指令格式，-i前面的options(选择)指定输入文件，[outfile options]指定输出文件。举个例子：（官方文档的）将输入文件的帧速率强制为1fps，将输出文件的帧速率强制为24fps，进行转换： ffmpeg -r 1 -i Kobe_in.flv -r 24 Kobe_out.avi 其中-r是作用转换帧率，描述：-r rate set frame rate (Hz value, fraction or abbreviation)。 输入和输出对比一下： 这是输入： Duration: 00:00:30.00, start: 0.000000, bitrate: 221 kb/s Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 182 kb/s, 15.07 fps, 15 tbr, 1k tbn, 30 tbc 这是输出： Input #0, avi, from 'Kobe_out.avi': Metadata: encoder : Lavf58.29.100 Duration: 00:07:30.04, start: 0.000000, bitrate: 62 kb/s Stream #0:0: Video: mpeg4 (Simple Profile) (FMP4 / 0x34504D46), yuv420p, 384x216 [SAR 1:1 DAR 16:9], 44 kb/s, 24 fps, 24 tbr, 24 tbn, 24 tbc 3）打印某个协议的具体信息（如h264） -h type=name -- print all options for the named decoder/encoder/demuxer/muxer/filter/bsf 就是这一行有点难理解，-h type=name，这是指定对类型decoder/encoder/...中某个类型的\"选项\"信息，举一个例子就知道，如： ffmpeg -h decoder=h264 打印信息如下： Decoder h264 [H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10]: General capabilities: dr1 delay threads Threading capabilities: frame and slice Supported hardware devices: videotoolbox H264 Decoder AVOptions: -enable_er .D.V..... Enable error resilience on damaged frames (unsafe) (default auto) -x264_build .D.V..... Assume this x264 version if no x264 version found in any SEI (from -1 to INT_MAX) (default -1) 而从哪里知道有哪些decoder呢？ffmpeg -help时显示出-decoders show available decoders，也就是可以通过这个命令打印的： ffmpeg -decoders 其他encoder、...、filter也是一样的操作，先通过找有哪些存在的，然后再打印，又如filter: ffmpeg -filters 找到有 acopy，然后查看它的具体用法： ffmpeg -h filter=acopy 4）滤镜命令 官方文档 1）我们从上面的帮助文档(ffmpeg -h)中找到有关滤镜输出的，分别是视频滤镜输出，以及音频滤镜输出： -vf filter_graph set video filters 和 -af filter_graph set audio filters ，而通过上一步ffmpeg -filters找到处理视频的boxblur（动态模糊）以及处理音频的aecho（回声处理）两条，然后分别打印两个的用法如下： ffmpeg -h filter=boxblur 输出如下： Filter boxblur Blur the input. Inputs: #0: default (video) Outputs: #0: default (video) boxblur AVOptions: luma_radius ..FV..... Radius of the luma blurring box (default \"2\") lr ..FV..... Radius of the luma blurring box (default \"2\") luma_power ..FV..... How many times should the boxblur be applied to luma (from 0 to INT_MAX) (default 2) lp ..FV..... How many times should the boxblur be applied to luma (from 0 to INT_MAX) (default 2) chroma_radius ..FV..... Radius of the chroma blurring box cr ..FV..... Radius of the chroma blurring box chroma_power ..FV..... How many times should the boxblur be applied to chroma (from -1 to INT_MAX) (default -1) cp ..FV..... How many times should the boxblur be applied to chroma (from -1 to INT_MAX) (default -1) alpha_radius ..FV..... Radius of the alpha blurring box ar ..FV..... Radius of the alpha blurring box alpha_power ..FV..... How many times should the boxblur be applied to alpha (from -1 to INT_MAX) (default -1) ap ..FV..... How many times should the boxblur be applied to alpha (from -1 to INT_MAX) (default -1) This filter has support for timeline through the 'enable' option. 通过以上说明，我们设置视频滤镜处理为：boxblur=luma_radius=3:luma_power=8。 以及音频： ffmpeg -h filter=aecho 输出如下： Filter aecho Add echoing to the audio. Inputs: #0: default (audio) Outputs: #0: default (audio) aecho AVOptions: in_gain ..F.A.... set signal input gain (from 0 to 1) (default 0.6) out_gain ..F.A.... set signal output gain (from 0 to 1) (default 0.3) delays ..F.A.... set list of signal delays (default \"1000\") decays ..F.A.... set list of signal decays (default \"0.5\") 通过上面说明我们设置音频滤镜处理为：aecho=0.8:0.88:60:0.4 或者 aecho=in_gain=0.8:out_gain=0.88:delays=60:decays=0.4 。 2）合并两条处理命令如下： ffmpeg -i Kobe_in.flv -vf boxblur=luma_radius=3:luma_power=8 -af aecho=0.8:0.88:60:0.4 Kobe_out.flv 3）如果使用多条滤镜指令则使用,进行分割，如添加视频水平翻转(hflip)如下： ffmpeg -i Kobe_in.flv -vf boxblur=luma_radius=1:luma_power=8,hflip Kobe_out.flv 4）其他：滤镜处理是非常非常丰富的，例如：高斯模糊、3D降噪、添加logo、视频截切、视频反转、滤镜以及各种各样的特性（近200种处理）。 5）协议命令 官方文档 首先我们得知道在FFmpeg中所有的输入源都是一种协议(protocol)，如file、http、rtmp、concat等等，这都是一种协议格式，所以网络资源等同于本地文件一样处理就好了。 通过开始时的帮助信息，我们使用ffmpeg -h full输出所有帮助信息，然后我们对一下进行测试： 1）将flv格式转换成fps为60的git格式进行输出，我们先找到git的封装信息： GIF muxer AVOptions: -loop E........ Number of times to loop the output: -1 - no loop, 0 - infinite loop (from -1 to 65535) (default 0) -final_delay E........ Force delay (in centiseconds) after the last frame (from -1 to 65535) (default -1) 注：AVOptions是针对该类型的\"选项\"参数，上面打印的帮助日志包含了非常多的AVOptions，自己多尝试两边就知道用法了。 2）通过上面AVOptions指定封装成GIF的\"选项\"进行输出，如下： ffmpeg -r 60 -i Kobe_in.flv -r 60 -loop 0 output.GIF 3）同样，当我们把网络数据进行转换到本地文件，便可以这样： ffmpeg -r 60 -i rtmp://58.200.131.2:1935/livetv/hunantv -r 60 -loop 0 output.GIF 4.1）推送道理也一样，只是将本地协议的格式转为远程协议的格式（将mp4转换成flv格式推送到rtmp服务器）： ffmpeg -r 60 -i lol.mp4 -r 60 -f flv rtmp://8.129.163.125:1935/myapp/testpush 注：这里特意说一下-f的使用，强制使用格式，如这里的flv是flv muxer AVOptions: 4.2）然后可以通过ffplay进行拉流播放： ffplay rtmp://8.129.163.125:1935/myapp/testpush 6）把多个图片合并视频命令 使用-f指定image2解封装格式，解封装-pattern_type\"选项\"的值glob_sequence、从下表为5开始，最多200张作为输入。输入fps为15，用x264进行编码输出mp4： ffmpeg -f image2 -pattern_type glob_sequence -start_number 5 -start_number_range 200 -framerate 12 -i 'loading/loading%d.png' -r 15 -vcodec libx264 out.mp4 7）合并视频命令 首先要明确合并的效果，以及合并的输入流。明确是否需要重新编解码、滤镜处理等等。这就是需要看输入的类型了。比如当两中尺寸不等，编码格式不同的两个输入文件，如果合并时，是否需要把尺寸统一、编码统一，然后才能进行合并。 _________ | | | input 0 |\\ |_________| \\ \\ _________ \\ | | _________ \\| decode? | _________ | | | | | | | input 1 |---->| filter? |--->| output | |_________| | | |_________| /| encode? | / | ... | _________ / |_________| | | / | input 2 |/ |_________| 指令使用看：FFMpeg无损合并视频的多种方法 （原文没找到），为了方便看，搬过来： 1）方法一：FFmpeg concat协议 对于 MPEG 格式的视频，可以直接连接： ffmpeg -i \"concat:input1.mpg|input2.mpg|input3.mpg\" -c copy output.mpg 对于非 MPEG 格式容器，但是是 MPEG 编码器（H.264、DivX、XviD、MPEG4、MPEG2、AAC、MP2、MP3 等），可以包装进 TS 格式的容器再合并。在新浪视频，有很多视频使用 H.264 编码器，可以采用这个方法 ffmpeg -i input1.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input1.ts ffmpeg -i input2.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input2.ts ffmpeg -i input3.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input3.ts ffmpeg -i \"concat:input1.ts|input2.ts|input3.ts\" -c copy -bsf:a aac_adtstoasc -movflags +faststart output.mp4 保存 QuickTime/MP4 格式容器的时候，建议加上 -movflags +faststart。这样分享文件给别人的时候可以边下边看。 2）方法二：FFmpeg concat 分离器 这种方法成功率很高，也是最好的，但是需要 FFmpeg 1.1 以上版本。先创建一个文本文件 filelist.txt： file 'input1.mkv' file 'input2.mkv' file 'input3.mkv' 然后： ffmpeg -f concat -i filelist.txt -c copy output.mkv 注意：使用 FFmpeg concat 分离器时，如果文件名有奇怪的字符，要在 filelist.txt 中转义。 3）方法三：Mencoder 连接文件并重建索引 这种方法只对很少的视频格式生效。幸运的是，新浪视频使用的 FLV 格式是可以这样连接的。对于没有使用 MPEG 编码器的视频（如 FLV1 编码器），可以尝试这种方法，或许能够成功。 mencoder -forceidx -of lavf -oac copy -ovc copy -o output.flv input1.flv input2.flv input3.flv 4）方法四：使用 FFmpeg concat 过滤器重新编码（有损） 语法有点复杂，但是其实不难。这个方法可以合并不同编码器的视频片段，也可以作为其他方法失效的后备措施。 ffmpeg -i input1.mp4 -i input2.webm -i input3.avi -filter_complex '[0:0] [0:1] [1:0] [1:1] [2:0] [2:1] concat=n=3:v=1:a=1 [v] [a]' -map '[v]' -map '[a]' output.mkv 如你所见，上面的命令合并了三种不同格式的文件，FFmpeg concat 过滤器会重新编码它们。 注意这是有损压缩。 [0:0] [0:1] [1:0] [1:1] [2:0] [2:1] 分别表示第一个输入文件的视频、音频、第二个输入文件的视频、音频、第三个输入文件的视频、音频。 concat=n=3:v=1:a=1 表示有三个输入文件，输出一条视频流和一条音频流。 [v] [a] 就是得到的视频流和音频流的名字，注意在 bash 等 shell 中需要用引号，防止通配符扩展。 提示: 以上三种方法，在可能的情况下，最好使用第二种。第一种次之，第三种更次。第四种是后备方案，尽量避免。规格不同的视频合并后可能会有无法预测的结果。有些媒体需要先分离视频和音频，合并完成后再封装回去。对于 Packed B-Frames 的视频，如果封装成 MKV 格式的时候提示 Can't write packet with unknown timestamp，尝试在 FFmpeg 命令的 ffmpeg 后面加上 -fflags +genpts 5）了解了上面情况下，我们合并两个尺寸不一样的视频，lol_640x360.mp4与Kobe_384x216.flv合并成out_640x360.avi。思路：我们通过把两个尺寸统一成一致（处理方案很多），然后转码成统一格式.ts，使用concat协议输出到一个文件。 a. 使用scale滤镜把Kobe_384x216.flv放大成lol_640x360.mp4尺寸，setsar(像素比)&setdar （宽高比），vcodec（h264编码），acodec（aac编码），不然默认其他编码（还应该制定码率什么的，这里从简了）： ffmpeg -i Kobe_384x216.flv -vf scale=w=640:h=360,setsar=sar=1/1,setdar=dar=16/9 -vcodec libx264 -acodec aac Kobe_640x360.flv 注：setsar等时通过使用ffprobe查看lol_640x360.mp4文件所知道的，如： Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 256 kb/s, 15 fps, 15 tbr, 15360 tbn, 30 tbc (default) b. 统一转码成.ts格式： ffmpeg -i Kobe_640x360.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input1.ts ffmpeg -i lol_640x360.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts input2.ts c. 使用concat协议输出到一个文件： ffmpeg -i \"concat:input1.ts|input2.ts\" -c copy -bsf:a aac_adtstoasc -movflags +faststart out_640x360.avi ffprobe 官方文档 源码所在：fftools/ffprobe.c ffprobe作为FFmpeg的分析工具，内容非常强大，可以说是能分析输入源一切的信息。 1）帮助文档 同上操作，先看帮助文档： ffprobe --help 这里就不贴出来了，而且很多跟ffmpeg都是一样的。 2）指令格式 usage: ffprobe [OPTIONS] [INPUT_FILE] 最简单使用就是直接忽略[OPTIONS]，如： ffprobe Kobe.flv 输出结果如下： Input #0, flv, from 'Kobe.flv': Metadata: metadatacreator : iku hasKeyframes : true hasVideo : true hasAudio : true hasMetadata : true canSeekToEnd : false datasize : 828648 videosize : 690936 audiosize : 133316 lasttimestamp : 30 lastkeyframetimestamp: 25 lastkeyframelocation: 696106 Duration: 00:00:30.00, start: 0.000000, bitrate: 221 kb/s Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 182 kb/s, 15.07 fps, 15 tbr, 1k tbn, 30 tbc Stream #0:1: Audio: aac (HE-AAC), 44100 Hz, stereo, fltp, 33 kb/s 其他也非常简单明了，如查看packets信息-show_packets show packets info： ffprobe -show_packets Kobe.flv 输出将是所有packet信息，这里拿出一小段： [PACKET] codec_type=video stream_index=0 pts=29533 pts_time=29.533000 dts=29533 dts_time=29.533000 duration=66 duration_time=0.066000 convergence_duration=N/A convergence_duration_time=N/A size=777 pos=818689 flags=__ [/PACKET] ffplay 官方文档 源码所在：fftools/ffplay.c 这是一个非常非常强大的播放器，市面上的视频格式都支持播放，而且播放裸流数据一样是可以的，如pcm音频，yuv视频、rgb视频等。 1）帮助文档 同上操作，先看帮助文档： ffplay --help 这里就不贴出来了，而且很多跟ffmpeg都是一样的。 2）指令格式 usage: ffplay [options] input_file 最简单使用就是直接忽略[OPTIONS]，如： ffplay Kobe.flv 3）播放pcm数据 关于裸数据的播放，就必须要设置这几个参数：采样率、声道数、位宽、值的范围(fmt)和排序方式(大端/小端)。原因就是：一个存pcm裸流数据的文件是不包含任何参数的，只是按顺序存放一个个采样点的值，具体请看：直播推流全过程：音频数据源之PCM（2） 。 1）我们先从Kobe.flv解码出pcm裸流数据，存储到文件中，我们通过ffprobe Kobe.flv命令参看解码前的pcm信息如下： Stream #0:1: Audio: aac (HE-AAC), 44100 Hz, stereo, fltp, 33 kb/s 注：可以直接通过FFmpeg Decode（解码） 生成pcm裸流数据。 2）所以解码后得到的pcm数据信息为：采样率=44100，声道数=2，位宽=4，值的范围(fmt)=fltp（32位浮点类型），排序方式=小端。根据这几个英文意思，从帮助文档里面找到先关的几条\"选项\"如下： -ar ED..A.... set audio sampling rate (in Hz) (from 0 to INT_MAX) (default 0) -f fmt force format f32be demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) f32le demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) s32be demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) u32be demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) 3）这里特意多取几个进行介绍一下： f32be=浮点类型32位大端排序；f32le=浮点类型32位小端排序；s32be=带符号整形32位大端排序；u32be=无符号整形32位大端排序。相信你一下子就能找到规律了。 第一个字母：f为浮点类型，s为带符号整形，u为无符号整形； 中间数字：表示占位，有8、16、24、32、64可选。 4）所以，最后播放pcm命令为： ffplay -ar 44100 -channels 2 -f f32le Kobe-44100-2-32-33ks.pcm 4）播放yuv数据 跟pcm一样，分析过程并无多大区别。而且yuv相关更少，只有跟像素、yuv类型以及fps相关。 1）我们先从Kobe.flv解码出yuv裸流数据，存储到文件中，我们通过ffprobe Kobe.flv命令参看解码前的yuv信息如下： Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 182 kb/s, 15.07 fps, 15 tbr, 1k tbn, 30 tbc 2）所以解码后得到的yuv数据信息为：像素=384x216，yuv类型=yuv420p，fps=15。根据这几个英文意思，从帮助文档里面找到先关的几条\"选项\"如下： rawvideo demuxer AVOptions: -video_size .D....... set frame size -pixel_format .D....... set pixel format (default \"yuv420p\") -framerate .D....... set frame rate (default \"25\") colorspace AVOptions: format ..FV..... Output pixel format (from -1 to 164) (default -1) yuv420p ..FV..... yuv420p10 ..FV..... yuv420p12 ..FV..... yuv422p ..FV..... yuv422p10 ..FV..... yuv422p12 ..FV..... yuv444p ..FV..... yuv444p10 ..FV..... yuv444p12 ..FV..... 3）所以，最后播放yuv命令为： ffplay -f rawvideo -video_size 384x216 -pixel_format yuv420p -framerate 15 Kobe-384x216-15fps.yuv 5）播放网络资源 上面我们叫的输入源都被称为一种协议格式，跟本地文件（如.flv等）处理是一样的，所以播放网络资源后面直接跟地址就行了，如： rtmp://58.200.131.2:1935/livetv/hunantv 6）播放的其他重要参数 1）打印日志，这有助于我们分析问题，或者阅读源码： -v loglevel set logging level 参数\"选项\"： \"quiet\" \"panic\" \"fatal\" \"error\" \"warning\" \"info\" \"verbose\" \"debug\" \"trace\" 使用： ffplay -v \"debug\" lol.mp4 2）生成日志 -report generate a report 使用： ffplay -v \"debug\" `-report lol.mp4 3）一些常规的播放设置相关 -x width force displayed width //指定宽度 -y height force displayed height //指定高度 -s size set frame size (WxH or abbreviation) //设置帧的大小（过时？），使用：video_size？ -fs force full screen //全屏播放 -an disable audio //禁用声音 -vn disable video //禁用视频 -sn disable subtitling //禁用字幕 -ss pos seek to a given position in seconds //从指定开始播放（跳过多少秒） -t duration play \"duration\" seconds of audio/video //播放多长时间（单位秒） -bytes val seek by bytes 0=off 1=on -1=auto -seek_interval seconds set seek interval for left/right keys, in seconds //设置按下左右键时快退/快进多少秒 -nodisp disable graphical display //播放时无显示（可用于调试） -noborder borderless window //无边框播放 -alwaysontop window always on top //窗口总在最上面 -volume volume set startup volume 0=min 100=max //设置音量（0——100），0为静音 -f fmt force format //强制播放的格式 -window_title window title set window title //播放器窗口添加标题显示 -af filter_graph set audio filters //播放前添加音频滤镜处理后再播放 -vf filter_graph set video filters //播放前添加视频滤镜处理后再播放 -showmode mode select show mode (0 = video, 1 = waves, 2 = RDFT) //画面显示的模式 -i input_file read specified file //指定输入文件（可以忽略） -codec decoder_name force decoder //指定解码器 其中播放前的滤镜处理跟ffmpeg使用一样，只用替换成ffplay即可，如： ffplay -vf boxblur=luma_radius=3:luma_power=8 -af aecho=0.8:0.88:60:0.4 Kobe.flv Kobi.flv下载 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/19_swscale.html":{"url":"FFmpeg/19_swscale.html","title":"Swscale（图像转换）","keywords":"","body":"Swscale（图像转换）FFmpeg图像转换流程libswscale库代码例子Swscale（图像转换） 本文对应官方例子：doc/examples/scaling_video.c。 FFmpeg图像转换流程 只有三个简单的函数就可以实现了，而在FFmpeg Transcode(转码) 中的应用就是对应着处理音频重采样的地方。重采样是对解码后的裸流数据（AVFrame中的data）进行格式转换等，而图像转换则是视频对解码后的裸流数据（AVFrame中的data）进行格式转换。 libswscale库 该库就是对图片转换等处理，提供了高级别的图像转换API，例如它允许进行图像缩放和像素格式转换等大量；根音频对应的处理库是swresample，它允许操作音频采样、音频通道布局转换与布局调整等。 我们平常也可以通过学习这些源码，来理解这些格式；也可以把里面的函数实现拷贝出来当做自己常规处理的工具。 代码例子 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/18 * description: 来着官方例子：doc/examples/transcoding.c * */ #define __STDC_CONSTANT_MACROS extern \"C\" { #include #include #include } int main(int argc, char **argv) { uint8_t *src_data[4], *dst_data[4]; int src_linesize[4], dst_linesize[4]; //384x216 int src_w = 384, src_h = 216, dst_w = src_w, dst_h = src_h; enum AVPixelFormat src_pix_fmt = AV_PIX_FMT_YUV420P, dst_pix_fmt = AV_PIX_FMT_YUV444P; const char *dst_filename = NULL, *src_filename = NULL; FILE *dst_file, *src_file; struct SwsContext *sws_ctx; int i, ret; //输入文件从上几篇文章中的Kobe.flv解码而来的yuv420p数据 src_filename = \"source/Kobe-384x216-yuv420p.yuv\"; dst_filename = \"output/Kobe-384x216-yuv44p.yuv\"; remove(dst_filename); src_file = fopen(src_filename, \"rb+\"); dst_file = fopen(dst_filename, \"wb+\"); if (!dst_file) { fprintf(stderr, \"Could not open destination file %s\\n\", dst_filename); exit(1); } /**1. 获取SwsContext*/ /* create scaling context */ sws_ctx = sws_getContext(src_w, src_h, src_pix_fmt, dst_w, dst_h, dst_pix_fmt, SWS_BILINEAR, NULL, NULL, NULL); if (!sws_ctx) { fprintf(stderr, \"Impossible to create scale context for the conversion \" \"fmt:%s s:%dx%d -> fmt:%s s:%dx%d\\n\", av_get_pix_fmt_name(src_pix_fmt), src_w, src_h, av_get_pix_fmt_name(dst_pix_fmt), dst_w, dst_h); ret = AVERROR(EINVAL); goto end; } /**2.1 申请输入方内存，并初始化大小（对应AVFrame中的data和linesize）*/ /* allocate source and destination image buffers */ if ((ret = av_image_alloc(src_data, src_linesize, src_w, src_h, src_pix_fmt, 16)) Kobe.flv Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"OpenGL/":{"url":"OpenGL/","title":"OpenGL","keywords":"","body":"OpenGL简述推荐文章OpenGL 简述 OpenGL使用GPU渲染视频，释放宝贵的CPU资源，学习它是必不可少的。但是，正如雷神所说 ：作为一个搞视频技术的人研究OpenGL，需要耗费大量时间和精力，这样学习不是很经济。所以推荐只学习有关视频渲染相关知识。 推荐文章 OpenGL介绍，和相关程序库 纹理有关的基础知识 、OpenGL播放RGB/YUV 、OpenGL播放YUV420P（通过Texture，使用Shader） Android OpenGL ES官方文档 LearnOpenGL-CN OpenGL电子书下载 OpenGL基础知识 AFPlayer：OpenGL ES播放RGB Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"OpenGL/01_opengl.html":{"url":"OpenGL/01_opengl.html","title":"OpenGL基础知识","keywords":"","body":"OpenGL基础知识OpenGL简述OpenGL ESOpenGL播放YUV学习资料OpenGL基础知识 OpenGL简述 OpenGL（英语：Open Graphics Library，译名：开放图形库或者“开放式图形库”）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序编程接口（API）。对于我们音视频开发最大的好处是可以使用OpenGL操控显卡（GPU）播放（渲染）视频，实现硬件加速，并且释放了宝贵的CPU资源。 OpenGL只提供了渲染的功能，核心API没有窗口系统、音频、打印、键盘／鼠标或其他输入设备的概念。然而，有些集成于原生窗口系统的东西需要允许和宿主系统交互，并且为了减轻繁重的编程工作，封装了OpenGL函数，为开发者提供相对简单的用法，实现一些较为复杂的操作。 1）以下包可以用来创建并管理 OpenGL 窗口，也可以管理输入，但几乎没有除此以外的其它功能： GLFW ——跨平台窗口和键盘、鼠标、手柄处理；偏向游戏。 freeglut ——跨平台窗口和键盘、鼠标处理；API 是 GLUT API 的超集，同时也比 GLUT 更新、更稳定。 GLUT ——早期的窗口处理库，已不再维护。 2）支持创建 OpenGL 窗口的还有一些“多媒体库”，同时还支持输入、声音等类似游戏的程序所需要的功能： Allegro 5——跨平台多媒体库，提供针对游戏开发的 C API SDL——跨平台多媒体库，提供 C API SFML——跨平台多媒体库，提供 C++ API；同时也提供 C#、Java、Haskell、Go 等语言的绑定 3）窗口包： FLTK——小型的跨平台 C++ 窗口组件库 Qt——跨平台 C++ 窗口组件库，提供许多OpenGL辅助对象 wxWidgets——跨平台 C++ 窗口组件库 OpenGL ES OpenGL ES （OpenGL for Embedded Systems）是三维图形应用程序接口OpenGL的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，如Android 。 OpenGL播放YUV /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/21 * description: MacOS使用OpenGL播放YUV * */ #define GLFW_INCLUDE_GLU #define NDEBUG extern \"C\" { #include //MacOS上引入 } //set '1' to choose a type of file to play #define LOAD_RGB24 0 #define LOAD_BGR24 0 #define LOAD_BGRA 0 #define LOAD_YUV420P 1 int screen_w = 384, screen_h = 216; const int pixel_w = 384, pixel_h = 216; //Bit per Pixel #if LOAD_BGRA const int bpp=32; #elif LOAD_RGB24 | LOAD_BGR24 const int bpp = 24; #elif LOAD_YUV420P const int bpp = 12; #endif //YUV file FILE *fp = NULL; unsigned char buffer[pixel_w * pixel_h * bpp / 8]; unsigned char buffer_convert[pixel_w * pixel_h * 3]; inline unsigned char CONVERT_ADJUST(double tmp) { return (unsigned char) ((tmp >= 0 && tmp > 1); int ypSize = nWidth * nHeight; int upSize = (ypSize >> 2); int offSet = 0; y_planar = yuv_src; u_planar = yuv_src + ypSize; v_planar = u_planar + upSize; for (int i = 0; i > 1) * (u_width) + (j >> 1); V = *(u_planar + offSet); // Get the U value from the v planar U = *(v_planar + offSet); // Cacular the R,G,B values // Method 1 R = CONVERT_ADJUST((Y + (1.4075 * (V - 128)))); G = CONVERT_ADJUST((Y - (0.3455 * (U - 128) - 0.7169 * (V - 128)))); B = CONVERT_ADJUST((Y + (1.7790 * (U - 128)))); offSet = rgb_width * i + j * 3; rgb_dst[offSet] = B; rgb_dst[offSet + 1] = G; rgb_dst[offSet + 2] = R; } } free(tmpbuf); } void display(void) { if (fread(buffer, 1, pixel_w * pixel_h * bpp / 8, fp) != pixel_w * pixel_h * bpp / 8) { // Loop fseek(fp, 0, SEEK_SET); fread(buffer, 1, pixel_w * pixel_h * bpp / 8, fp); } //Make picture full of window //Move to(-1.0,1.0) glRasterPos3f(-1.0f, 1.0f, 0); //Zoom, Flip glPixelZoom((float) screen_w / (float) pixel_w, -(float) screen_h / (float) pixel_h); #if LOAD_BGRA glDrawPixels(pixel_w, pixel_h,GL_BGRA, GL_UNSIGNED_BYTE, buffer); #elif LOAD_RGB24 glDrawPixels(pixel_w, pixel_h, GL_RGB, GL_UNSIGNED_BYTE, buffer); #elif LOAD_BGR24 glDrawPixels(pixel_w, pixel_h,GL_BGR_EXT, GL_UNSIGNED_BYTE, buffer); #elif LOAD_YUV420P CONVERT_YUV420PtoRGB24(buffer, buffer_convert, pixel_w, pixel_h); glDrawPixels(pixel_w, pixel_h, GL_RGB, GL_UNSIGNED_BYTE, buffer_convert); #endif //GLUT_DOUBLE glutSwapBuffers(); //GLUT_SINGLE //glFlush(); } void timeFunc(int value) { display(); // Present frame every 40 ms glutTimerFunc(40, timeFunc, 0); } void processSpecialKeys(int key, int x, int y) { printf(\"OpenGL processSpecialKeys: %d\\n\", key); switch (key) { case GLUT_KEY_UP: break; case GLUT_KEY_DOWN: break; case GLUT_KEY_LEFT: break; case GLUT_KEY_RIGHT: break; } } int main(int argc, char **argv) { const char *src_filename = \"source/Kobe-384x216-yuv420p.yuv\"; fp = fopen(src_filename, \"rb+\"); // GLUT init glutInit(&argc, argv); //Double, Use glutSwapBuffers() to show glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB); //Single, Use glFlush() to show // glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB ); //在屏幕中显示的位置 glutInitWindowPosition(0, 0); //窗口的大小 glutInitWindowSize(screen_w, screen_h); //创建窗口 glutCreateWindow(\"Simplest Video Play OpenGL\"); printf(\"OpenGL Version: %s\\n\", glGetString(GL_VERSION)); //显示的函数 glutDisplayFunc(&display); //轮询函数 glutTimerFunc(40, timeFunc, 0); //监听按键 glutSpecialFunc(processSpecialKeys); // Start! glutMainLoop(); return 0; } 学习资料 OpenGL介绍，和相关程序库 纹理有关的基础知识 。 最简单的视音频播放示例5：OpenGL播放RGB/YUV 最简单的视音频播放示例6：OpenGL播放YUV420P（通过Texture，使用Shader） Android OpenGL ES官方文档 LearnOpenGL-CN 电子书 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/":{"url":"MustRead/","title":"重点技术","keywords":"","body":"重点技术重点技术 手撕FLV协议 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/01_flv.html":{"url":"MustRead/01_flv.html","title":"手撕FLV协议","keywords":"","body":"手撕FLV协议实现效果FLV协议实现代码使用ffplay查看输出文件测试文件下载地址手撕FLV协议 实现效果 纯代码实现分离FLV音视频流，并组装成AAC和H264文件，最后能正常播放。注：本文只对有AAC和H264格式音视频流组成的flv进行分离。 FLV协议 注：脚本部分本文未使用，可前往这查看。 实现代码 代码注释已经很详细了，其实就是对照协议表进行数据解析，其中： (1) H264协议请看直播推流全过程：视频编码之H.264（3）; (2) AAC协议请看直播推流全过程：音频编码之AAC（4）; #include /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/01 * description: */ #include #include using namespace std; typedef struct Tag { int TagType; unsigned DataSize; unsigned Timestamp; unsigned TimestampExtended; int StreamID; unsigned char *Data; } Tag; //从flv中第一帧获取的数据 typedef struct FLV_AAC_HEAD { unsigned char audioObjectType: 5; unsigned char samplingFrequencyIndex: 4; unsigned char channelConfiguration: 4; unsigned char other: 3;//占位；这个值不需要的 000 } FLV_AAC_HEAD; //aac封装 每个帧头结构设定恒为7Byte typedef struct AAC_ADST_HEAD { //1. adts_fixed_header unsigned short syncword: 12; /* 恒为 1111 1111 1111*/ unsigned char ID: 1; /* 0：MPEG-4，1：MPEG-2*/ unsigned char layer: 2; /* 00*/ unsigned char protection_absent: 1; /* 是否使用error_check()。0：使用，1：不使用。*/ unsigned char profile: 2; /* AAC类型*/ unsigned char sampling_frequency_index: 4;/* 采样率的数组下标，即：sampling frequeny[sampling_frequency_index] */ unsigned char private_bit: 1; /* 0*/ unsigned char channel_configuration: 3; /* 声道数*/ unsigned char original_copy: 1; /* 0*/ unsigned char home: 1; /* 0*/ //2. adts_variable_header unsigned char copyright_identification_bit: 1; /* 0*/ unsigned char copyright_identification_start: 1; /* 0*/ unsigned short frame_length: 13; /* 帧的长度，包括head*/ unsigned short adts_buffer_fullness: 11; /* 固定0x7FF*/ unsigned char number_of_raw_data_blocks_in_frame: 2; /*在ADTS帧中有number_of_raw_data_blocks_in_frame + 1个AAC原始数据块。number_of_raw_data_blocks_in_frame == 0 表示说ADTS帧中有一个AAC原始数据块。*/ } AAC_ADST_HEAD; int checkFLV(FILE *fp_in) { int ret = 1; char *head = (char *) malloc(9); fread(head, 1, 9, fp_in); //Signature(FLV) if (head[0] != 'F' || head[1] != 'L' || head[2] != 'V') { printf(\"Not FLV File !\\n\"); ret = 0; } //第5个字节为Flags(流信息)，视频标志位：.... ...1；音频标志位：.... .1.. int vFlags = head[4] & 0x01; int aFlags = (head[4] & 0x04) >> 1; if (!vFlags) { printf(\"Not Video Data !\\n\"); ret = 0; } if (!aFlags) { printf(\"Not Audio Data !\\n\"); ret = 0; } free(head); return ret; } int getTagAndTagSize(FILE *fp_in, Tag *tag) { int length = -1; //Tag表中除了Data之外，其他所有的字节 unsigned char *buf = (unsigned char *) malloc(11); //读取Tag前面11个字节 fread(buf, 1, 11, fp_in); tag->TagType = buf[0]; tag->DataSize = (buf[1] Timestamp = (buf[4] TimestampExtended = buf[7]; tag->StreamID = (buf[8] Data = (unsigned char *) calloc(tag->DataSize, sizeof(char)); //读取Data fread(tag->Data, 1, tag->DataSize, fp_in); //读取PreviousTagSize的4个字节 fread(buf, 1, 4, fp_in); //这个长度就是 Tag(n)+PreviousTagSize(n)的长度 length = 11 + tag->DataSize + 4; unsigned PreviousTagSize = (buf[0] TagType, tag->DataSize, tag->Timestamp, tag->TimestampExtended, tag->StreamID, PreviousTagSize); return length; } unsigned char *sps_nalu = NULL; unsigned char *pps_nalu = NULL; char *buf_start = NULL; unsigned sps_nalu_size = 0; unsigned pps_nalu_size = 0; int videoToH264(FILE *fp_video, Tag *tag) { int ret = 1; //tag->Data = FrameType + CodecID + VideoData ; //VideoData = AVCPacketType + CompositionTime + 【Data（注：视频数据）】 unsigned FrameType = (tag->Data[0] & 0xF0) >> 4;//Data表(视频)：高4bit unsigned CodecID = tag->Data[0] & 0x0F; //Data表(视频)：低4bit unsigned AVCPacketType = tag->Data[1]; //VideoData表( AVCVIDEOPACKE) unsigned CompositionTime = (tag->Data[2] Data[3] Data[4];//VideoData表( AVCVIDEOPACKE) if (CodecID != 7) {//7: AVC(高级视频编码) ret = 0; printf(\"Not AVC Error\\n\"); goto end; } printf(\"FrameType=%d\\t|CodecID=%d\\t|AVCPacketType=%d\\t|CompositionTime=%d\\t\\n\", FrameType, CodecID, AVCPacketType, CompositionTime); //从tag->Data[5]开始至结束为【Data（注：视频数据）】 //一般FLV第一个视频Tag为sps和pps数据，需要根据AVCDecoderConfigurationRecord进行解析，单独抽出来封装成NALU数据 if (AVCPacketType == 0) {//0: AVC sequence header printf(\"0: AVC sequence header\\n\"); if (sps_nalu) { free(sps_nalu); } if (pps_nalu) { free(pps_nalu); } unsigned Version = tag->Data[5];//sps：版本 unsigned AVCProfileIndication = tag->Data[6];//sps：编码规格 unsigned profile_compatibility = tag->Data[7];//sps：编码规格 unsigned AVCLevelIndication = tag->Data[8];//sps：编码规格 unsigned reserved_And_lengthSizeMinusOne = tag->Data[9];//sps：应恒为0xFF unsigned numOfSequenceParameterSets = tag->Data[10] & 0x1F;//sps：sps个数，取低5位；tag->Data[10]一般为0xE1 unsigned sps_len = (tag->Data[11] Data[12];//sps：sps长度 sps_nalu_size = sps_len + 4; sps_nalu = (unsigned char *) malloc(sps_nalu_size);//加4个为起始位 memcpy(sps_nalu, buf_start, 4); memcpy(sps_nalu + 4, tag->Data + 13, sps_len); printf(\"sps_len=%d\\nsps_data=\", sps_len); for (int i = 4; i Data[ppsIndex];//pps：pps个数，1个字节； unsigned pps_len = (tag->Data[ppsIndex + 1] Data[ppsIndex + 2];//pps：pps长度 pps_nalu_size = pps_len + 4; pps_nalu = (unsigned char *) malloc(pps_nalu_size);//加4个为起始位 memcpy(pps_nalu, buf_start, 4); memcpy(pps_nalu + 4, tag->Data + ppsIndex + 3, pps_len); printf(\"pps_len=%d\\npps_data=\", pps_len); for (int i = 4; i = tag->DataSize) { printf(\"----break-----, pic_start + 4 >= tag->DataSize\\n\"); break; } printf(\"pic_size=0x%.2x%.2x%.2x%.2x\\n\", tag->Data[pic_start], tag->Data[pic_start + 1], tag->Data[pic_start + 2], tag->Data[pic_start + 3]); pic_size = (tag->Data[pic_start] Data[pic_start + 1] Data[pic_start + 2] Data[pic_start + 3]; pic_next_index = pic_start + 4 + pic_size; if (pic_size DataSize DataSize); break; } printf(\"write pic data,pic_start=%d\\tpic_size=%d\\tpic_next_index=%d\\tDataSize=%d \\n\", pic_start, pic_size, pic_next_index, tag->DataSize); //当前帧数据范围 pic_start + 4 ~ pic_next_index fwrite(buf_start, 1, 4, fp_video); fwrite(tag->Data + pic_start + 4, 1, pic_size, fp_video); pic_start = pic_next_index; } } else if (AVCPacketType == 2) {//2: AVC end of sequence printf(\"2: AVC end of sequence\\n\"); } end: free(tag->Data); return ret; } AAC_ADST_HEAD *aacAdstHead = NULL; int getAACHeadBuf(unsigned char *buf, AAC_ADST_HEAD *aacAdstHead, const unsigned int frame_size) { int ret = 0; aacAdstHead->frame_length = frame_size; //取syncword高位1Byte buf[0] = aacAdstHead->syncword >> 4; //syncword(低4位)+ID(1)+layer(2)+protection_absent(1) buf[1] = ((aacAdstHead->syncword & 0x00F) ID >> 3) + (aacAdstHead->layer >> 1) + aacAdstHead->protection_absent; //profile(2)+sampling_frequency_index(4)+private_bit(1)+channel_configuration(3bit取最高1bit) buf[2] = (aacAdstHead->profile sampling_frequency_index private_bit channel_configuration >> 2); // ...00000 00000000 //channel_configuration(3bit取低2bit)+original_copy(1)+home(1)+copyright_identification_bit(1)+copyright_identification_start(1)+frame_length(13bit取最高2bit) buf[3] = (aacAdstHead->channel_configuration original_copy home copyright_identification_bit copyright_identification_start frame_length >> 11); //frame_length(13bit取：去掉最高2bit开始8位) buf[4] = (aacAdstHead->frame_length >> 3) & 0x0FF; //frame_length(13bit取最低3bit)+adts_buffer_fullness(11bit取高5bit) buf[5] = (aacAdstHead->frame_length adts_buffer_fullness >> 6); //adts_buffer_fullness(11bit取低6bit) + number_of_raw_data_blocks_in_frame(2) buf[6] = (aacAdstHead->adts_buffer_fullness number_of_raw_data_blocks_in_frame; return ret; } int audioToAac(FILE *fp_audio, Tag *tag) { int ret = 1; unsigned SoundFormat = tag->Data[0] >> 4; unsigned SoundRate = (tag->Data[0] & 0x0F) >> 2; unsigned SoundSize = (tag->Data[0] & 0x02) >> 1; unsigned SoundType = tag->Data[0] & 0x01; //SoundData = AACPacketType + Data unsigned AACPacketType = tag->Data[1]; if (SoundFormat != 10) {//10 = AAC printf(\"Not AAC Error\\n\"); ret = 0; goto end; } if (AACPacketType == 0) {//0: AAC sequence header(参考14496-3 AudioSpecificConfig https://wiki.multimedia.cx/index.php/MPEG-4_Audio#Audio_Specific_Config) printf(\"0: AAC sequence header\\n\"); //读取flv文件中的头信息 FLV_AAC_HEAD *flvAacHead = (FLV_AAC_HEAD *) malloc(sizeof(FLV_AAC_HEAD)); //只要前面两个字节就行，如0x1390=00010011 10010000 flvAacHead->audioObjectType = tag->Data[2] >> 3;//UB[5]=00010... ........ flvAacHead->samplingFrequencyIndex = ((tag->Data[2] & 0x07) Data[3] >> 7);//UB[4]=.....011 1....... flvAacHead->channelConfiguration = (tag->Data[3] & 0x7F) >> 3;//UB[4]=........ .0010... flvAacHead->other = tag->Data[3] & 0x07;//两个字节剩余的bit，为0也行。 //构造AAC_ADST_HEAD if (aacAdstHead) { free(aacAdstHead); } aacAdstHead = (AAC_ADST_HEAD *) malloc(sizeof(AAC_ADST_HEAD)); aacAdstHead->syncword = 0xFFF; aacAdstHead->ID = 1; aacAdstHead->layer = 0; aacAdstHead->protection_absent = 1; aacAdstHead->profile = flvAacHead->audioObjectType; // aacAdstHead->profile = 1; aacAdstHead->sampling_frequency_index = flvAacHead->samplingFrequencyIndex; aacAdstHead->private_bit = 0; aacAdstHead->channel_configuration = flvAacHead->channelConfiguration; aacAdstHead->original_copy = 0; aacAdstHead->home = 0; aacAdstHead->copyright_identification_bit = 0; aacAdstHead->copyright_identification_start = 0; // aacAdstHead->frame_length = xx; //帧的长度动态计算的 aacAdstHead->adts_buffer_fullness = 0x7FF; aacAdstHead->number_of_raw_data_blocks_in_frame = 0; printf(\"audioObjectType=%d\\tsamplingFrequencyIndex=%d\\t channelConfiguration=%d\\n \", flvAacHead->audioObjectType, flvAacHead->samplingFrequencyIndex, flvAacHead->channelConfiguration); free(flvAacHead); } else if (AACPacketType == 1) {//1: AAC raw printf(\"1: AAC raw\\n\"); unsigned char *buf = (unsigned char *) malloc(7);; if (getAACHeadBuf(buf, aacAdstHead, tag->DataSize - 2 + 7) == -1) { printf(\"get AAC Head Buf Error\\n\"); ret = 0; goto end; } fwrite(buf, 1, 7, fp_audio); fwrite(tag->Data + 2, 1, tag->DataSize - 2, fp_audio); free(buf); } end: free(tag->Data); return ret; } int test() { return 0; } int main() { if (test()) {//非0位true exit(1); } printf(\"delete cache video file ? %d\\n\", remove(\"output/Kobe.h264\")); printf(\"delete cache audio file ? %d\\n\", remove(\"output/Kobe.aac\")); FILE *fp_in = fopen(\"assets/Kobe.flv\", \"rb+\"); FILE *fp_video = fopen(\"output/Kobe.h264\", \"wb+\"); FILE *fp_audio = fopen(\"output/Kobe.aac\", \"wb+\"); Tag *tag; if (!fp_in) { cout TagType == 0x12) {//脚本数据 } if (tag->TagType == 0x08) {//音频数据 ret = audioToAac(fp_audio, tag); } if (tag->TagType == 0x09) {//视频数据 ret = videoToH264(fp_video, tag); } } if (sps_nalu) { free(sps_nalu); } if (pps_nalu) { free(pps_nalu); } if (buf_start) { free(buf_start); } fclose(fp_video); return 0; } 使用ffplay查看输出文件 h264文件播放： ffplay xxx.h264 aac文件播放： ffplay xxx.aac 测试文件下载地址 参考 https://www.cnblogs.com/chyingp/p/flv-getting-started.html ISO/IEC_14496-3 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-06 14:28:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/02_h265.html":{"url":"MustRead/02_h265.html","title":"基于H.264看H.265","keywords":"","body":"基于H.264看H.265简述H265结构解析1. NALU type看回H264-视频编码之H.264H2652.2 实例分析基于H.264看H.265 简述 相对于H264来说，H265结构变化看H.265与H.264的差异详解 。但对于解析来说，最主要是多了视频参数集VPS。编码压缩方面相对于H264的宏块单位划分，增加最大为64x64（虽然名字不叫宏块）（压缩率更大了），预测方向也更多了（细节也更清晰了），以树状结构算法存储（可能照成I帧内压缩率高，因为要存这些树状信息），P帧得到很大的压缩率。 H265结构解析 1. NALU type 看回H264-视频编码之H.264 在H264中，每一个前缀码后面跟随的第一个字节即为NALU的语法元素，主要有三部分组成： forbidden_bit(1bit)，nal_reference_bit(2bits)（优先级），nal_unit_type(5bits)（类型） 所以，在H264中，我们如果需要获取NALU的类型，则可以通过以下方式进行解析： nalu_type = first_byte_in_nal & 0x1F 1 H265 而在H265中，每一个前缀码后面跟随的前两个字节为NALU的语法元素，主要有四部分组成： forbidden_zero_bit(1):nal_unit_type(6):nuh_layer_id(6):nuh_temporal_id_plus1(3) 1 在文档中定义如下： 可以看到，NALU的语法元素由H264的一个字节变为两个字节，而nal_unit_type则为NALU的类型，因此我们可以通过以下获取NALU的类型： int type = (code & 0x7E)>>1; 1 type的定义值如下： 上图，即为H265的NALU的TYPE,这里可以将上面的type简单的理解为如下我们需要的类型： VPS=32 SPS=33 PPS=34 IDR=19 P=1 B=0 2.2 实例分析 如下，为下载的视频文件surfing.265的头部信息 如上我们看到了四个NALU包，每个NALU的头部信息为： ① 00 00 00 01 40 01 ---> (0x40 & 0x7E)>>1 = 32 ---> VPS ② 00 00 00 01 42 01 ---> (0x42 & 0x7E)>>1 = 33 ---> SPS ③ 00 00 00 01 44 01 ---> (0x44 & 0x7E)>>1 = 34 ---> PPS ④ 00 00 00 01 26 01 ---> (0x26 & 0x7E)>>1 = 19 ---> IDR 通过以上头结构也可以看到，NALU的与语法元素中，forbidden_zero_bit通常为0，nuh_layer_id通常为0，nuh_temporal_id_plus1通常为1。 参考： H265码流结构分析 H.265与H.264的差异详解 H265码流结构文档参考：《T-REC-H.265-201504-I!!PDF-E.pdf》 高效率视频编码 首页资料 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-10 16:09:31 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"其他零散知识/01_c_compile.html":{"url":"其他零散知识/01_c_compile.html","title":"其他零散知识","keywords":"","body":"初步认识c/c++编译程序的生命周期编译过程与编译器gcc/g++编译器执行过程静态库与动态库(Linux)或者重要工具gcc/g++重要参数初步认识c/c++编译 程序的生命周期 通过了解程序的生命周期，而直到编译所在生命周期的哪一个部分。 编译过程与编译器 编译过程是指编写的源代码通过编译器进行编译，最后生成cpu所能识别的二进制形式存在的源代码的过程。而编译器则是指能够使源代码编译生成二进制形式的工具，根据平台不同，工具也不同，如window是XXX.exe可执行程序，unix系统侧不定后缀名，系统根据文件的头部信息来判断。 例如：在屏幕上输出“VIP会员”，C语言的写法为： puts(\"VIP会员\"); 二进制的写法为： gcc/g++编译器执行过程 gcc/g++编译器能把一个源文件生成一个执行文件，这是因为该编译器是集成了各种程序（预处理器、汇编器等），这个过程中的工作如下： 例子：编写 test.c 如下： #include int main() { printf(\"hello world\\n\"); return 0; } 在控制台执行命令(-o：为指定输出文件)： qincji:build mac$ ls test.c qincji:build mac$ gcc -E test.c -o test.i qincji:build mac$ ls test.c test.i qincji:build mac$ gcc -S test.i -o test.s qincji:build mac$ ls test.c test.i test.s qincji:build mac$ gcc -c test.s -o test.o qincji:build mac$ ls test.c test.i test.o test.s qincji:build mac$ gcc test.o -o test qincji:build mac$ ls test test.c test.i test.o test.s qincji:build mac$ ./test hello world qincji:build mac$ 静态库与动态库(Linux) 动态库与静态库统称为函数库，根据系统不一样，后缀名标识也不一定，如图： 静态库 静态库是指编译链接时，把库文件的代码全部加入到可执行文件中，因此生成的文件比较大，但在运行时也就不再需要库文件了。优点：静态库节省时间，不需要再进行动态链接，需要调用的代码直接就在代码内部。 生成静态库 首先用gcc编绎该文件，生成.o文件，然后ar(archive)工具生成静态库。-fPIC生成与位置无关代码，不用此选项的话，编译后的代码是位置相关的，所以动态载入时，是通过代码拷贝的方式来满足不同进程的需要，而不能达到真正代码段共享的目的。 gcc -fPIC -c test.c -o test.o ar r libtest.a test.o 动态库 动态库与之相反，在编译链接时并没有把库文件的代码加入到可执行文件中，而是在程序执行时由运行时链接文件加载库。优点：动态库节省空间，如果一个动态库被两个程序调用,那么这个动态库只需要在内存中。 首先用gcc编绎该文件，生成.o文件，然后再用gcc指定-shared生成动态库。 gcc -fPIC -c test.c -o test.o gcc -shared test.o -o libtest.so 或者 gcc -fPIC -shared test.c -o libtest.so 简单使用 编辑test.c文件如下： #include void output() { printf(\"hello world\\n\"); } 编辑main.c文件如下： #include \"test.c\" int main(){ output(); return 0; } (1)使用动态库 首页将test.c编译生成动态库libtest.so，然后执行一下命令生成可执行文件： gcc main.c -L. -ltest -o main 当编译main.c时，需要链接代码中引入的test.c文件，其中，-L.：是指编译的时候，在当前目录搜索库的路径；-ltest：指编译的时候使用的库（详见gcc/g++重要参数）。 在控制台中演示： qincji:build mac$ ls libtest.so main.c test.c test.o qincji:build mac$ gcc main.c -L. -ltest -o main qincji:build mac$ ls libtest.a libtest.so main main.c test.c test.o qincji:build mac$ ./main hello world qincji:build mac$ (2)强制使用静态库 gcc main.c -L. -Wl,-Bstatic -ltest -Wl,-Bdynamic -o main 其中：-Wl 表示传递给 ld 链接器的参数，通过ld --help查看 ld 帮助文档时。 注意：我使用MacOs系统时，在查看ld帮助文档(man ld)发现系统并不支持链接动态库，描述如下： -static Produces a mach-o file that does not use the dyld. Only used building the kernel. (3)将静态库打包到动态库中 gcc -shared -o libtest.so -Wl,--whole-archive libtest.a -Wl,--no-whole-archive 其中：--whole-archive: 将未使用的静态库符号(函数实现)也链接进动态库；--no-whole-archive: 默认，未使用不链接进入动态库。 重要工具 ar : 创建或修改archive文件，或从存档文件中提取。如生成静态库，查看详细参数命令： ar --help。 nm : 列出目标文件中的符号（方法），查看动态库方法等，查看详细参数命令： nm --help。 objcopy : 复制和翻译目标文件，查看详细参数命令： objcopy --help。 objdump : 显示目标文件中的信息，查看详细参数命令： objdump --help。 readelf : 显示可执行和可链接格式的文件的信息，查看详细参数命令： readelf --help。 gcc/g++重要参数 通过gcc --help命令查看更多。 -E 只激活预处理,这个不生成文件, 你需要把它重定向到一个输出文件里面。 -S 只激活预处理和编译，就是指把文件编译成为汇编代码。 -c 只激活预处理,编译,和汇编,也就是他只把程序做成obj文件 -o 制定目标名称, 默认的时候, gcc 编译出来的文件是 a.out。 -fPIC 生成与位置无关代码。 -shared 生成动态库，使用例子： gcc -fPIC -shared test.c -o libTest.so -include file 包含某个代码,简单来说,就是便以某个文件,需要另一个文件的时候,就可以用它设定,功能就相当于在代码中使用 #include。例子： gcc test.c -include xxx.h -Dmacro 相当于 C 语言中的 #define macro -Dmacro=defn 相当于 C 语言中的 #define macro=defn -Umacro 相当于 C 语言中的 #undef macro -undef 取消对任何非标准宏的定义 -Idir 在你是用 #include \"file\" 的时候, gcc/g++ 会先在当前目录查找你所制定的头文件, 如果没有找到, 他回到默认的头文件目录找, 如果使用 -I 制定了目录,他会先在你所制定的目录查找, 然后再按常规的顺序去找。 对于 #include, gcc/g++ 会到 -I 制定的目录查找, 查找不到, 然后将到系统的默认的头文件目录查找 。 -I- 就是取消前一个参数的功能, 所以一般在 -Idir 之后使用。 -idirafter dir 在 -I 的目录里面查找失败, 讲到这个目录里面查找。 -iprefix prefix 、-iwithprefix dir 一般一起使用, 当 -I 的目录查找失败, 会到 prefix+dir 下查找 -M 生成文件关联的信息。包含目标文件所依赖的所有源代码你可以用 gcc -M test.c 来测试一下，很简单。 -MM 和上面的那个一样，但是它将忽略由 #include 造成的依赖关系。 　　 -MD 和-M相同，但是输出将导入到.d的文件里面 　　 -MMD 和 -MM 相同，但是输出将导入到 .d 的文件里面。 -Wa,option 此选项传递 option 给汇编程序; 如果 option 中间有逗号, 就将 option 分成多个选项, 然 后传递给会汇编程序。 -Wl.option 此选项传递 option 给连接程序; 如果 option 中间有逗号, 就将 option 分成多个选项, 然 后传递给会连接程序。 -llibrary 指编译的时候使用的库，library是指动态库或静态库的名称。如：liblibrary.a或liblibrary.so，系统会自动加上lib前缀和.a(.so)后缀。 -Ldir 指编译的时候，搜索库的路径。比如你自己的库，可以用它指定目录，不然编译器将只在标准库的目录找。这个dir就是目录的名称。 -O0 、-O1 、-O2 、-O3 编译器的优化选项的 4 个级别，-O0 表示没有优化, -O1 为默认值，-O3 优化级别最高。 -g 只是编译器，在编译的时候，产生调试信息。 　　 -gstabs 此选项以 stabs 格式声称调试信息, 但是不包括 gdb 调试信息。 　　 -gstabs+ 此选项以 stabs 格式声称调试信息, 并且包含仅供 gdb 使用的额外调试信息。 　　 -ggdb 此选项将尽可能的生成 gdb 的可以使用的调试信息。 -static 此选项将禁止使用动态库，所以，编译出来的东西，一般都很大，也不需要什么动态连接库，就可以运行。 -share 此选项将尽量使用动态库，所以生成文件比较小，但是需要系统由动态库。 参考 http://c.biancheng.net/view/450.html https://www.runoob.com/w3cnote/gcc-parameter-detail.html https://cloud.tencent.com/developer/article/1343895 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"其他零散知识/02_net_base.html":{"url":"其他零散知识/02_net_base.html","title":"网络基础知识","keywords":"","body":"网络基础知识通信简单流程IP⼦⽹掩码端口网关UDPTCP其他网络基础知识 通信简单流程 上图简单的描述了一下网络通信的过程，通过上面的知识点来描述一下网络基础知识，补补这方面的基础知识。 IP Internet上每一台计算机都有唯一的地址来标识它的身份，即IP地址，使用域名其实也是要转化为IP地址的。IP地址的结构为x.x.x.x，每个x的值0~255（1Btye的范围）。IP地址分类如下： 由上图可知，把IP地址转换为二进制时，根据最高的位第一个0开始区分什么类型，所以IP的组成结构为： IP = 网络地址(含类型标识) + 主机地址 上图所示的结构是固定的，但是实际上除了类型标识固定之外，网络地址和主机地址占位却是不固定的，如B类地址： IP地址：180.210.242.131； 即：10110100.11010010.11110010.10000011 同时掩码：255.255.248.0； 即：11111111.11111111.11111000.00000000 网络地址：两者进行与运算； 即：10110100.11010010.11110000.00000000（180.210.240.0） 主机地址：掩码取反再和IP做与运算，即：00000000.00000000.00000010.10000011（0.0.2.131） B类的网络地址应该为：xx*.**.xxxxxxxx.xxxxxxxx 但真实的网络地址却是：xx110100.11010010.11110xxx.xxxxxxxx 所以本来的网络号是16位，但它实际网络号是21位，就是借了5位网络位，所以可以划分252^52​5​​个子网，即32个，实际使用30个，这个网段可以容纳主机2^11^个，即2048个，有效2046个一头一尾分别做网络号和广播。 ⼦⽹掩码 ⼦⽹掩码只有⼀个作⽤，就是将某个IP地址划分成⽹络地址和主机地址两部分⼦⽹掩码的设定必须遵循⼀定的规则。与IP地址相同，⼦⽹掩码的⻓度也是32位： 左边是⽹络位，⽤⼆进制数字“1”表示； 右边是主机位，⽤⼆进制数字“0”表示。 端口 通信过程中通过IP地址找到了该台电脑后，需要再跟进端口号来识别该通过那个应用程序。端⼝号只有整数，范围是从0到65535。 端口号有知名端口和动态端口。知名端⼝是众所周知的端⼝号，范围从0到1023（类似电话中的110、119等）。⼀般情况下，如果⼀个程序需要使⽤知名端⼝的需要有root权限。 80端⼝分配给HTTP服务 21端⼝分配给FTP服务 网关 网关(Gateway)又称网间连接器、协议转换器，实质上是一个网络通向其他网络的IP地址。这一设备成为网关设备，路由器就是了。根据\"路由\"协议进行不同网段的信息传递，如图： UDP ⽤户数据报协议，是⼀个⽆连接的简单的⾯向数据报的运输层协议。UDP不提供可靠性，它只是把应⽤程序传给IP层的数据报发送出去，但是并不能保证它们能到达⽬的地。由于UDP在传输数据报前不⽤在客户和服务器之间建⽴⼀个连接，且没有超时重发等机制，故⽽传输速度很快。UDP是⼀种⾯向⽆连接的协议，每个数据报都是⼀个独⽴的信息，包括完整的源地址或⽬的地址，它在⽹络上以任何可能的路径传往⽬的地，因此能否到达⽬的地，到达⽬的地的时间以及内容的正确性都是不能被保证的。 UDP特点： UDP是⾯向⽆连接的通讯协议，UDP数据包括⽬的端⼝号和源端⼝号信息，由于通讯不需要连接，所以可以实现⼴播发送。 UDP传输数据时有⼤⼩限制，每个被传输的数据报必须限定在64KB之内。 UDP是⼀个不可靠的协议，发送⽅所发送的数据报并不⼀定以相同的次序到达接收⽅。 UDP是⾯向消息的协议，通信时不需要建⽴连接，数据的传输⾃然是不可靠的，UDP⼀般⽤于多点通信和实时的数据业务，⽐如： 语⾳⼴播 视频 QQ TFTP(简单⽂件传送） SNMP（简单⽹络管理协议） RIP（路由信息协议，如报告股票市场，航空信息） DNS(域名解释） UDP操作简单，⽽且仅需要较少的监护，因此通常⽤于局域⽹⾼可靠性的分散系统中client/server应⽤程序。例如视频会议系统，并不要求⾳频视频数据绝对的正确，只要保证连贯性就可以了，这种情况下显然使⽤UDP会更合理⼀些。 TCP TCP长/短连接优缺点 ⻓连接可以省去较多的TCP建⽴和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适⽤⻓连接。但是，client与server之间的连接如⼀直不关闭的话，会存在⼀个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取⼀些策略，如关闭⼀些⻓时间没有读写事件发⽣的连接，这样可以避免⼀些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最⼤⻓连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。短连接对于服务器来说管理较为简单，存在的连接都是有⽤的连接，不需要额外的控制⼿段。但如果客户请求频繁，将在TCP的建⽴和关闭操作上浪费时间和带宽。 其他 抓包工具推荐：wireshark 模拟网络环境：Packet Tracer 参考 https://blog.csdn.net/zhangbaoanhadoop/article/details/82745769 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-05 09:22:07 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"其他零散知识/03_adb_dy.html":{"url":"其他零散知识/03_adb_dy.html","title":"ADB的使用","keywords":"","body":"ADB的使用ADB的命令使用大全其他简单实用打开应用，在命令行输入该命令来查看当前在前台的activity：编写shell脚本，然后执行：ADB的使用 ADB的命令使用大全 awesome-adb 其他简单实用 打开应用，在命令行输入该命令来查看当前在前台的activity： qincji:Downloads mac$ adb shell dumpsys activity activities | grep mResumedActivity mResumedActivity: ActivityRecord{e3c2d05 u0 com.ss.android.ugc.aweme.lite/com.ss.android.ugc.aweme.splash.SplashActivity t122} 编写shell脚本，然后执行： #!/bin/bash adb shell am start -n com.ss.android.ugc.aweme.lite/com.ss.android.ugc.aweme.splash.SplashActivity sleep 10s a=1 while (($a Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-03 11:09:48 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "}}