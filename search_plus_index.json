{"./":{"url":"./","title":"打怪之路总览","keywords":"","body":"一、前言二、学习技能三、学习音视频理论知识1.重要知识点2.书籍推荐3.实践3.1.视音频数据处理入门3.2.完整的RTMP推送小项目3.3.播放器小项目四、学习过程的分析工具五、重点开发工具/组件/开源库1.FFmpeg1.1.学习途径1.2.学习路线2 OpenGL3.音频相关处理库4.视频相关处理库5.其他常用库六、实践项目七、最后音视频学习资料为了更好的阅读请前往GitBook 欢迎加入QQ技术交流群：389713575 一、前言 这里整理有着丰富的音视频开发的学习资源、开发工具、优秀书籍、教程和开源项目，旨在帮助开发者和爱好者更好地学习、实践和工作。而下图是开发处理的过程： 二、学习技能 语言 重要度 作用 C/C++ ★★★★★ 作为底层开发语言，可以实现音视频编码/解码，滤镜等高性能的操作，是音视频开发中最为重要的基础语言之一。 Python ★★★☆☆ Python 在音视频开发中可以用于快速开发或原型开发，尤其在深度学习及音视频处理应用中表现突出。 Java/Kotlin ★★★☆☆ Java/Kotlin 适用于 Android平台的音视频开发，它们提供了音视频录制、剪辑、播放等 API。 Objective-C/Swift ★★★☆☆ 用于MacOS、iOS平台的开发，其开发API包含音视频流的处理和流媒体播放等。 JavaScript ★☆☆☆☆ JavaScript是Web开发的常用编程语言，它的WebRTC技术可以用于浏览器中的音视频流处理和实时通讯。 Shell ★☆☆☆☆ 脚本编写、系统管理等。 CMake ★☆☆☆☆ 跨平台编译、构建工具等。 GLSL ★☆☆☆☆ 图形学、OpenGL着色器等。 三、学习音视频理论知识 1.重要知识点 知识点 重要度 作用 YUV/RGB ★★★★★ 视频原始（裸流）数据，解码最终显示就是一帧帧YUV数据 。 PCM ★★★★★ 音频原始（裸流）数据，解码最终播放的就是PCM数据。 H.264(AVC) ★★★★★ 目前主流的视频编解码协议。 H.265(HEVC) ★★★☆☆ 基于H.264的升级版，大幅度提升了编码大小和质量。因为版权和收费问题没有普及。 AAC ★★★★★ 目前主流的音频编解码协议。 RTMP ★★★☆☆ 直播推流，看侧重点 封装格式 ★★☆☆☆ MP4、AVI、MKV、RMVB、FLV等容器，把音频、视频、字幕等通道封装成一个文件 webrtc(VP8) ★★★☆☆ P2P的音视频通话，看侧重点 OpenGL ★★★☆☆ 使用GPU渲染视频，释放宝贵的CPU资源，看侧重点 2.书籍推荐 书：音视频开发进阶指南：基于Android与iOS平台的实践(京东) ：第1章　音视频基础概念；电子书往最后翻。 书：Android 音视频开发_何俊林(京东) ：第1章　音视频基础知识；电子书往最后翻。 书：新一代视频压缩编码标准-H.264/AVC(第二版)(京东) : 讲述H.264等编解码原理实现，其中几个算法 这篇文章讲的很深刻。 这系列文章通俗易懂讲述编解码的一些知识 。 3.实践 3.1.视音频数据处理入门 [总结]视音频编解码技术零基础学习方法 系列文章，介绍了视音频编解码技术大体上原理和流程，通俗易懂。包括以下文章： 视音频数据处理入门：RGB、YUV像素数据处理 ：视频就是由它们组成的。 视音频数据处理入门：PCM音频采样数据处理 ：音频就是由它们组成的。 视音频数据处理入门：H.264视频码流解析 ：视频编码技术的一种（现代音视频开发必须掌握）。 视音频数据处理入门：AAC音频码流解析 ：音频编码技术的一种（现代音视频开发必须掌握）。 视音频数据处理入门：FLV封装格式解析 ：音视频封装格式的一种。 视音频数据处理入门：UDP-RTP协议解析 ：音视频协议的一种。 3.2.完整的RTMP推送小项目 此过程，体现从0~1。0是采集到的原始视频和音频数据，进行编码、封装，变为1（文件）。 直播推流全过程：总纲 直播推流全过程：视频数据源之YUV 直播推流全过程：音频数据源之PCM 直播推流全过程：视频编码之H.264 直播推流全过程：音频编码之AAC 直播推流全过程：直播推流编码之RTMP 其他：H.264符号描述 其他：直播优化基础 3.3.播放器小项目 此过程，体现从1~0。与3.2正好相反，把封装的文件(网络数据)通过解封装和解码，得到原始的裸数据（一帧帧图片）进行播放。 AFPlayer项目 Android实现FFmpeg、OpenSL ES、OpenGL SE、MediaCodec等，实现简单的播放器，主要体现出相关知识点的使用。 四、学习过程的分析工具 工具 作用 下载地址 VideoEye 来自雷神的强大实时视频码流分析软件。 地址 Codecian H264/H265等分析工具（跨平台）。 地址 H264Visa H.264/AVC实时视频分析工具。 （略） Elecard StreamEye 编码视频的可视化表现，流结构分析，这些流是MPEG-1/2/4 or AVC/H.264 VES(视频基本流)、SS（MPEG1的系统流）、PS(MPEG2的程序流)、TS(mpeg2的传输流)。 （略） Hxd Hex Editor 16进制查看工具。 地址 ffprobe ffmpeg中自带的分析工具，非常强大，不过上手有难度。 参考 五、重点开发工具/组件/开源库 1.FFmpeg 音视频开发是绕不开FFmpeg的，因为它是一个\"集大成者\"，里面已经包含或可集成现代几乎所有的音视频技术（库）。 1.1.学习途径 阅读官方文档 学习官方例子（源码中doc/examples/xxx） [总结]FFMPEG视音频编解码零基础学习方法 FFmpeg原理（推荐） FFmpeg 中文文档 书籍（电子书往最后翻） 1.FFmpeg从入门到精通(京东) 2.FFMPEG_FFPLAY源码剖析(CSDN) 3.音视频开发进阶指南：基于Android与iOS平台的实践(京东) 4.Android 音视频开发_何俊林(京东) 1.2.学习路线 这里不推荐直接学习雷神的 [总结]FFMPEG视音频编解码零基础学习方法，建议是通过在学习FFmpeg官方例子中进行学习，避免先入为主使用了过时的API。 1.2.1.源码编译 编译ffmpeg4.2.2通过这篇文章我们基本可以编译出我们想要的FFmpeg库 1.2.2.源码阅读 源码导入：FFmpeg调试环境搭建 阅读参考：FFMPEG_FFPLAY源码剖析(CSDN) 、雷神的FFmpeg源代码系列 1.2.3.学习官方例子 FFmpeg重要结构体（转载） ，因为在学习FFmpeg中，必须得知道结构体中重要参数的含义，否则举步维艰。 FFmpeg Demuxing（解封装） 对应 doc/examples/demuxing_decoding.c 中的解封装部分。 FFmpeg Muxing（封装） 对应 doc/examples/muxing.c 。 FFmpeg Remuxing（重新封装） 对应 doc/examples/remuxing.c 。 FFmpeg Decode（解码） 对应 doc/examples/decode_audio.c 和 doc/examples/decode_video.c 。 FFmpeg Encode（编码） 对应 doc/examples/encode_audio.c 和 doc/examples/encode_video.c 。 FFmpeg 简单实现转码 汇总解封装、解码、编码、封装放到一起方便理解 。 FFmpeg Filter和SDL（Video） 对应 doc/examples/filtering_video.c 。 FFmpeg Filter和SDL（Audio） 对应 doc/examples/filtering_video.c 。 FFmpeg Transcode(转码) 对应 doc/examples/transcoding.c 。 FFmpeg Swscale（图像转换） 对应 doc/examples/scaling_video.c 。 1.2.4.音视频同步 FFmpeg 音视频同步处理 1.2.5.FFmpeg相关 FFmpeg 命令使用指南：分析ffmpeg、ffprobe、ffplay工具使用文档，关联官方文档，以及滤镜、协议、视频合并、各种播放参数等相关介绍 2 OpenGL OpenGL使用GPU渲染视频，释放宝贵的CPU资源，学习它是必不可少的。但是，正如雷神所说 ：作为一个搞视频技术的人研究OpenGL，需要耗费大量时间和精力，这样学习不是很经济。所以推荐只学习有关视频渲染相关知识。 OpenGL介绍，和相关程序库 纹理有关的基础知识 、OpenGL播放RGB/YUV 、OpenGL播放YUV420P（通过Texture，使用Shader） Android OpenGL ES官方文档 LearnOpenGL-CN OpenGL电子书下载 OpenGL基础知识 GLSL（着色器语言）中文手册 Android OpenGL ES 3.0 从入门到精通系统性学习教程 OpenGL ES推荐：WebGL编程指南（资源见下） 3.音频相关处理库 库名称 作用 平台 项目地址 SoX SoX 可以进行音频处理，如音调、声速、混响、后暂式等等 Linux、macOS、Windows https://github.com/chirlu/sox Essentia Essentia 可以进行音频处理、分析、提取等，提供了很多现成的功能算法 Windows、macOS、Linux、Android、iOS https://github.com/MTG/essentia Spleeter Spleeter 是一款分离歌曲音轨的库，可以分离歌曲中的伴奏、人声等（如：不带背景乐的伴奏） Linux、macOS、Windows https://github.com/deezer/spleeter libsndfile libsndfile 可以读写、处理多种音频文件，支持变速、变调等处理 Linux、macOS、Windows http://www.mega-nerd.com/libsndfile/ Rubber Band Audio Rubber Band Audio 可以进行音频处理，支持变速、变调、变形和时间拉伸等多种处理操作 Linux、macOS、Windows https://breakfastquay.com/rubberband/ librubberband librubberband 是 Rubber Band Audio 的 C++ 接口，简化了音频处理操作。 Linux、macOS、Windows https://github.com/breakfastquay/rubberband/tree/master/src SoundTouch SoundTouch 可用进行音频连续变速变调处理，包括作品（“chipway/Pydub-Playground”），可实现歌曲同步播放。 Windows、macOS、Linux、Android、iOS https://gitlab.com/soundtouch/soundtouch Sonic Sonic 可实现音频变速、变调等处理，运用于 播客、游戏等领域。 Windows、macOS、Linux、Android、iOS https://github.com/waywardgeek/sonic Oboe Oboe是一个C++库，它使在Android上建立高性能的音频应用变得容易。 Android https://github.com/google/oboe 4.视频相关处理库 库名称 作用 平台 项目地址 FFmpeg FFmpeg 可以转换视频文件格式、添加滤镜、剪辑、提取音频等功能 Linux、macOS、Windows https://github.com/FFmpeg/FFmpeg Libav libav 同样可以转换视频文件格式、添加滤镜、剪辑、提取音频等功能，是 FFmpeg 的一个分支版本 Linux、macOS、Windows https://libav.org/ GStreamer GStreamer 是一个流媒体框架，支持流式传输视频文件并进行处理，可以添加滤镜、编解码、格式转换等功能 Linux、macOS、Windows https://gstreamer.freedesktop.org/ MLT Framework MLT Framework 可以用于视频混合、转换、添加滤镜、效果等，也是 Kdenlive 软件所使用的引擎 Linux、macOS、Windows https://github.com/mltframework/ OpenCV OpenCV 是一个计算机视觉库，但也支持读取、写入视频文件、添加滤镜、实现对象追踪等功能 Linux、macOS、Windows https://github.com/opencv/opencv Blender Blender 是一个 3D 制作软件，支持读取、写入视频文件、添加滤镜、实现对象制作等功能 Linux、macOS、Windows https://www.blender.org/ MLV App MLV App 主要用于处理和编辑 Magic Lantern 的电影录制格式，可以进行视频转换和色彩分级等 Linux、macOS、Windows https://bitbucket.org/dmilligan/mlv_app/ HandBrake HandBrake 是一个跨平台的视频转换器，支持多种输入、输出格式，可以添加滤镜、转换分辨率等操作 Linux、macOS、Windows https://github.com/HandBrake/HandBrake Avidemux Avidemux 是一款视频编辑器，支持许多基本的编辑功能，如修建、编码、添加滤镜等 Linux、macOS、Windows http://avidemux.sourceforge.net/ Shotcut Shotcut 是一个跨平台的视频编辑器，支持多种基本编辑功能，并提供了丰富的视频滤镜 Linux、macOS、Windows https://www.shotcut.org/ Lightworks Lightworks 是一款视频编辑软件，支持多种高级编辑功能，如多个视频轨道、音频同步、字幕、剪辑等 Linux、macOS、Windows https://www.lwks.com/ Kino Kino 是一款开源的视频编辑器，支持视频的剪辑、滤镜、导出等 Linux http://www.kinodv.org/ Cinelerra GG Cinelerra GG 是截至目前正在活跃开发的 Cinelerra 的一个独特分支，具有更多先进的编辑功能 Linux https://cinelerra-gg.org/ Flowblade Flowblade 是一个跨平台的视频编辑器，具有自动编辑、视频剪辑、渐变调色、音轨图、文本动画等功能 Linux、macOS https://github.com/jliljebl/flowblade Olive Olive 是一款跨平台的视频编辑器，支持多个视频、音频和图像轨道，具有类似 Adobe Premiere 和 Final Cut Pro 等视频编辑器的界面 Linux、macOS、Windows https://www.olivevideoeditor.org/ 5.其他常用库 库名称 描述 作用平台 项目地址 ARToolKit 一个用于创建增强现实体验的开源跟踪库 多平台 https://github.com/artoolkit/ARToolKit5/ Kurento 可以集成WebRTC和媒体服务器的平台 Web https://www.kurento.org/ Webrtc 实时通信的开源项目，包括音频、视频和数据通信 Web https://webrtc.org/ VLC 一个免费的开源跨平台媒体播放器和框架，可播放大多数多媒体文件 多平台 https://www.videolan.org/vlc/index.html ExoPlayer 一个可扩展的Android媒体播放器，包括支持DASH和HLS的内置组件 Android https://github.com/google/ExoPlayer Vitamio Android和iOS的流媒体解决方案 Android，iOS https://www.vitamio.org/en/ AVPlayer 一个iOS播放器框架，支持本地和网络音频和视频文件 iOS https://developer.apple.com/documentation/avfoundation/avplayer FFmpegKit FFmpeg库的原生iOS和Android包装器 iOS，Android https://github.com/tanersener/ffmpeg-kit libVLC 适用于Android、iOS、tvOS和macOS的VLC音视频框架 Android，iOS，tvOS，macOS https://code.videolan.org/videolan/libvlc-framework AVKit 一个macOS框架，提供音频和视频播放和处理功能 macOS https://developer.apple.com/documentation/avkit Unity Video Player 一种用于在游戏中播放视频的Unity组件 Unity https://docs.unity3d.com/Manual/class-VideoPlayer.html Three.js 一个基于WebGL的跨平台JavaScript库，可用于创建和显示3D计算机图形 Web https://threejs.org/ A-Frame 一个用于构建虚拟现实和增强现实的Web框架 Web https://aframe.io/ WebVR 一种可用于任何Web浏览器的虚拟现实解决方案 Web https://webvr.info/ Web Audio API JavaScript API，用于处理和操纵音频 Web https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API Tone.js 用于Web音乐软件的框架 Web https://tonejs.github.io/ Pygame 用于开发2D游戏的Python库 Python https://www.pygame.org/news OpenAL 多平台上的跨API的3D音频库 多平台 https://www.openal.org/ SDL_mixer 用于游戏和其他多媒体应用程序的音频库 多平台 https://www.libsdl.org/projects/SDL_mixer/ Houdini 针对3D艺术家和游戏开发人员的3D建模、动画和视觉效果软件 多平台 https://www.sidefx.com/ Natron 免费的、开源的视频合成软件 多平台 https://natrongithub.github.io/ OBS Studio 免费、开源、跨平台的流媒体和录制软件 多平台 https://obsproject.com/ GIMP 免费的开源图像编辑器 多平台 https://www.gimp.org/ Inkscape 一个用于创建和编辑矢量图形的开源软件 多平台 https://inkscape.org/ MediaInfo 一个开源的多媒体信息解析器，用于获取媒体文件的各种信息 多平台 https://mediaarea.net/en/MediaInfo libde265 一种异步HEVC解码器库 多平台 https://github.com/strukturag/libde265 libvpx 适用于WebRTC、VP8和VP9的开源视频编解码器 多平台 https://github.com/webmproject/libvpx x264 一种高质量的H.264 / AVC编码器 多平台 https://www.videolan.org/developers/x264.html x265 一种用于UHD编码的开源HEVC编码器 多平台 https://bitbucket.org/multicoreware/x265_git Theora 一种开源视频编解码器 Web https://www.theora.org/ Ogg 一种开源多媒体容器格式，通常与Theora和Vorbis一起使用 Web https://www.xiph.org/ogg/ Vorbis 一种开源音频编解码器 Web https://xiph.org/vorbis/ WebVTT 一种用于视频文本替代品的Web文本轨道 Web https://w3c.github.io/webvtt/ PyAV 基于FFmpeg的Python封装，可用于解码、编码和处理音频和视频 Python https://pyav.org/ FFpyplayer Python+FFmpeg解决方案，在Python中播放音频和视频 Python https://ffpyplayer.readthedocs.io/en/latest/ SoundManager2 一个轻型、大众化的JavaScript音频播放器库 Web https://github.com/scottschiller/SoundManager2 wave.js 用于简化Web音频的JavaScript库 Web https://github.com/jaz303/waveform-data Howler.js 用于现代Web音频的JavaScript音频库 Web https://github.com/goldfire/howler.js Web Audio DAW 用于在浏览器中创建数字音频工作站（DAW）的Web Audio Editor Web https://beryju.org/web 六、实践项目 AFPlayer项目 Android实现FFmpeg、OpenSL ES、OpenGL SE、MediaCodec等，实现简单的播放器，主要体现出相关知识点的使用。 OpenGLES基本使用 ：点、线、三角形、四边形、矩阵、纹理（贴纸）、摄像头显示、FBO、EGL、滤镜叠加、视频流编码输出。 直接阅读上方优秀开源库效果更佳... 七、最后 面试题整理：祝君拿到满意的offer！ 音视频学习资料 创作皆不易，有条件的朋友请支持原版，谢谢！ 密码:lqi9 网易视频 价值几千块的音视频视频 动脑视频 C++侯捷视频 音视频开发进阶指南：基于Android与iOS平台的实践.pdf 音视频05-H265码流分析.pdf 音视频04-H265之CU TU PU划分.pdf 音视频03-H265深度解析.pdf 音视频02-H265编码与H264区别.pdf 音视频01-H265编码.pdf 新一代视频压缩编码标准-H.264_AVC(第二版).pdf 数字信号处理教程（第四版）.pdf 视频技术手册(第5版).pdf 《FFmpeg从入门到精通》.pdf 《FFmpeg_Basics(260页)》.pdf webrtc介绍.pdf WebGL编程指南.zip video_file_format_spec_v10_1.pdf STL源码剖析简体中文完整版(清晰扫描带目录).pdf SDL2-API手册.doc rtmp规范翻译1.0.docx rtmp_specification_1.0.pdf rtmp.part3.Commands-Messages.pdf rtmp.part2.Message-Formats.pdf rtmp.part1.Chunk-Stream.pdf jni基础介绍.pdf OpenGL ES 2.0 编程指南 中文版.pdf OpenGL+着色语言第三版.pdf OpenGL.ES.2.0.Programming.Guide.pdf OpenGL着色语言.pdf ISO_IEC_14496-14_2003-11-15.pdf ISO_IEC-14496-3-2009.pdf hls-mpeg-ts-VB_WhitePaper_TransportStreamVSProgramStream_rd2.pdf hls-mpeg-ts-iso13818-1.pdf H.264官方中文版.pdf H.264_MPEG-4-Part-10-White-Paper.pdf H.264-AVC-ISO_IEC_14496-15.pdf H.264-AVC-ISO_IEC_14496-10.pdf ffmpeg命令大全.pdf FFmpeg命令大全.docx ffmpeg翻译文档.pdf FFMPEG_FFPLAY源码剖析.7z CMake中文手册.pdf Cmake在Android studio Ndk使用.pdf C++ Primer(第5版)中文版.pdf C++ Primer Plus（第6版）中文版.azw3 C Primer中文版 第五版 .pdf Android 音视频开发_何俊林.pdf amf3_spec_121207.pdf amf0_spec_121207.pdf Advanced C and C++ Compiling.pdf 【重点声明】此系列仅用于学习，禁止用于非法攻击，非法传播。一切遵守《网络安全法》。且如有发现商用，必纠法律责任。如有侵权请联系我（邮箱：xhunmon@126.com）进行删除。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-24 09:18:13 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/":{"url":"RTMP/","title":"直播推流全过程","keywords":"","body":"总纲总纲 本系列介绍了Android设备RTMP直播推流全过程 完整的项目地址 以下文章是针对每一个情况，介绍音视频相关知识，以及实现的原理，总共分五章： 第一章：视频数据源之YUV（1） RGB或YUV 组成一张画面，很多个的画面就可以组成一个视频，而在视频编解码领域中YUV则是这一切的基础。 第二章：音频数据源之PCM（2） 音频处理就是对声音特性采集成数字信号后进行处理，而PCM则是最原始采集到的数据，称“裸流”。 第三章：视频编码之H.264（3） 为了减少视频大小，以及改善网络传输，H.264编码在网络传输中可是非常重要。 第四章：音频编码之AAC（4） AAC编码是音频公认的主流编码。 第五章：直播推流编码之RTMP（5） 结合RTMP分块的特性，把数据较大的视频数据进行分块传输，这必定是直播界的宠儿！ 其他：H.264符号描述 其他：直播优化基础 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-28 17:09:25 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/1-yuv.html":{"url":"RTMP/1-yuv.html","title":"视频数据源之YUV","keywords":"","body":"视频数据源之YUV（1）视频基本概念YUV基本介绍YUV初步实战参考视频数据源之YUV（1） 视频基本概念 一个视频简单点理解就是播放一张张画面。我们就从这里面把视频的相关名词扯出来：①一张画面，一张画面也就一帧画面；属性为图像的大小或尺寸称分别率；画面的成像组成的方式有：rgb和yuv；跟计算器关联起来还不是用0101的比特来表示②当画面遇上了时间，爱的结晶就出来了：比特率、帧率和刷新率。接下来我们简单介绍一下他们的作用： 视频帧： 常见有I帧（关键帧，含完整画面，所以数据量大）、P帧（前向参考帧，参考前面I帧编码的图像信息）、B帧（双向预测帧，参考前面I帧、前面P帧和后面I帧编码的图像信息）；我们网上看视频时常常会遇到拖动进度条出现回退一两秒的情况吧？因为那个位置的当前帧不是I帧，没有完整的画面。 分辨率： 图像的大小或尺寸。 RGB： 任何彩色图像可由红绿蓝组成。RGB每一个通道占8位，1个字节内存。每个像素包含一个RGB，占3个字节（如果加上透明度RGBA则占32位，总共4个字节）。如：1920 x 1080内存大小=1920 x 1080 x 3=5.9M。 YUV(YCbCr)： Y：亮度；UV：色度和饱和度；wiki YCbCr 介绍。目前大多数都是使用yuv格式来表示视频帧的裸流数据。具体详情请往下阅读。 比特率(码率)： 单位时间内播放媒体（包括视频和音频）的比特数量（bit的数量）。文件大小计算公式： 文件大小（b）= 码率（b/s）x 时间（s） 帧率(帧数)： 画面每秒传输帧数，单位：fps（frame per second）或者 “赫兹”（Hz）。对于人眼感官常用范围在15~75fps之间。 刷新率： 屏幕在每秒刷新（画面）的次数。单位：赫兹（Hz）。 YUV基本介绍 人类视觉系统（HVS）对亮度比彩色更敏感，所以把Y和UV单独抽出来，每一个像素点都有一个Y，4个Y和一组UV（UV数量不定）共同绘制4个像素点，而Y和UV的比例不一样就分了多种 取样格式。 YUV与RGB的相互转化 RGB转YUV： Y = 0.299R + 0.587G + 0.114B U = 0.564(B - Y) V = 0.713(R - Y) YUV转RGB： R = Y + 1.402V B = Y + 1.772U G = Y - 0.344U - 0.714V YUV取样格式 常见的取样格式有以下3种 4：4：4 每4个素位置都有4个YUV，内存计算：1920 x 1080 = 1920 x 1080 x 3=5.9M； 4：2：2 每4个Y像素具有2个U和2个V；内存计算：1920 x 1080 = 1920 x 1080 x (1 + 2/4 + 2/4)=3.9M； 4：2：0 每4个Y像素具有1个U和1个V，使用在视频领域中应用最广泛。内存计算：1920 x 1080 = 1920 x 1080 x (1 + 1/4 + 1/4)=2.8M； YUV数据排列格式 在这里介绍两种格式，一种是Android平台特有NV21（又称YUV420SP），另一种则是其他大部分平台同样的I420（又称YUV420P），这两个取样格式都是4:2:0，所以说他们两种的数据完全一样，只是放到内存里面的顺序变了。（我们要实现把从Android采样NV21数据转成I420然后推送到服务器。） NV21： ①先把Y数据全部排序完；②UV数据交替排序完； I420： ①先把Y数据全部排序完；②U数据排序完；③V数据排序完； YUV初步实战 手写分离YUV分量以及对其进行播放 请前往 视音频数据处理入门：RGB、YUV像素数据处理 进行学习。对“前人种树，后人乘凉”感触颇深，只是太可惜了！ YUV数据源的采集（Android端） //1.相机权限 //2.获取相机，有后置摄像头：Camera.CameraInfo.CAMERA_FACING_BACK和前置摄像头：Camera.CameraInfo.CAMERA_FACING_FRONT Camera camera = Camera.open(cameraId); //3、Parameters这里封装着当前摄像头所能提供的参数（真实宽高等） Camera.Parameters parameters = camera.getParameters(); //根据parameters.getSupportedPreviewSizes()提供的宽高尺寸挑选一个设置进去 parameters.setPreviewSize(width, height); //设置预览数据为nv21（注意：仅仅是预览的数据，通过onPreviewFrame回调的仍没有发生变化） parameters.setPreviewFormat(ImageFormat.NV21); //设置预览角度，通过WindowManager.getDefaultDisplay().getRotation()参数查看。（因为android手机厂商安装摄像头传感器方向不统一，所以数据可能是旋转过的，所以要回正） camera.setDisplayOrientation(degrees); //设置修改过的数据，使得生效 camera.setParameters(parameters); //4、设置数据监听，我们会在onPreviewFrame(byte[] data, Camera camera)处理回调的数据，这里的数据就是每一帧原始数据流。我们会先把数据按照角度回正（注意回正后的宽高可能是调换的），然后转成I420就行编码发送。 camera.setPreviewCallbackWithBuffer(this); //5、启动预览画面 camera.setPreviewDisplay(holder); camera.startPreview(); NV21数据旋转 比如NV21数据以及顺时针旋转90度后的对比： 实现顺时针和逆时针旋转90度的代码： /** *yuv_n21_rotation(\"assets/yuv_nv21_800x480_back.yuv\",800,480,90,\"output/out_nv21_480x800_back.yuv\"); * 从android摄像机获取到的nv21格式数据，进行旋转 */ int yuv_n21_rotation(const char *url_in, int width, int height, int rotation, const char *url_out) { FILE *pIn = fopen(url_in, \"rb+\"); FILE *pOut = fopen(url_out, \"wb+\"); int yuvSize = width * height * 3 / 2; unsigned char *simple = (unsigned char *) malloc(yuvSize); unsigned char *simpleOut = (unsigned char *) malloc(yuvSize); fread(simple, 1, yuvSize, pIn); //顺时针旋转90 if (rotation == 90) { //宽高取反，把竖变行 int k = 0; //宽高取反，把竖变行 //y数据 for (int w = 0; w = 0; h--) { simpleOut[k++] = simple[h * width + w]; } } //uv数据 height*width -> 3/2height*width for (int w = 0; w = 0; h--) { // *(simpleOut + k) = simple[width * height + h * width + w]; // u simpleOut[k++] = simple[width*height + width * h + w]; // v simpleOut[k++] = simple[width*height + width * h + w + 1]; } } } else if(rotation == -90){ //宽高取反，把竖变行 int k = 0; //宽高取反，把竖变行 //y数据 for (int w = width -1; w >= 0; w--) { for (int h = 0; h 3/2height*width for (int w = 0; w = 0; h--) { // *(simpleOut + k) = simple[width * height + h * width + w]; simpleOut[k++] = simple[width*height + width * h + w]; simpleOut[k++] = simple[width*height + width * h + w + 1]; } } } fwrite(simpleOut, 1, yuvSize, pOut); return 0; } NV21数据格式转I420数据格式 //nv21_to_i420(\"assets/yuv_nv21_800x480_back.yuv\",800,480,\"output/out_yuv_i420_800x480_back.yuv\"); int nv21_to_i420(const char *url_in, int width, int height, const char *url_out){ FILE *pIn = fopen(url_in, \"rb+\"); FILE *pOut = fopen(url_out, \"wb+\"); int ySize = width * height; int uvSize = ySize /2; int yuvSize = ySize * 3 / 2; unsigned char *simple = (unsigned char *) malloc(yuvSize); unsigned char *simpleOut = (unsigned char *) malloc(yuvSize); fread(simple,yuvSize,1,pIn); //y memcpy(simpleOut, simple, ySize); for (int i = 0; i 参考 wiki YCbCr 视音频数据处理入门：RGB、YUV像素数据处理 新一代视频压缩编码标准-H.264_AVC(第二版) （书） 音视频开发进阶指南：基于Android与iOS平台的实践（书） Android 音视频开发_何俊林（书） Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/2-pcm.html":{"url":"RTMP/2-pcm.html","title":"音频数据源之PCM","keywords":"","body":"音频数据源之PCM（2）声音与音频音频采集与关键名词PCM数据的基本使用Android终端音频采样介绍播放pcm原始数据参考音频数据源之PCM（2） 声音与音频 声音是波，成为声波，而声波的三要素是频率、振幅和波形。频率代表音阶的高低（女高音、男低音）单位赫兹（Hz），人耳能听到的声波范围：频率在20Hz~20kHz之间；振幅代表响度（音量）；波形代表音色。而我们音频处理就是对声波采集成数字信号后进行处理。 音频采集与关键名词 音频采集的过程主要是通过设备设置采样率、采样数，将音频信号采集为pcm（Pulse-code modulation，脉冲编码调制）编码的原始数据（无损压缩），然后编码压缩成mp3、aac等封装格式的数据。音频关键知识： 采样率： 一段音频数据中单位时间内（每秒）采样的个数。 位宽： 一次最大能传递数据的宽度，可以理解成放单个采集数据的内存。常有8位和16位，而8位：代表着每个采集点的数据都使用8位（1字节）来存储；16位：代表着每个采集点的数据都使用16位（2字节）来存储。 声道数： 扬声器的个数，单声道、双声道等。每一个声道都占一个位宽。 来一张图来描述一下： 一段时间内的数据大小如何计算？ 采样率 x (位宽 / 8) x 声道数 x 时间 = 数据大小（单位：字节） 比如 2分钟的CD（采样率为：44100，位宽：16，声道数：2）的数据大小：44100 x (16 / 8) x 2 x 120 = 20671.875 Byte 约为 20.18M。 PCM数据的基本使用 我们采集到的pcm原始数据要怎么玩？首先得知道怎么这些数据都代表啥意思，然后才能入手处理。 1、pcm数据时如何组成（存储）？ 举个例子，分别使用不同的方式存储一段采集数据 0x11 0x22 0x33 0x44 0x55 0x66 0x77 0x88 总共8个字节。 8位单声道： 按照数据采集时间顺序存储，即：0x11 0x22 0x33 0x44 0x55 0x66 0x77 0x88 8位双声道： L声道-R声道-L声道-R声道形式存储，即：0x11(L) 0x22(R) 0x33(L) 0x44(R) …… 16位单声道： 首先从 维基多媒体：pcm了解到位宽大于8位时，字节的排序方式是有差别的，描述如下： When more than one byte is used to represent a PCM sample, the byte order (big endian vs. little endian) must be known. Due to the widespread use of little-endian Intel CPUs, little-endian PCM tends to be the most common byte orientation. 当使用一个以上的字节表示PCM样本时，必须知道字节顺序（大端与小端）。由于低端字节Intel CPU的广泛使用，低端字节PCM往往是最常见的字节方向。 举个栗子：当位宽为16位（2字节）存储一个采集数据时，如：0x12ab，大端和小端分别是： big-endian: 0x12 0xab； little-endian: 0xab 0x12。 所以： big-endian存储方式：0x1122 0x3344 0x5566 0x7788； little-endian存储方式：0x2211 0x4433 0x6655 0x8877。 16位双声道： L声道-R声道-L声道-R声道形式存储： big-endian：0x1122(L) 0x3344(R) 0x5566(L) 0x7788(R) little-endian: 0x2211(L) 0x4433(R) 0x66550(L) 0x8877(R) 2、pcm原始数据可以怎么玩？ 将little-endian_2_44100_16.pcm采样数据进行切割，只保留后面5秒的数据 /** * 将little-endian_2_44100_16.pcm采样数据进行切割，只保留后面5秒的数据 * 1、该类型数据5秒有多长？ * 2、从哪里开始截取？ */ int cut5second(const char *url){ FILE *in = fopen(url, \"rb+\"); FILE *out = fopen(\"./output/spit5second.pcm\", \"wb+\"); long long data5Length = 44100 * (16/8) * 2 * 5; struct stat statbuf; stat(url,&statbuf); long long fileLength = statbuf.st_size; long long start = fileLength - data5Length; char *simple = (char *)malloc(data5Length); //把指针位置移动到start位置开始读取 fseek(in,start,1); //每次从in文件中读取1组data5Length个长度数据的到simple中 fread(simple,data5Length,1,in); fwrite(simple,data5Length,1,out); fclose(in); fclose(out); return 0; } 分离各声道的数据：把各个声道的采集点数据分开存储。 /** * 将little-endian_2_44100_16.pcm 分离各声道的数据，即把各个声道的采集点数据分开存储。 */ int separateLR(const char *url){ FILE *in = fopen(url, \"rb+\"); FILE *outL = fopen(\"./output/l.pcm\", \"wb+\"); FILE *outR = fopen(\"./output/r.pcm\", \"wb+\"); int simpleLength = 16 / 8 * 2; char *simple = (char *)malloc(simpleLength); while (1){ //每次从in文件中读取1组4个长度数据的到simple中 fread(simple,4,1,in); if(feof(in)){ break; } //l(0声道)：1-2 fwrite(simple,2,1,outL); //r(1声道)：3-4 fwrite(simple+2,2,1,outR); } fclose(in); fclose(outL); fclose(outR); return 0; } 音量调节：把每个采集点数据的值 x 调节比例。注意：需要注意的是可调节范围，如：8位有无符号时最大是多少。 /** * 将little-endian_2_44100_16.pcm 调节音量 比例 * 1、little-endian排序的值是如何排序的？真正的值是多少？ * 2、little-endian转成真正的值之后再进行计算，得到的结果再反转little-endian。 * 如：原始pcm数据：0xaa 0x01(左声道采样点数据)，当scale=2： * -> 值：0x01aa * 2 = 0x0354 * -> 转回little-endian再进行存储：0x5403（缩放后的值） */ int volumeAdjustment(const char *url, float scale){ FILE *in = fopen(url, \"rb+\"); FILE *out = fopen(\"./output/volume_adjustment.pcm\", \"wb+\"); char *simple = (char *)malloc(4); while (1){ //每次从in文件中读取1组4个长度数据的到simple中 fread(simple, 4, 1, in); if(feof(in)){ break; } short *simple8bitTemp = (short *)malloc(2); //l(0声道)： simple8bitTemp[0] = (simple[0] + (simple[1] > 8; simple[2] = simple8bitTemp[1] & 0x00FF; simple[3] = simple8bitTemp[1] >> 8; for(int i=0; i 播放速度：按照比例丢弃（或插入0）采集点的数据即可。（涉及到不是整数倍不单单是这么处理，我也不懂） 参照雷神的必看项目： 视音频数据处理入门：PCM音频采样数据处理 。 Android终端音频采样介绍 1、关于采集的主要api介绍 /** * @param audioSource 音频来源{@link MediaRecorder.AudioSource}；如指定麦克风：MediaRecorder.AudioSource.MIC * @param sampleRateInHz 采样率{@link AudioFormat#SAMPLE_RATE_UNSPECIFIED}，单位Hz；安卓支持所有的设备是：44100Hz * @param channelConfig 声道数{@link AudioFormat#CHANNEL_IN_MONO}； * @param audioFormat 位宽{@link AudioFormat#ENCODING_PCM_8BIT} * @param bufferSizeInBytes 采集期间缓存区的大小 */ public AudioRecord(int audioSource, int sampleRateInHz, int channelConfig, int audioFormat, int bufferSizeInBytes) //获取最小缓存区，参数跟AudioRecord保持一致 int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat) {} 2、实现采集的伪代码 //1、申请权限 //2、获取最小缓存大小（根据api介绍，应该取要比预期大的缓冲区大小），这个大小其实也可以取①和②计算得来的大小 int minBufferSize = AudioRecord.getMinBufferSize(44100, AudioFormat.CHANNEL_IN_STEREO, AudioFormat.ENCODING_PCM_16BIT) * 2; //3、初始化AudioRecord对象 AudioRecord audioRecord = new AudioRecord(MediaRecorder.AudioSource.MIC, 44100, AudioFormat.CHANNEL_IN_STEREO, AudioFormat.ENCODING_PCM_16BIT, minBufferSize); //4、开始在子线程中进行采集数据 new Thread(new Runnable() { @Override public void run() { audioRecord.startRecording(); while(isRunning){ byte[] bytes = new byte[minBufferSize]; int len = audioRecord.read(bytes, 0, bytes.length); //这里就可以把数据直接写入到sdcard了，如：xxx.pcm；输出排序方式为：little endian。 } //5、停止录音机 audioRecord.stop(); } }).start(); //6、最后释放资源 audioRecord.release(); 播放pcm原始数据 ffmpeg： ffplay -f s16le -sample_rate 44100 -channels 2 -i xxx.pcm 其他： Adobe Audition 参考 维基多媒体：pcm 视音频数据处理入门：PCM音频采样数据处理 Android 音视频开发_何俊林（书） Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/3-h264.html":{"url":"RTMP/3-h264.html","title":"视频编码之H.264","keywords":"","body":"视频编码之H.264（3）简单说说编码H.264编码SPS（序列参数集）PPS（图像参数集）其他常见名称缩写X264参考视频编码之H.264（3） 简单说说编码 当我们把摄像头采集画面直接写入到文件中时，我们会发现没一会文件已经非常大了。这导致很不适合保存和传输，所以需要编码，把画面数据进行压缩。视频编码标准有很多，而我们这里讲的是H.264编码。其他请看：视频编码标准汇总及比较 。 H.264编码 制订H.264的主要目标有两个： （1）视频编码层(VCL，全称：Video Coding Layer)：得到高的视频压缩比。 （2）网络提取层(NAL，全称：Network Abstraction Layer)：具有良好的网络亲和性，即可适用于各种传输网络。而NAL则是以NALU（NAL Unit）为单元来支持编码数据在基于包交换技术网络中传输的。 H.264编码器 上面是H264的编码器原理图，编码时进行帧内编码和帧间编码。用相机录像举例，当相机录像过程帧出现一帧帧画面，没有压缩之间都是可以单独作为一张完整的照片，经过H264编码器后会出现： 1.SPS、PPS：生成图片的一些参数信息，比如宽高等。一般是开启编码器后的第一次I帧前输出一次。 2.I帧：编码后第一个完整帧，包含一张图片的完整信息，但是也是压缩的。主要是进行帧内压缩，把一帧图片划分为很多宏块，如：16x16、8x16、8x8、4x4等等，记录宏块顶部和左侧像素点数据，以及预测的方向。 3.B帧：叫双向预测帧。编码器遇到下一帧画面与前面帧变化非常非常小（相似度在95%之内），比如在录像中的人在发呆，当遇到变化略微有点大的P帧才停止，这会出现多张B帧，为了尽可能的存储更少的信息，将参考前面的I和后面的P帧，把中间变化的数据存储下来，这样编码记录运动矢量和残差数据即可。所以编码器当遇到这些帧时，先等待P帧编码结束后才进行编码B帧，因此输出顺序在P帧之后。 4.P帧：有了I帧作为基础，P就有参考的对象。跟前面I帧的变化非常少（相似度70%之内）。 5.GOF：按上面说录像人在发呆场景时，将一组数据相差较小的图片进行编码，这就有了GOF，即：group of picture，即一组图片，也叫一个场景的一组图片。是从一个I帧开始到下一个I帧前的一组数据。接着编码就是这样的一个个GOF的轮回。 如果不追求其中的细节，把他看成一个\"黑盒\"的话，编码成了： 解码成了： 编解码器工作具体细节我们可以不用深究，但是输入和输出的结果一定要知道，因为我们根据这个可以做很多事情，比如： 直播是过多的输出B帧会有什么影响？会出现延迟！！！ 编码的输入与输出 一张张画面通过以H.264编码标准的编码器(如x264)编码后，输出一段包含N个NALU的数据，每个NALU之间通过起始码来分隔，如图： 起始码： 0x00 00 01 或者 0x00 00 00 01。 在网络传输（如RTMP）或者一些容器中（如FLV），通常会把NALU整合到视频区域的数据中。如下图的flv格式： 所以这篇文章主要学习NALU的基本知识，学会如何去分析一段NALU数据。 NALU（NAL 单元） NALU(NAL Unit，NAL 单元)的组成部分如下图。其中，f(0)占1bit，u(2)占2bit，u(5)占5bit，文中如有出现类似描述符请看H.264描述符 。 forbidden_zero_bit：禁止位，初始为0。当客户端接受到该位为1时，则会丢弃该NAL单元的解码，表示异常数据。 nal_ref_idc：优先级，越大优先级越高。比如：I帧优先级大于B帧，DPS芯片会优先解码重要性高的。 从上图可以看出来，当前NAL单元属于什么样的类型，这取决于RBSP具体是什么样的类型，而RBSP的类型是根据nal_unit_type的值来定义的。 ①当nal_unit_type为1~5时：RBSP为切片类型（有5种切片类型）；整个NAL单元类型为VCL NAL单元，VCL是上面说的视频编码层，里面有编码后画面数据。 ②当nal_unit_type为其他时：RBSP为序列参数集类型、图像参数集类型等等；整个NAL单元类型为非VCL NAL单元。 具体的nal_unit_type所对应的RBSP类型如下表所示： nal_unit_type NAL 单元和 RBSP 语法结构的内容 0 未指定 1 一个非IDR图像的编码条带slice_layer_without_partitioning_rbsp( ) 2 编码条带数据分割块Aslice_data_partition_a_layer_rbsp( ) 3 编码条带数据分割块Bslice_data_partition_b_layer_rbsp( ) 4 编码条带数据分割块Cslice_data_partition_c_layer_rbsp( ) 5 IDR图像的编码条带slice_layer_without_partitioning_rbsp( ) 6 辅助增强信息 (SEI)sei_rbsp( ) 7 序列参数集（SPS）seq_parameter_set_rbsp( ) 8 图像参数集(PPS)pic_parameter_set_rbsp( ) 9 访问单元分隔符access_unit_delimiter_rbsp( ) 10 序列结尾end_of_seq_rbsp( ) 11 流结尾end_of_stream_rbsp( ) 12 填充数据filler_data_rbsp( ) 13 序列参数集扩展seq_parameter_set_extension_rbsp( ) 14..18 保留 19 未分割的辅助编码图像的编码条带slice_layer_without_partitioning_rbsp( ) 20..23 保留 24..31 未指定 SPS（序列参数集） SPS全称 Sequence parameter set(序列参数集)，当nal_unit_type=7时，RBSP就是SPS类型，也可以说NAL单元为SPS的NAL单元。SPS主要包含的是针对一连续编码视频序列的参数，如帧数、图像尺寸等；详见下表 序列参数集RBSP 语法： 上面中的主要参数的含义： profile_idc profile_idc 档次（H.264编码标准有几个档次，等级越高视频越清晰）： 值（十进制） 表示的意义 66 Baseline（直播） 77 Main（一般场景） 88 Extended 100 High (FRExt) 110 High 10 (FRExt) 122 High 4:2:2 (FRExt) 144 High 4:4:4 (FRExt) level_idc level_idc 最大支持码流范围： 标识当前码流的Level。编码的Level定义了某种条件下的最大视频分辨率、最大视频帧率等参数，码流所遵从的level由level_idc指定。 值(十进制) 表示最大支持每秒码流大小 10 1 (supports only QCIF format and below with 380160 samples/sec) 11 1.1 (CIF and below. 768000 samples/sec) 12 1.2 (CIF and below. 1536000 samples/sec) 13 1.3 (CIF and below. 3041280 samples/sec) 20 2 (CIF and below. 3041280 samples/sec) 21 2.1 (Supports HHR formats. Enables Interlace support. 5 068 800 samples/sec) 22 2.2 (Supports SD/4CIF formats. Enables Interlace support. 5184000 samples/sec) 30 3 (Supports SD/4CIF formats. Enables Interlace support. 10368000 samples/sec) 31 3.1 (Supports 720p HD format. Enables Interlace support. 27648000 samples/sec) 32 3.2 (Supports SXGA format. Enables Interlace support. 55296000 samples/sec) 40 4 (Supports 2Kx1K format. Enables Interlace support. 62914560 samples/sec) 41 4.1 (Supports 2Kx1K format. Enables Interlace support. 62914560 samples/sec) 42 4.2 (Supports 2Kx1K format. Frame coding only. 125829120 samples/sec) 50 5 (Supports 3672x1536 format. Frame coding only. 150994944 samples/sec) 51 5.1 (Supports 4096x2304 format. Frame coding only. 251658240 samples/sec) seq_parameter_set_id seq_parameter_set_id 标识符，本序列的id号，会被PPS引用。 chroma_format_idc chroma_format_idc 与亮度取样对应的色度取样 chroma_format_idc 的值应该在 0到 3的范围内（包括 0和 3）。当 chroma_format_idc不存在时，应推断其值为 1（4：2：0的色度格式）。 色度采样结构 chroma_format_idc 色彩格式 SubWidthC SubHeightC 0 单色 - - 1 4:2:0 2 2 2 4:2:2 2 1 3 4:4:4 1 1 num_ref_frames 规定了可能在视频序列中任何图像帧间预测的解码过程中用到的短期参考帧和长期参考 帧、互补参考场对以及不成对的参考场的最大数量。num_ref_frames 字段也决定了 8.2.5.3 节规定的滑动窗口操作 的大小。num_ref_frames 的取值范围应该在 0 到 MaxDpbSize （参见 A.3.1 或 A.3.2 节的定义）范围内，包括 0 和 MaxDpbSize 。 pic_width_in_mbs_minus1 pic_width_in_mbs_minus1 加1是指以宏块为单元的每个解码图像的宽度。 本句法元素加 1 后指明图像宽度，以宏块为单位：PicWidthInMbs = pic_width_in_mbs_minus1 + 1 通过这个句法元素解码器可以计算得到亮度分量以像素为单位的图像宽度： PicWidthInSamplesL = PicWidthInMbs * 16 从而也可以得到色度分量以像素为单位的图像宽度： PicWidthInSamplesC = PicWidthInMbs * 8 以上变量 PicWidthInSamplesL、 PicWidthInSamplesC 分别表示图像的亮度、色度分量以像素为单位的宽。 H.264 将图像的大小在序列参数集中定义，意味着可以在通信过程中随着序列参数集动态地改变图像的大小，甚至可以将传送的图像剪裁后输出。 frame_width = 16 × (pic_width_in_mbs_minus1 + 1); pic_height_in_map_units_minus1 pic_height_in_map_units_minus1 加1表示以条带组映射为单位的一个解码帧或场的高度。 本句法元素加 1 后指明图像高度： PicHeightInMapUnits = pic_height_in_map_units_minus1 + 1 PicSizeInMapUnits = PicWidthInMbs * PicHeightInMapUnits 图像的高度的计算要比宽度的计算复杂，因为一个图像可以是帧也可以是场，从这个句法元素可以在帧模式和场模式下分别计算出出亮度、色度的高。值得注意的是，这里以 map_unit 为单位， map_unit的含义由后文叙述。 PicHeightInMapUnits = pic_height_in_map_units_minus1 + 1; frame_height = 16 × (pic_height_in_map_units_minus1 + 1); 手撕SPS 下面为从一个只放h.264视频编码文件的一段（SPS）： ue(v)和se(v)的计算公式见 H.264描述符 。 # 00000001 6764001E ACD940A0 2FF96100 00030001 00000300 320F162D 96 00000001 #起始码 #NAL单元头---0x67 0110 0111 -------------- 0... .... # forbidden_zero_bit -->u(1) .11. .... # nal_ref_idc -->u(2) -->HIGHEST ...0 0111 # nal_unit_type -->u(5) -->SPS 64 # profile_idc=103 -->u(8) #---0x00 0000 0000 -------------- 0... .... #constraint_set0_flag .0.. .... #constraint_set1_flag ..0. .... #constraint_set2_flag ...0 .... #constraint_set3_flag .... 0000 #reserved_zero_4bits 1E # level_idc -->u(8) --> 30 #-----------0xAC 1010 1100 -------------- 1... .... # seq_parameter_set_id --> ue(v) --> 0 .010 .... # log2_max_frame_num_minus4 --> ue(v) --> 1 .... 1... # pic_order_cnt_type --> ue(v) -->1 执行else if( pic_order_cnt_type == 1 ) .... .1..#delta_pic_order_always_zero_flag -->u(1) #----------0xACD9 (1010 11)00 1101 1001 ====== 括号里面的bit上面已使用 .... ..00 110. .... #offset_for_non_ref_pic -->se(v)->codeNum=5->value=3 .... .... ...1 .... #offset_for_top_to_bottom_field -->se(v)->codeNum=0->value=0 .... .... .... 1... #num_ref_frames_in_pic_order_cnt_cycle -->ue(v)->0 #----------0xD940 (1101 1)001 0100 0000 ====== 括号里面的bit上面已使用 .... .001 01.. .... #num_ref_frames -->ue(v)->4 .... .... ..0. .... #gaps_in_frame_num_value_allowed_flag -->u(1)->0 #----------0x40A0 (010)0 0000 1010 0000 ====== 括号里面的bit上面已使用 ...0 0000 1010 00.. #pic_width_in_mbs_minus1 -->ue(v)->32-1+8=39 --> 视频宽 = (宏块 + 1) X 16 = (39 + 1)X16 = 640 #----------0xA02F (1010 00)00 0010 1111 ====== 括号里面的bit上面已使用 .... ..00 0010 111. #pic_height_in_map_units_minus1 -->ue(v)->16-1+7=22--> 视频高 = (宏块 + 1) X 16 = (22 + 1)X16 = 368 …………就到这里了，偷个懒，有兴趣大家自己分析下去，哈哈 PPS（图像参数集） PPS全称picture parameter set(图像参数集)，当nal_unit_type=8时，RBSP就是PPS类型，也可以说NAL单元为SPS的NAL单元。一个序列中某一幅图像或者某几幅图像，其参数如标识符pic_parameter_set_id、可选的seq_parameter_set_id、熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系数调整标识等；详见下表 图像参数集RBSP 语法： 其他常见名称缩写 名词缩写 中文含义 CABAC 基于上下文的自适应二进制算术编码 CAVLC 基于上下文的自适应变长编码 CBR 恒定比特率 CPB 编码图像缓存区 DPB 解码图像缓存区 DUT 被测解码器 FIFO 先进先出 HRD 假想参考解码器 HSS 假想码流调度器 IDR 即时解码刷新（I帧） LSB 最低有效位 MB 宏块 MBAFF 宏块自适应帧－场编码 MSB 最高有效位 NAL 网络抽象层 RBSP 原始字节序列载荷 SEI 补充的增强信息 SODB 数据比特串 UUID 通用唯一性标识符 VBR 可变比特率 VCL 视频编码层 VLC 变长编码 X264 这是国际好评的H.264协议标准的编码工具，这里简单介绍一下如何使用。 （1）下载：https://www.videolan.org/developers/x264.html （2）编译（android的交叉编译，平台：Mac） #!/bin/sh ##########脚本忘记是参考哪位大神的了########## #ndk的路径 NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c API=17 #最低支持Android版本 #编译平台darwin-x86_64为mac，linux-x86_64为linux HOST_PLATFORM=darwin-x86_64 function build_x264 { OUTPUT=$(pwd)/\"android\"/\"$CPU\" ./configure \\ --prefix=$OUTPUT \\ --cross-prefix=$CROSS_PREFIX \\ --sysroot=$SYSROOT \\ --host=$HOST \\ --disable-asm \\ --disable-shared \\ --enable-static \\ --disable-opencl \\ --enable-pic \\ --disable-cli \\ --extra-cflags=\"$EXTRA_CFLAGS\" \\ --extra-ldflags=\"$EXTRA_LDFLAGS\" make clean make install echo \"编译结束，路径如下：\" echo \"$OUTPUT\" } CPU=\"armeabi-v7a\" CROSS_PREFIX=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi- SYSROOT=$NDK/platforms/android-$API/arch-arm/ EXTRA_CFLAGS=\"-D__ANDROID_API__=$API -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" EXTRA_LDFLAGS=\"-marm\" HOST=arm-linux build_x264 （3）把编译生成的静态库移入android studio 在CMakeList.txt文件中添加： ... include_directories(include) ... target_link_libraries( ... x264 ) （4）API简单使用 /** 关键步骤，来自雷神：https://blog.csdn.net/leixiaohua1020/article/details/42078645 x264_param_default()：设置参数集结构体x264_param_t的缺省值。 x264_picture_alloc()：为图像结构体x264_picture_t分配内存。 x264_encoder_open()：打开编码器。 x264_encoder_encode()：编码一帧图像。 x264_encoder_close()：关闭编码器。 x264_picture_clean()：释放x264_picture_alloc()申请的资源。 存储数据的结构体如下所示。 x264_picture_t：存储压缩编码前的像素数据。 x264_nal_t：存储压缩编码后的码流数据。 */ int X264Rtmp::encode(const char *url, int width, int height, int bitRate, int fps) { //计算一帧等信息 int ySize = width * height; int uvSize = ySize / 4; FILE *fp_src = fopen(url, \"rb\"); //初始化VLC图片编码层的参数 x264_picture_t *pic_in = (x264_picture_t *) malloc(sizeof(x264_picture_t)); x264_t *videoCodec = 0; x264_param_t param; x264_param_default(&param); //根据应用场景设置编码速度，以及编码质量。2：x264_preset_names，3：x264_tune_names x264_param_default_preset(&param, x264_preset_names[0], x264_tune_names[7]); //输入数据格式， yuv 4:2:0 param.i_csp = X264_CSP_I420; param.i_width = width; param.i_height = height; //base_line 3.2 编码规格，影响网络带宽，图像分辨率等。 -- https://en.wikipedia.org/wiki/Advanced_Video_Coding param.i_level_idc = 32; //两张参考图片间b帧的数量 param.i_bframe = 0; //参数i_rc_method表示码率控制，CQP(恒定质量)，CRF(恒定码率)，ABR(平均码率) param.rc.i_rc_method = X264_RC_ABR; //比特率(码率, 单位Kbps) param.rc.i_bitrate = bitRate / 1000; //瞬时最大码率 param.rc.i_vbv_max_bitrate = bitRate / 1000 * 1.2; //设置了i_vbv_max_bitrate必须设置此参数，码率控制区大小,单位kbps param.rc.i_vbv_buffer_size = bitRate / 1000; //帧率（每秒显示多少张画面） param.i_fps_num = fps; param.i_fps_den = 1; param.i_timebase_den = param.i_fps_num; param.i_timebase_num = param.i_fps_den; // param.pf_log = x264_log_default2; //用fps而不是时间戳来计算帧间距离 param.b_vfr_input = 0; //帧距离(关键帧) 2s一个关键帧 param.i_keyint_max = fps * 2; // 是否复制sps和pps放在每个关键帧的前面 该参数设置是让每个关键帧(I帧)都附带sps/pps。 param.b_repeat_headers = 1; //多线程 param.i_threads = 1; x264_param_apply_profile(&param, \"baseline\"); //打开编码器 videoCodec = x264_encoder_open(&param); x264_picture_alloc(pic_in, X264_CSP_I420, width, height); //编码：h264码流 while (!feof(fp_src)) { //y数据 fread(pic_in->img.plane[0], ySize, 1, fp_src); //Y fread(pic_in->img.plane[1], uvSize, 1, fp_src); //U fread(pic_in->img.plane[2], uvSize, 1, fp_src); //V //编码出来的数据 （帧数据） x264_nal_t *pp_nal; //编码出来有几个数据 （多少帧） int pi_nal; x264_picture_t pic_out; x264_encoder_encode(videoCodec, &pp_nal, &pi_nal, pic_in, &pic_out); //如果是关键帧 3 int sps_len; int pps_len; uint8_t sps[100]; uint8_t pps[100]; // chroma_format_idc for (int i = 0; i 参考 视频编码标准汇总及比较 H.264-AVC-ISO_IEC_14496-10 新一代视频压缩编码标准-H.264_AVC(第二版) H.264官方中文版.pdf https://stackoverflow.com/questions/28421375/usage-of-start-code-for-h264-video/29103276 https://blog.csdn.net/engineer_james/article/details/81750864 x264流程 x264参数注释 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/4-aac.html":{"url":"RTMP/4-aac.html","title":"音频编码之AAC","keywords":"","body":"音频编码之AAC（4）回顾简述AAC的音频文件格式faac开源库参考音频编码之AAC（4） 回顾 还记得我们采集到的PCM原始数据流（俗称裸流）吗？由于PCM裸流过大，不便于储存与传输，于是就出现了针对于PCM裸流的压缩编码标准，包含AAC，MP3，AC-3 等等（wiki audio file format）；而AAC则是当前的主流。这里的AAC指的是一套编码标准（协议），而faac是一个开源的AAC编解码工具。 简述 AAC：高级音频编码(Advanced Audio Coding)，基于MPEG-2的音频编码技术，目的是取代MP3格式。2000年，MPEG-4标准出现后，AAC重新集成了其特性，为了区别于传统的MPEG-2 AAC又称为MPEG-4 AAC。 AAC的音频文件格式 AAC的音频文件格式有两种ADIF和ADTS。这两种格式主要区别：ADIF只有一个文件头，ADTS每个包前面有一个文件头。而我们重点讲解的是ADTS格式。 ADIF Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不需进行在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。故这种格式常用在磁盘文件中。编码格式如下： 在MPEG-2 AAC中ADIF语法规则如下： ADTS Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始。也就是说ADTS的每一帧都有一个header和aac音频数据，这可以在网络传输的时候进行实时解码。 下图为ADTS的组成部分以及在MPEG-2 AAC的语法结构： 下图为ADTS的组成部分以及在MPEG-4 AAC的语法结构： 注：ES：全称elementary stream，这里意为编码后的音频数据。 adts_fixed_header关键参数如下： syncword 恒为 '1111 1111 1111'，也就是0xFFF。作为每个adts_freme的分割。 ID 使用那个MPEG版本。0：MPEG-4，1：MPEG-2。 layer 应该恒 为‘00’。 protection_absent 是否使用error_check()。0：使用，1：不使用。 profile(MPEG-4:profile_ObjectType) 见下表（左边是MPEG-2版本；右边是MPEG-4版本，Profile_ObjectType的值）： sampling_frequency_index 采样率的数组下标，即：sampling frequeny[sampling_frequency_index] ： private_bit 私有位，编码时设置为0，解码时忽略。 channel_configuration 声道数。 original_copy 编码时设置为0，解码时忽略。 home 编码时设置为0，解码时忽略。 adts_variable_header关键参数如下： copyright_identification_bit 72位版权标识字段中的一位。 copyright_identification_start 一位表示 该音频帧中的copyright_identification_bit是 72位版权标识的第一位。如果不 版权标识已传输，此位应 保持为'0'。'0'在 此音频帧“ 1”开始在 此音频帧。 frame_length(MPEG-4:aac_frame_length) 帧的长度，包括header和以字节为单位的error_check。 adts_buffer_fullness 固定0x7FF，表示比特流是可变速率比特流。 number_of_raw_data_blocks_in_frame 在ADTS帧中有number_of_raw_data_blocks_in_frame + 1个AAC原始数据块。number_of_raw_data_blocks_in_frame == 0 表示说ADTS帧中有一个AAC原始数据块。 下图为一个ADTS格式的文件开头部分 我们来数一个第一个frame的header ####adts_fixed_header() #-------FFF1--------- 1111 1111 1111 .... #0xFFF; syncword的值 .... .... .... 0... #ID=0;使用MPEG-4 .... .... .... .00. #layer .... .... .... ...0 #protection_absent，不使用error_check() #-------4C80--------- 01.. .... .... .... #Profile_ObjectType=AAC MAIN ..00 11.. .... .... #sampling_frequency_index=3,采样率为48000 .... ..0. .... .... #private_bit .... ...0 10.. .... #channel_configuration=2（声道数） .... .... ..0. .... #original_copy .... .... ...0 .... #home ####adts_variable_header() #-------(4C8)0--------- .... .... .... 0... #copyright_identification_bit .... .... .... .0.. #copyright_identification_start #-------(4C8)0 223F-------- ..00 0010 0010 001. .... #aac_frame_length=0x111=273字节-->下一帧到上图的FFF14C #-------(22)3F FC-------- ...1 1111 1111 11.. #adts_buffer_fullness=0x7FF #-------(F)C-------- ..00 #number_of_raw_data_blocks_in_frame faac开源库 （1）下载 https://nchc.dl.sourceforge.net/project/faac/faac-src/faac-1.29/faac-1.29.9.2.tar.gz （2）编译生成静态库（这里是android的交叉编译脚本，ndk21，平台mac） #!/bin/bash PREFIX=`pwd`/android/armeabi-v7a NDK_ROOT=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 # 注意：Mac为darwin-x86_64，linux为linux-x86_64；一定要确保路径真实有效 TOOLCHAIN=$NDK_ROOT/toolchains/llvm/prebuilt/darwin-x86_64 CROSS_COMPILE=$TOOLCHAIN/bin/arm-linux-androideabi #在android studio中新建一个NDK项目，并且保持NDK版本与这里的一致。该FLAGS从build.ninja文件中拷贝。 FLAGS=\"-g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -march=armv7-a -mthumb -Wformat -Werror=format-security -Oz -DNDEBUG -fPIC\" export CC=$TOOLCHAIN/bin/armv7a-linux-androideabi21-clang export CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi21-clang++ export CFLAGS=\"$FLAGS\" export PATH=$PATH:$TOOLCHAIN/bin ./configure \\ --prefix=$PREFIX \\ --host=arm-linux-androideabi \\ --with-pic \\ --enable-shared=no make clean make install （3）移入项目中 并且配置CMakeList.txt文件 ... include_directories(include) ... target_link_libraries( ... faac ) （4）API简单使用 //1.打开编码器，获取inputSamples和maxOutputBytes的值，用于后面编码 //1：采样率；2：声道数；3：单次输入的样本数；4：输出数据最大字节数 faacEncOpen(unsigned long sampleRate,unsigned int numChannels,unsigned long *inputSamples,unsigned long *maxOutputBytes); //2.设置编码器参数 faacEncConfigurationPtr config = faacEncGetCurrentConfiguration(faacEncHandle hEncoder); //指定mpeg4编码标准 config->mpegVersion = MPEG4; //config->mpegVersion = MPEG2; //lc 标准 config->aacObjectType = LOW; //16位 config->inputFormat = FAAC_INPUT_16BIT; // 编码出原始数据；0 = Raw; 1 = ADTS config->outputFormat = 1; faacEncSetConfiguration(faacEncHandle hEncoder, config); //3.进行编码 //1：FAAC的handle；2：采集的pcm的原始数据；3：从faacEncOpen获取的inputSamples；4：至少有从faacEncOpen获取maxOutputBytes大小的缓冲区；5：从faacEncOpen获取maxOutputBytes //返回值为编码后数据字节的长度 int encodeLenght = faacEncEncode(faacEncHandle hEncoder, int32_t * inputBuffer, unsigned int samplesInput, unsigned char *outputBuffer, unsigned int bufferSize); 参考 AAC格式简介 wiki audio file format https://csclub.uwaterloo.ca/~ehashman/ISO14496-3-2009.pdf Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/5-rtmp.html":{"url":"RTMP/5-rtmp.html","title":"直播推流编码之RTMP","keywords":"","body":"直播推流编码之RTMP（5）简述Handshake Diagram（握手流程）分块RTMPDump参考直播推流编码之RTMP（5） 简述 Adobe 公司的实时消息传输协议 (RTMP) 通过一个可靠地流传输提供了一个双向多通道消息服务，意图在通信端之间传递带有时间信息的视频、音频和数据消息流。 Handshake Diagram（握手流程） Uninitialized (未初始化)： 客户端发送C0包(1 字节，版本信息)，如果服务器支持这个版本会响应S0和S1，否则终止连接。 Version Sent (版本已发送)： 当服务器接收到版本号后(已发送S0和S1)，客户端等S1，服务器等C1，当都接收后，客户端发送C2，服务器发送S2，然后两者状态变成Ack Sent。 Ack Sent (确认已发送)： 客户端和服务器分别等待 S2 和 C2。 Handshake Done (握手结束)： 客户端和服务器可以开始交换消息了。 分块 网络传输过程中，每一个块（每个rtmp包）必须被完全发送才可以发送下一块，而在接收端，这些块被根据 chunk stream ID 被组装成消息。分块允许上层协议将大的消息分解为更小的消息，例如，防止体积大的但优先级小的消息 (比如视频) 阻碍体积较小但优先级高的消息 (比如音频或者控制命令)。分块也让我们能够使用较小开销发送小消息，因为块头包含包含在消息内部的信息压缩提示。下面块格式就是一个块的组成。（注意：当连续接收到chunk stream ID 相同时，这些快是同一个消息，需要合并。） Chunk Format（块格式） 下图大致的概括了一块的组成，从当前块的 第一个字节 大致能分析出该块头的组成信息。 Extended Timestamp 和 Chunk Data具体计算在下面介绍。 Basic Header（块基本头） cs id保留0和1的值，而2的值保留用于下层协议控制消息和命令；具体如下： 第一个字节低6bit值>1： Basic Header 为1byte；fmt=2bit；cs id =6bit，范围：2-63（也就是6bit最大能支持的范围）。 fmt cs id 高2bit 低6bit 如：0100 1010->fmt：01.. ....=2；cs id：..00 1010=10 第一个字节低6bit值=0： Basic Header 为2byte；fmt=2bit；此时第一个字节中后6bit的值为0，cs id=第2个byte，范围：64 ~ 319（也就是第2个byte的值 + 64）。 fmt 0 cs id - 64 高2bit 低6bit byte 如：1000 0000 0001 0001->fmt：10.. ....=2；0：..00 0000；cs id：(0001 0001)+64=17+64=81 第一个字节低6bit值=1： Basic Header 为3byte；fmt=2bit；此时第一个字节中后6bit的值为0，cs id=第2个byte，范围：64~65599（(第3个byte) * 256 + (第2个byte) + 64）。 fmt 1 cs id - 64 高2bit 低6bit 2byte 如：1100 0000 0001 0001 0010 0010->fmt：11.. ....=2；1：..00 0001；cs id：(0010 0010)x256 + (0001 0001)+64=8704 + 17 + 64=8785 Message Header（块消息头） 根据块基本头中的 fmt 的值来区分块消息头，从0—3共4种，上图标的很明确了，具体如下： fmt = 0： 块消息头为11字节，当前消息的 timestamp 在这表示(此时 Extended Timestamp 辅助用)，如果用户设置时间戳>=0xFFFFFF时(3字节容不下了)时，timestamp 字段就固定为0xFFFFFF。message length 是指 Chunk Data 的大小。Extended Timestamp 字段用4字节表示； timestamp message length message type id message stream id 3byte 3byte 1byte 4byte fmt = 1： 块消息头为7字节，少了 message stream id ，这一块使用前一块一样的流 ID。 timestamp delta message length message type id 3byte 3byte 1byte fmt = 2： 块消息头为3字节，只有 timestamp delta ，计算方式同 fmt = 0 时一样处理；流 ID与 fmt = 1 相同。 fmt = 3： 无块消息头；流 ID与 fmt = 1 相同；当一个消息被切割成多块时，除第一块外，其他都应 fmt = 3。 Extended Timestamp（扩展时间戳） 当块消息头中的 timestamp 或者 timestamp delta 字段(3字节)容不下时(fmt = 0，1或2)，Extended Timestamp 才会被使用。 RTMPDump tmpdump 是一个用来处理 RTMP 流媒体的工具包，支持 rtmp://, rtmpt://, rtmpe://, rtmpte://, and rtmps:// 等。源码详细api以及流程图请看雷神的RTMPdump 源代码分析 1： main()函数 ，这里只是简单介绍，集成到android中使用。因为源码很少，所以直接在as中进行编译生成静态库。 （1）下载：http://rtmpdump.mplayerhq.hu/download/rtmpdump-2.3.tgz （2）把源码导入到as中，如下图所示： librtmp/CMakeLists.txt文件配置： cmake_minimum_required(VERSION 3.4.1) #预编译宏 set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DNO_CRYPTO\" ) #所有源文件放入 rtmp_source 变量 file(GLOB rtmp_source *.c) #编译静态库 add_library(rtmp STATIC ${rtmp_source} ) 项目的CMakeLists.txt文件引用编译生成的静态库 ... # 引入指定目录下的CMakeLists.txt add_subdirectory(${CMAKE_SOURCE_DIR}/librtmp) ... #4、链接到库文件，jni/c/c++可以引入链接到 target_link_libraries( rtmp ...) 参考 （1）rtmp_specification_1.0 （2）https://www.cnblogs.com/Kingfans/p/7083100.html （3）https://blog.csdn.net/commshare/article/details/103393461 （4）https://blog.csdn.net/leixiaohua1020/article/details/12952977 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/h264-descriptor.html":{"url":"RTMP/h264-descriptor.html","title":"其他：H.264符号描述","keywords":"","body":"H.264描述符简述浅谈哥伦布编码ue(v)se(v)me(v)te(v)其他参考H.264描述符 简述 H264/AVC文档中存在着大量元素描述符ue(v)，me(v)，se(v)，te(v)等，编码为ue(v)，me(v)，或者 se(v) 的语法元素是指数哥伦布编码，而编码为te(v)的语法元素是舍位指数哥伦布编码。这些语法元素的解析过程是由比特流当前位置比特开始读取，包括非0 比特，直至leading_bits 的数量为0。由于解码过程中需要计算出每个元素的占位(bit数量)，所以我们就不得不理解这些描述符了。 浅谈哥伦布编码 在说哥伦布编码之前首先先谈谈什么是定长编码？什么是可变长度编码？ 定长编码 即固定长度的编码，如：对汉字进行编码时，每个汉字都是2两个字节。 好处是只要读取该固定的字节长度就能取出值。 坏处是会浪费很多不必要的内存，比如用1个字节能存储的值，却必须要2个字节，浪费了一个字节。 可变长编码 长度不固定的编码，随内容大小而改变编码长度。 好处是有多少内容，就占用多少内存。 但是因为内容不固定，所以得在内存中是如何知道这些内容的长度呢？所以一般都是在内容前面加上内容的长度。 为何H264中采用哥伦布编码 在H264编码中，有大量的短数据（在255以内），如：宏块大小，各种标志位等。为了减少码流的长度，把这些短数据使用可变长度编码，而在众多可变长度编码的算法中，哥伦布编码尤其合适H264编码。因为数据的值小，在其二进制的值上+1再补长度个数的0即可，这样就可以把每个bit用到，不会产生多余的浪费。 而为何只是用于短数据呢？因为多少个0代码数据长度，也就是说数据大的，就会产生很多很多0，会造成更大的浪费。具体看下面原理。 ue(v) 无符号整数指数哥伦布码编码的语法元素，左位在先。计算公式： codeNum = 2leadingZeroBits2^{leadingZeroBits}2​leadingZeroBits​​ − 1 + read_bits( leadingZeroBits ) codeNum： 占的位数（bit数量） leadingZeroBits： 遇到第一个1前面0的个数；如：0010 1011，leadingZeroBits的值为2； read_bits( leadingZeroBits )： 遇到第一个1后面leadingZeroBits个组成的无符号二进制值；如：0010 1011，值为...0 1...，即01，即1； 举两个栗子： （1）0001 1001 =>leadingZeroBits== 000. ....= 3，read_bits( 3 )==.... 100.=4，所以：codeNum=232^32​3​​-1+4=13 （2）0000 0101 0000 0000 => leadingZeroBits=5，read_bits( 5 ) = .... ..01 000. ....=8，codeNum=252^52​5​​-1+8=39 se(v) 有符号整数指数哥伦布码编码的语法元素位在先。在按照上面ue(v)公式计算出codeNum后，然后使用该计算公式： value = (−1)k+1(-1)^{k+1}(−1)​k+1​​ Ceil( k÷2 ) Ceil： 返回大于或者等于指定表达式的最小整数，如： Ceil(1.5)= 2 如上例（1）0001 1001 => codeNum=232^32​3​​-1+4=13，value = (−1)13+1(-1)^{13+1}(−1)​13+1​​Ceil(13÷2)=Ceil(6.5)=7 me(v) 映射的指数哥伦布码编码的语法元素，左位在先。在按照上面ue(v)公式计算出codeNum后，然后查表。（在H.264-AVC-ISO_IEC_14496-10的9.1.2 章节中表9-4） te(v) 舍位指数哥伦布码编码语法元素，左位在先。明确取值范围在0-x；当x>1时，te(v)就是ue(v)；否则(x=1)，b = read_bits( 1 )，codeNum = !b 其他 ae(v)： 上下文自适应算术熵编码语法元素 b(8)： 任意形式的8比特字节。 f(n)： n位固定模式比特串（由左至右），左位在先。 i(n)： 使用n比特的有符号整数。 u(n)： n位无符号整数。 参考 (1).H.264-AVC-ISO_IEC_14496-10 (2).https://blog.csdn.net/lizhijian21/article/details/80982403 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"RTMP/6-optimize.html":{"url":"RTMP/6-optimize.html","title":"其他：直播优化基础","keywords":"","body":"直播优化基础简单介绍学习参考优化重要知识点直播优化基础 简单介绍 当前直播系统涉及到的范畴是非常多的，从主播推流到观众播放，涉及到编解码、推流拉流、CND网络、粉丝间互动、给主播刷礼物等等，这系统非常庞大，还涉及到人力物力的安排，框架的选择，跨平台的处理…… 我在这方面没有过多的经验，全是通过\"大牛\"们了解到的，所以这里只是给出学习参考。 学习参考 《音视频开发进阶指南：基于Android与iOS平台的实践》 \"第11章　直播应用的构建\"：介绍直播系统所涉及到的范畴；\"第12章　直播应用中的关键处理\"：推流和拉流过程中的优化有理方案。 《Android 音视频开发_何俊林》\"第9章 直播技术\"：涉及到真个直播流程的介绍、处理以及优化。 音视频播放过程中的问题解决(播放质量优化) ：直播过程异常情况及原因简介。 优化重要知识点 H.264协议：目前还是占主流位置，需要根据懂得协议才能设置优化编码解码参数。 硬编：1：加快编码速度；2：解放CPU。 OpenGL：使用GPU播放视频，解放CPU。 注：由于H.265所涉及到的专利非常多且收费不明确等原因，导致现在很多企业都没有使用。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/":{"url":"AFPlayer/","title":"AFPlayer","keywords":"","body":"总纲简述总纲 简述 AFPlayer是指在Android上使用FFmpeg做的播放器。Android 上使用FFmpeg、OpenSL ES、OpenGL SE、MediaCodec等，实现简单的播放器，主要体现出相关知识点的使用。 项目地址：AFPlayer 第一章：OpenSL ES播放PCM OpenSL ES全称 Open Sound Library for Embedded Systems，即嵌入式系统的开放音频库。是无授权费、跨平台、硬件加速的C语言音频API，用于2D和3D音频。 第二章：OpenGL ES播放RGB OpenGL（英语：Open Graphics Library，译名：开放图形库或者“开放式图形库”）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序编程接口（API）。而OpenGL ES （OpenGL for Embedded Systems）是三维图形应用程序接口OpenGL的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，如Android 。 第三章：FFmpeg实现MediaCodec解码 MediaCodec是Android平台的硬件编解码，FFmpeg从3.1版本开始支持，但是，目前支持实现解码功能。官网硬件编解码支持结束 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/01_opensl_es.html":{"url":"AFPlayer/01_opensl_es.html","title":"OpenSL ES播放PCM","keywords":"","body":"OpenSL ES播放PCM简述API基本介绍对象和接口重要接口播放流程代码实现编辑CMakeList.txt进行链接动态库关键流程参考OpenSL ES播放PCM 简述 OpenSL ES全称 Open Sound Library for Embedded Systems，即嵌入式系统的开放音频库。是无授权费、跨平台、硬件加速的C语言音频API，用于2D和3D音频。 API基本介绍 我们可以从Android官网中了解到一些基本的API介绍。 对象和接口 对象：SLObjectItf，理解成只有存储能力的JavaBean，没有调用其他函数（方法）的途径。所有对象的名称使用这个，而创建的对象都需要调用Realize进行初始化。 接口：SLEngineItf，接口是跟每一个SLObjectItf进行绑定，可以有多个，拥有调用其他函数（方法）的途径。获取的流程如下：流程图 +----------------------------+ | 创建SLObjectItf（实例化过程） | +----------------------------+ | V +----------------------------+ | Realize（初始化过程） | +----------------------------+ | V +-----------------------------+ | 获取GetInterface（绑定过程） | +-----------------------------+ 重要接口 SLEngineItf：OpenSL ES引擎接口，全局唯一。用于创建混音器对象，播放器对象等。 SLPlayItf：播放器接口。用于获取播放状态，设置播放状态等。 SLAndroidSimpleBufferQueueItf：数据队列接口。 播放流程 通过上图可知： 只有接口拥有调用其他函数的功能；即：SLEngineItf调用了CreateOutputMix和CreateAudioPlayer。 一个对象可以生成（绑定）多个接口；即：\"播放器对象\"生成了SLPlayItf和SLAndroidSimpleBufferQueueItf。 代码实现 有个比较关键的点是：OpenEL SE 不支持浮点类型的值，所以需要转换成整形再使用。本文数据来源的形式是通过读取本地Sdcard文件来完成的。如果是想通过网络或则asset目录，请查看Android官方项目：native-audio 。 编辑CMakeList.txt进行链接动态库 target_link_libraries( native-lib log android OpenSLES) 关键流程 void bqPlayerCallback(SLAndroidSimpleBufferQueueItf bq, void *ctx) { (static_cast(ctx))->ProcessSLCallback(bq); } AudioPlayer::AudioPlayer(const char *filename, uint32_t sampleRate, uint8_t channels, uint32_t bitPerChannel) { //一帧的大小 frameSize = sampleRate * channels * bitPerChannel / 8; inFile = fopen(filename, \"rb+\"); SLresult result; //1.1）创建引擎对象。引擎对象是OpenSL ES提供API的唯一入 result = slCreateEngine(&slEngineObj_, 0, nullptr, 0, NULL, NULL); SLASSERT(result);//断言，用于调试，能快速定位问题 //1.2）实例化引擎对象，需要通过在第1步得到的引擎对象接口来实例化(在ELSE中，任何对象都需要使用接口来进行实例化) result = (*slEngineObj_)->Realize(slEngineObj_, SL_BOOLEAN_FALSE); SLASSERT(result); //1.3）获取这个引擎对象的方法接口，通过GetInterface方法，使用第2步已经实例化好了的对象 result = (*slEngineObj_)->GetInterface(slEngineObj_, SL_IID_ENGINE, &slEngineItf_); SLASSERT(result); //2.创建混音器对象 result = (*slEngineItf_)->CreateOutputMix(slEngineItf_, &outputMixObjectItf_, 0, NULL, NULL); SLASSERT(result); result = (*outputMixObjectItf_)->Realize(outputMixObjectItf_, SL_BOOLEAN_FALSE); SLASSERT(result); //3、创建播放器 // numBuffers：设置2个缓冲数据 SLDataLocator_AndroidSimpleBufferQueue android_queue = {SL_DATALOCATOR_ANDROIDSIMPLEBUFFERQUEUE, 2}; SLDataFormat_PCM pcm = { SL_DATAFORMAT_PCM,//播放pcm格式的数据 channels,//声道数（立体声） sampleRate * 1000,//44100hz -> 44100000 的频率；参考：SL_SAMPLINGRATE_44_1 bitPerChannel == 32 ? SL_PCMSAMPLEFORMAT_FIXED_32 : SL_PCMSAMPLEFORMAT_FIXED_16,//位数 32位 bitPerChannel == 32 ? SL_PCMSAMPLEFORMAT_FIXED_32 : SL_PCMSAMPLEFORMAT_FIXED_16,//和位数一致就行 SL_SPEAKER_FRONT_LEFT | SL_SPEAKER_FRONT_RIGHT,//立体声（前左前右） SL_BYTEORDER_LITTLEENDIAN//小端排序 }; SLDataSource slDataSource = {&android_queue, &pcm}; SLDataLocator_OutputMix outputMix = {SL_DATALOCATOR_OUTPUTMIX, outputMixObjectItf_}; SLDataSink audioSnk = {&outputMix, NULL}; const SLInterfaceID ids[3] = {SL_IID_BUFFERQUEUE, SL_IID_EFFECTSEND, SL_IID_VOLUME}; const SLboolean req[3] = {SL_BOOLEAN_TRUE, SL_BOOLEAN_TRUE, SL_BOOLEAN_TRUE}; result = (*slEngineItf_)->CreateAudioPlayer(slEngineItf_, &playerObjectItf_, &slDataSource, &audioSnk, 3, ids, req); SLASSERT(result); result = (*playerObjectItf_)->Realize(playerObjectItf_, SL_BOOLEAN_FALSE); SLASSERT(result); result = (*playerObjectItf_)->GetInterface(playerObjectItf_, SL_IID_PLAY, &playItf_); SLASSERT(result); //4、获取播放器对象的数据队列接口 result = (*playerObjectItf_) ->GetInterface(playerObjectItf_, SL_IID_BUFFERQUEUE, &playBufferQueueItf_); SLASSERT(result); //5. 设置回调函数 result = (*playBufferQueueItf_) ->RegisterCallback(playBufferQueueItf_, bqPlayerCallback, this); SLASSERT(result); //6. 获取播放状态接口 result = (*playItf_)->SetPlayState(playItf_, SL_PLAYSTATE_PLAYING); SLASSERT(result); //7. 主动调用回调函数开始工作 ProcessSLCallback(playBufferQueueItf_); } void AudioPlayer::ProcessSLCallback(SLAndroidSimpleBufferQueueItf bq) { void *buffer; getPcmData(&buffer); if (NULL != buffer) { SLresult result; result = (*bq)->Enqueue(bq, buffer, frameSize); SLASSERT(result); } } void AudioPlayer::getPcmData(void **pcm) { outBuf = new uint32_t[frameSize]; while (!feof(inFile)) { //读取一帧数据 memset(outBuf, 0, frameSize); fread(outBuf, frameSize, 1, inFile); if (outBuf == nullptr) {//读取结束 break; } *pcm = outBuf; goto end; } __android_log_print(ANDROID_LOG_DEBUG, \"audio_play\", \"red finish?\"); (*playItf_)->SetPlayState(playItf_, SL_PLAYSTATE_STOPPED); end: __android_log_print(ANDROID_LOG_DEBUG, \"audio_play\", \"playing？\"); } Demo地址：OpenSLESDemo 参考 https://en.wikipedia.org/wiki/OpenSL_ES https://blog.csdn.net/ywl5320/article/details/78503768 https://github.com/android/ndk-samples/tree/main/audio-echo Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/02_opengl_es.html":{"url":"AFPlayer/02_opengl_es.html","title":"OpenGL ES播放RGB","keywords":"","body":"OpenGL ES播放RGB简述屏幕图像显示流程OpengGL相关处理OpenGL渲染管线着色器几个重要概念纹理OpenGL小结EGL（Android）实现显示RGB24（Android）参考OpenGL ES播放RGB 简述 OpenGL（英语：Open Graphics Library，译名：开放图形库或者“开放式图形库”）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序编程接口（API）。而OpenGL ES （OpenGL for Embedded Systems）是三维图形应用程序接口OpenGL的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，如Android 。 屏幕图像显示流程 下图所示为常见的 CPU、GPU、显示器工作方式（网上找的）。CPU 计算好显示内容提交至 GPU，GPU 渲染完成后将渲染结果存入帧缓冲区，视频控制器会按照 VSync 信号逐帧读取帧缓冲区的数据，经过数据转换后最终由显示器进行显示。 所以在本节涉及到的内容主要有： CPU：获取本地数据（图像）并计算好作为GPU的输入数据。 GPU：通过OpengGL ES处理后的数据传递给Monitor进行显示。 Monitor：在Android中需要通过EGL作为桥梁衔接屏幕进行显示（因为OpenGL不负责窗口管理及上下文环境管理，而EGL承担了这一职责）。 OpengGL相关处理 OpenGL渲染管线 图形渲染管线（Graphics Pipeline）： 大多译为管线，实际上指的是一堆原始图形数据（顶点信息(坐标、法向量等)和像素信息(图像、纹理等)）途经一个输送管道，期间经过各种变化处理最终出现在屏幕的过程。 图形渲染管线的主要工作可以被划分为两个部分 1）把 3D 坐标转换为 2D 坐标（在OpenGL中，任何事物都在3D空间中，而屏幕和窗口却是2D像素数组）。 2）把 2D 坐标转变为实际的有颜色的像素 图形渲染管线的具体实现可分为六个阶段 注：蓝色部分代表着色器的部分。 第一阶段，顶点着色器： 顶点数据（Vertex Data） 数据（一系列顶点的集合）由3D 坐标转为另一种 3D 坐标，同时顶点着色器可以对顶点属性进行一些基本处理。（下面详细介绍） 第二阶段，形状（图元）装配： 该阶段将顶点着色器输出的所有顶点作为输入，并将所有的点装配成指定图元的形状。图中则是一个三角形。图元（Primitive） 用于表示如何渲染顶点数据，如：点、线、三角形。 第三阶段，几何着色器： 该阶段把图元形式的一系列顶点的集合作为输入，它可以通过产生新顶点构造出新的（或是其它的）图元来生成其他形状。例子中，它生成了另一个三角形。（这个阶段可以省略） 第四阶段，光栅化： 该阶段会把图元映射为最终屏幕上相应的像素，生成片段。片段（Fragment） 是渲染一个像素所需要的所有数据（一个矩形数组，包含了颜色、深度、线宽、点的大小等信息）。 第五阶段，片段着色器： 该阶段首先会对输入的片段进行 裁切（Clipping）。裁切会丢弃超出视图以外的所有像素，用来提升执行效率。（下面详细介绍） 第六阶段，测试与混合 该阶段会检测片段的对应的深度值（z 坐标），判断这个像素位于其它物体的前面还是后面，决定是否应该丢弃。此外，该阶段还会检查 alpha 值（ alpha 值定义了一个物体的透明度），从而对物体进行混合。因此，即使在片段着色器中计算出来了一个像素输出的颜色，在渲染多个三角形的时候最后的像素颜色也可能完全不同。 这是OpenGL内部需要处理的流程，而我们程序处理OpenGL的则是以下两大模块： 1）创建着色器（至少是顶点着色器和片段着色器），设置相关参数\"告诉\"OpenGL如何进行处理，以及绑定相关程序。 2）生成以及编译纹理，把纹理输入到着色器中，并且设置相关参数\"告诉\"OpenGL如何进行处理。 着色器 着色器(Shader)： 是指GPU上为每一个（渲染管线）阶段运行各自的小程序。而OpenGL着色器是用OpenGL着色器语言(OpenGL Shading Language, GLSL)写成的，这是语法简单介绍 。上面就介绍了3种着色器。 如果我们打算做渲染的话，在现代OpenGL中，我们必须定义至少一个顶点着色器和一个片段着色器（因为GPU中没有默认的顶点/片段着色器）。以下将要对两者做详细的介绍。 顶点着色器 主要的目的是把3D坐标转为另一种3D坐标(标准化设备坐标)，以及顶点着色器允许我们对顶点属性进行一些基本处理。 标准化设备坐标（就是下文纹理说的OpenGL物体表面坐标）是一个x、y和z值在-1.0到1.0的一小段空间。任何落在范围外的坐标都会被丢弃/裁剪，不会显示在你的屏幕上。下面你会看到我们定义的在标准化设备坐标中的三角形(忽略z轴)： 与通常的屏幕坐标不同，y轴正方向为向上，(0, 0)坐标是这个图像的中心，而不是左上角。最终你希望所有(变换过的)坐标都在这个坐标空间中，否则它们就不可见了。你的标准化设备坐标接着会变换为屏幕空间坐标(Screen-space Coordinates)，这是使用你通过glViewport函数提供的数据，进行视口变换(Viewport Transform)完成的。所得的屏幕空间坐标又会被变换为片段输入到片段着色器中。 顶点着色器允许我们指定任何以顶点属性为形式的输入。这使其具有很强的灵活性的同时，它还的确意味着我们必须手动指定输入数据的哪一个部分对应顶点着色器的哪一个顶点属性。所以，我们必须在渲染前指定OpenGL该如何解释顶点数据。 使用示例： 1）编写顶点着色器的源代码（根c语言基本一样）： const GLchar* VERTEX_SHADER = \"#version 330 core\\n\" \"layout (location = 0) in vec3 position;\\n\" \"layout (location = 1) in vec3 color;\\n\" \"layout (location = 2) in vec2 texCoord;\\n\" \"out vec3 ourColor;\\n\" \"out vec2 TexCoord;\\n\" \"void main(){\\n\" \" gl_Position = vec4(position, 1.0f);\\n\" \" ourColor = color;\\n\" \" TexCoord = texCoord;\\n\" \"}\\n\"; #version 330 core：表示需要使用OpenGL的版本。 layout (location = 0) in vec3 position;（第三、四行同理）： 1) layout (location = 0)：设定了输入变量position的位置值为0，跟glVertexAttribPointer配合使用，用来告诉OpenGL该如何解析顶点数据。下面纹理还会具体介绍。 2) in：表示position变量是接收外界出入的数据。 3）vec3：表示值为float类型，有3分量的容器，同理有：ivecn包含n个int分量的向量；uvecn包含n个unsigned int分量的向量等等。 out vec3 ourColor;： 表示ourColor变量的数据将作为输出，传递到\"片段着色器\"中作为输入被使用。 gl_Position = vec4(position, 1.0f);： 1) gl_Position：原始的顶点数据在顶点着色器中经过平移、旋转、缩放等数学变换后，生成新的顶点位置（一个四维 (vec4) 变量，包含顶点的 x、y、z 和 w 值）。新的顶点位置通过在顶点着色器中写入gl_Position传递到渲染管线的后继阶段继续处理。 2) vec4(position, 1.0f)：把3个分量的容器position再一个分量1.0f创建一个4个分量的容器。 2）编译顶点着色器（为了能够让OpenGL使用它）： GLuint vertexShader; vertexShader = glCreateShader(GL_VERTEX_SHADER); //创建顶点着色器 glShaderSource(vertexShader, 1, VERTEX_SHADER, NULL); //关联上面的源代码 glCompileShader(vertexShader); //在线编译 //以下为异常情况处理，打印编译错误（如果有的话） GLint success; GLchar infoLog[512]; glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(vertex, 512, NULL, infoLog); std::cout 3）链接到一个用来渲染的着色器程序，着色器程序 ，已激活着色器程序的着色器将在我们发送渲染调用的时候被使用。 //1）着色器程序 GLuint shaderProgram; shaderProgram = glCreateProgram(); //创建一个程序对象 glAttachShader(shaderProgram, vertexShader); //把顶点着色器附加到程序对象上 glAttachShader(shaderProgram, fragmentShader); //把片段着色器附加到程序对象上 glLinkProgram(shaderProgram); //链接它们 glUseProgram(shaderProgram); //激活这个程序对象，调用后每个着色器调用和渲染调用都会使用这个程序对象 // 打印连接错误（如果有的话） glGetProgramiv(shaderProgram, GL_LINK_STATUS, &success); if(!success){ glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog); std::cout 片段着色器 片段着色器(Fragment Shader)全是关于计算你的像素最后的颜色输出。在计算机图形中颜色被表示为有4个元素的数组：红色、绿色、蓝色和alpha(透明度)分量，通常缩写为RGBA。当在OpenGL或GLSL中定义一个颜色的时候，我们把颜色每个分量的强度设置在0.0到1.0之间。比如说我们设置红为1.0f，绿为1.0f，我们会得到两个颜色的混合色，即黄色。 使用示例： 1）编写片段着色器的源代码： const GLchar* FRAGMENT_SHADER = \"#version 330 core\\n\" \"in vec3 ourColor;\\n\" \"in vec2 TexCoord;\\n\" \"out vec4 color;\\n\" \"uniform sampler2D ourTexture;\\n\" \"void main(){\\n\" \" color = texture(ourTexture, TexCoord);\\n\" \"}\\n\"; uniform sampler2D ourTexture;：定义全局(uniform)变量ourTexture，sampler2D是指2D的纹理采样器。 color = texture(ourTexture, TexCoord);：GLSL内建的texture函数来采样纹理的颜色，它第一个参数是纹理采样器，第二个参数是对应的纹理坐标。 2）编译片段着色器（同顶点着色器，参数变了而已）： GLuint fragmentShader; vertexShader = glCreateShader(GL_FRAGMENT_SHADER); //创建顶点着色器 glShaderSource(fragmentShader, 1, FRAGMENT_SHADER, NULL); //关联上面的源代码 glCompileShader(fragmentShader); //在线编译 //以下为异常情况处理，打印编译错误（如果有的话） GLint success; GLchar infoLog[512]; glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(vertex, 512, NULL, infoLog); std::cout 3）链接到一个用来渲染的着色器程序，同顶点着色器。 几个重要概念 FBO FBO: 帧缓存对象(FrameBuffer Objects)；我们想象一个场景，蛋糕工厂加工时，一个蛋糕会贴上奶油，放各种水果，最后打包出一个可以出售的蛋糕给到商店。在GPU和显示器通讯过程中，CPU是工厂，显示器是商店，而帧缓存对象是流水线，流水线保存着蛋糕，直到最后所有加工完才会输出给显示器。这么做的好处主要有两点：①节省GPU和显示器通讯过程的消耗；②在CPU可以拿到最终的成品（蛋糕），比如：录像过程中，开启了各种美颜、贴纸等操作，我们可以使用帧缓存得到GPU中最终显示的数据，进行编码保存或者推送到服务端等操作。这个技术叫做——离屏渲染。 使用实例： //创建帧缓存对象，FBO，之后所有的渲染操作将会渲染到当前绑定帧缓冲的附件中 GLuint fbo; glGenFramebuffers(1, &fbo); glBindFramebuffer(GL_FRAMEBUFFER, fbo); //纹理附件：把一个纹理附加到帧缓冲的时候，所有的渲染指令将会写入到这个纹理中，就像它是一个普通的颜色/深度或模板缓冲一样 GLuint texture; glGenTextures(1, &texture); glBindTexture(GL_TEXTURE_2D, texture); glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, 800, 600, 0, GL_RGB, GL_UNSIGNED_BYTE, NULL); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); //把创建好的纹理texture，要做的最后一件事就是将它附加到帧缓冲上了 glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0,GL_TEXTURE_2D, texture, 0); //在完整性检查执行之前，我们需要给帧缓冲附加一个附件, 要保证所有的渲染操作在主窗口中有视觉效果，我们需要再次激活默认帧缓冲，将它绑定到0 glBindFramebuffer(GL_FRAMEBUFFER, 0); //当进行绘制到屏幕时，看是否需要推送到屏幕 //这里先将FrameBuffer绑定到当前的绘制环境上，所以，在没解绑之前，所有的GL图形绘制操作，都不是直接绘制到屏幕上，而是绘制到这个FrameBuffer上！ glBindFramebuffer(GL_FRAMEBUFFER, fbo);//先把所有本来应该渲染到屏幕的数据先推进FBO draw(texture);//本来应该是显示的屏幕的，结果到了FBO中。如果draw之前调用glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);解绑则会显示到屏幕 glBindFramebuffer(GL_FRAMEBUFFER, 0); //最后解绑FBO，下次如果直接调用onDraw则显示到屏幕 VBO VBO：顶点缓冲对象(Vertex Buffer Objects)；它会在GPU内存(通常被称为显存)中储存大量顶点，使用这些缓冲对象的好处是我们可以一次性的发送一大批数据到显卡上，而不是每个顶点发送一次。从CPU把数据发送到显卡相对较慢，所以只要可能我们都要尝试尽量一次性发送尽可能多的数据。当数据发送至显卡的内存中后，顶点着色器几乎能立即访问顶点，这是个非常快的过程。 使用实例： GLuint VBO; glGenBuffers(1, &VBO); glBindBuffer(GL_ARRAY_BUFFER, VBO); //此时：会生成 1 个 GL_ARRAY_BUFFER 类型的 VBO对象（数量可以一次多个） glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);//把之前定义的顶点数据复制到缓冲的内存中 //最后程序退出时： glDeleteBuffers(1, &VBO); glBufferData相关参数介绍： 参1：目标缓冲的类型：顶点缓冲对象当前绑定到GL_ARRAY_BUFFER目标上。还有GL_ELEMENT_ARRAY_BUFFER等，下面介绍。 参2：指定传输数据的大小(以字节为单位)；用一个简单的sizeof计算出顶点数据大小就行。 参3：我们希望发送的实际数据。 参4：指定了我们希望显卡如何管理给定的数据，它有三种形式： GL_STATIC_DRAW ：数据不会或几乎不会改变。 GL_DYNAMIC_DRAW：数据会被改变很多。 GL_STREAM_DRAW ：数据每次绘制时都会改变。 VAO（Android OpenGL ES3.0才支持） VAO：顶点数组对象(Vertex Array Object)；可以像顶点缓冲对象那样被绑定，任何随后的顶点属性调用都会储存在这个VAO中。这样的好处就是，当配置顶点属性指针时，你只需要将那些调用执行一次，之后再绘制物体的时候只需要绑定相应的VAO就行了。这使在不同顶点数据和属性配置之间切换变得非常简单，只需要绑定不同的VAO就行了。刚刚设置的所有状态都将存储在VAO中。 一个顶点数组对象会储存以下这些内容： glEnableVertexAttribArray和glDisableVertexAttribArray的调用。 通过glVertexAttribPointer设置的顶点属性配置。 通过glVertexAttribPointer调用进行的顶点缓冲对象与顶点属性链接。 使用实例： GLuint VAO; glGenVertexArrays(1, &VAO); //创建一个VAO和创建一个VBO很类似 // ..:: 初始化代码（只运行一次 (除非你的物体频繁改变)） ::.. // 1. 绑定VAO glBindVertexArray(VAO); // 2. 把顶点数组复制到缓冲中供OpenGL使用 glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // 3. 设置顶点属性指针 glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid*)(6 * sizeof(GLfloat))); glEnableVertexAttribArray(2);//对应glVertexAttribPointer中的第一个参数 2 //4. 解绑VAO glBindVertexArray(0); [...] // ..:: 绘制代（游戏循环中） ::.. // 5. 绘制物体 glUseProgram(shaderProgram); glBindVertexArray(VAO); //使用 glDrawElements 等进行绘制 glBindVertexArray(0); glVertexAttribPointer相关参数介绍： 参1：指定我们要配置的顶点属性。还记得我们在顶点着色器中使用layout(location = 0)定义了position顶点属性的位置值(Location)吗？它可以把顶点属性的位置值设置为2。因为我们希望把数据传递到这一个顶点属性中，所以这里我们传入2。 参2：指定顶点属性的大小。顶点属性是一个vec2，它由2个值组成，所以大小是2。 参3：指定数据的类型，这里是GL_FLOAT(GLSL中vec*都是由浮点数值组成的)。 参4：我们是否希望数据被标准化(Normalize)。如果我们设置为GL_TRUE，所有数据都会被映射到0（对于有符号型signed数据是-1）到1之间。我们把它设置为GL_FALSE。 参5：步长(Stride)，它告诉我们在连续的顶点属性组之间的间隔。由于下个组位置数据在8个GLfloat之后，我们把步长设置为8 * sizeof(GLfloat)。 参6：类型是GLvoid*，所以需要我们进行这个奇怪的强制类型转换。它表示位置数据在缓冲中起始位置的偏移量(Offset)。 该方法告诉了OpenGL如何指向（解析）不同的顶点。如上面定义的顶点着色器中的属性： \"layout (location = 0) in vec3 position;\\n\" \"layout (location = 1) in vec3 color;\\n\" \"layout (location = 2) in vec2 texCoord;\\n\" 说明一个顶点中存在了：3个向量顶点位置 + 3个向量RGB颜色 + 2个向量纹理 = 8个向量；根据下图更容易理解是如何如何计算寻找下一个顶点位置的纹理位置： EBO EBO：索引缓冲对象(Element Buffer Object，EBO，也叫Index Buffer Object，IBO)；和顶点缓冲对象一样，EBO也是一个缓冲，它专门储存索引，OpenGL调用这些顶点的索引来决定该绘制哪个顶点。因为OpenGL主要处理三角形的，所以如果绘制矩形则需要绘制两个三角形，而两个三角形有6个顶点，其中有两个顶点是重叠的，造成多余的浪费，为此，OpenGL引入了\"索引缓冲对象\"来处理，只需要4个顶点，然后画两个三角形即可；如： 原本通过6个顶点来绘制一个矩形的： GLfloat vertices[] = { // 第一个三角形 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, 0.5f, 0.0f, // 左上角 // 第二个三角形 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角 }; 现在通过EBO方法进行处理，只需要存储4个顶点，然后告诉OpenGL如何绘制即可： GLfloat vertices[] = { 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角 }; GLuint indices[] = { // 注意索引从0开始! 0, 1, 3, // 第一个三角形 1, 2, 3 // 第二个三角形 }; 下图为通过图： 下面使用索引绘制两个三角形拼成一个矩形，使用实例： GLuint EBO; glGenBuffers(1, &EBO); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); //和VBO类似， glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);//把索引复制到缓冲里 //当进行使用时 glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0);//使用当前绑定到GL_ELEMENT_ARRAY_BUFFER目标的EBO中获取索引进行绘制 glDrawElements相关参数介绍： 参1：指定绘制模式，这里三角形。 参2：绘制顶点的个数，绘制两个三角形，即6个顶点。 参3：索引的类型，GL_UNSIGNED_INT（无符号整形）。 参4：指定EBO中的偏移量，（或者传递一个索引数组，但是这是当你不在使用索引缓冲对象的时候）。 注意：VAO绑定时正在绑定的索引缓冲对象会被保存为VAO的元素缓冲对象。绑定VAO的同时也会自动绑定EBO，如图: VBO、VAO、EBO总结 VBO在GPU中缓存大量顶点数据，VAO存储顶点属性(函数)的调用，而EBO则根据索引来绘制顶点（三角形->矩形）。 纹理 纹理（Texture）是一个2D图片（甚至也有1D和3D的纹理），它可以用来添加物体的细节；在视频渲染中，只需要处理2D的纹理。这是纹理的介绍和使用 。 纹理所在流程 纹理(图片) --映射-> 纹理坐标 --传入-> 顶点着色器(进行采样) --传入-> 片段着色器(为每个片段进行纹理坐标的插值) 上面流程介绍了纹理(图片)被处理的过程，这都是在OpengGL渲染管道里面被处理的，而我们所需要做的就是如何设置参数，指引OpenGL处理，处理之前，需要介绍一下纹理的相关知识。 纹理坐标(Texture Coordinate) 这里说的纹理坐标是指OpenGL纹理坐标，除此之外，我们还需要知道另外两个坐标系，\"计算机图像纹理坐标\" 和 \"OpenGL物体表面坐标\"；通常使用是从图片等解析出来RGB格式的Byte数组是 \"计算机图像纹理坐标\" ；三者之间的关系如下： 计算机图像纹理坐标 -> OpenGL纹理坐标 -> OpenGL物体表面坐标 以下为三者的坐标系： 当我们直接把解析出来的数据（图：坐着电脑前的人）直接使用在 \"OpenGL纹理坐标\" 上，就会上下颠倒了，如下： 正确姿势如下（看\"计算机图像纹理坐标\"和\"OpenGL物体表面坐标\"对应的坐标与图像的变化）： 坐标理解非常重要，处理不好导致的效果就是歪画面。这里纹理还需要对应着顶点的坐标，见下面【纹理的使用】中定义的vertices纹理坐标。 采样(Sampling) 使用纹理坐标获取纹理颜色。 片段插值(Fragment Interpolation) 在片段中（假如无限放大后，纹理在物体表面中的片段），某些坐标点没有颜色，这时候需要对这些坐标进行插值。 纹理的使用 先看示例代码 /*****************************************/ /***** 1）生成纹理： ************/ /*****************************************/ GLuint texture; glGenTextures(1, &texture); //生成\"1\"个纹理 glBindTexture(GL_TEXTURE_2D, texture); //绑定为2D类型 //将image图片数据生成纹理，这里的image数据会根据下面vertices中- 纹理坐标 -传入到顶点着色器中进行处理 glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, image); glGenerateMipmap(GL_TEXTURE_2D); //这会为当前绑定的纹理自动生成所有需要的多级渐远纹理。 glBindTexture(GL_TEXTURE_2D, 0); //生成了纹理和相应的多级渐远纹理后，释放图像的内存并解绑纹理对象 /*****************************************/ /***** 2）应用纹理： ************/ /*****************************************/ //使用纹理坐标更新顶点数据： GLfloat vertices[] = { // ---- 顶点位置 ---- ---- 颜色 ---- - 纹理坐标 - 1.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 0.0f, // 右上 1.0f, -1.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 1.0f, // 右下 -1.0f, -1.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, // 左下 -1.0f, 1.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, 0.0f // 左上 }; //告诉OpenGL应该如何去找新的顶点纹理属性数据 glVertexAttribPointer(2, 2, GL_FLOAT,GL_FALSE, 8 * sizeof(GLfloat), (GLvoid*)(6 * sizeof(GLfloat))); glEnableVertexAttribArray(2); //调整顶点着色器使其能够接受顶点坐标为一个顶点属性，并把坐标传给片段着色器： const GLchar* VERTEX_SHADER = \"见 顶点着色器 源码\"; //片段着色器：接收顶点着色器传递过来的纹理对象，并使用sampler2D采集器进行采集 const GLchar* FRAGMENT_SHADER = \"见 片段着色器 源码\"; //这里省略着色器以及关联程序流程 //绑定纹理，并进行绘制 glBindTexture(GL_TEXTURE_2D, texture); glBindVertexArray(VAO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); glBindVertexArray(0); OpenGL小结 上面是直接使用RGB24格式的纹理进行渲染，而OpenGL是支持矩阵计算的(矩阵使用在这) ，也就是可以支持输入YUV格式数据，通过GPU在顶点着色器进行矩阵计算，转换成RGB后再进行使用。但这也会导致更复杂的计算以及适配问题（如各种yuv或其他格式的rgb数据转换为RGB时），所以我们可以选择牺牲一点CPU使用FFmpeg进行格式转换后再进行渲染。 上面的内容已经把OpenGL中纹理绘制讲解完了，然后在Mac上写了一个Demo，实现绘制一张rgb24格式的裸数据 Demo地址：OpenGLMacTextureDemo 。接下来要在Android中实现。 EGL（Android） 开头就已经说了EGL的作用了，就是作为OpenGL的输出与设备的屏幕之间架接起的桥梁，EGL是双缓冲的工作模式，即有一个Back Frame Buffer和一个Front Frame Buffer，正常绘制操作的目标都是Back Frame Buffer，操作完毕之后，调用eglSwapBuffer这个API，将绘制完毕的FrameBuffer交换到Front Frame Buffer并显示出来。而在Android平台上，使用的是EGL这一套机制，EGL承担了为OpenGL提供上下文环境以及窗口管理的职责。 下面直接来看代码： //_surface是从JAVA层传递过来的 ANativeWindow *window = ANativeWindow_fromSurface(env, _surface); // 1 EGL dispaly创建和初始化 EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); if(display == EGL_NO_DISPLAY){ LOGE(\"eglGetDisplay failed\"); return; } //初始化这个显示设备，该方法会返回一个布尔型变量来代表执行状态，后面两个参数则代表Major和Minor的版本。 if (EGL_FALSE == eglInitialize(display, 0, 0)) { LOGE(\"NativeEngine: failed to init display, error %d\", eglGetError()); return; } //一旦EGL有了Display之后，它就可以将OpenGL ES的输出和设备的屏幕桥接起来，但是需要指定一些配置项，类似于色彩格式、像素格式、RGBA的表示以及SurfaceType等 //2 surface窗口配置 EGLConfig config; EGLint configNum; EGLint attribs[] = { EGL_SURFACE_TYPE,EGL_WINDOW_BIT, EGL_BUFFER_SIZE, 24, EGL_RED_SIZE,8, EGL_GREEN_SIZE, 8, EGL_BLUE_SIZE, 8, EGL_DEPTH_SIZE, 16, EGL_NONE }; if(EGL_TRUE != eglChooseConfig(display, attribs, &config, 1, &configNum)){ LOGE(\"eglChooseConfig failed!\"); return; } //3. 创建surface，将EGL和设备的屏幕连接起来 EGLSurface surface = NULL; EGLint format; if (!eglGetConfigAttrib(display, config, EGL_NATIVE_VISUAL_ID, &format)) { LOGE(\"eglGetConfigAttrib() returned error %d\", eglGetError()); return; } ANativeWindow_setBuffersGeometry(window, 0, 0, format); if (!(surface = eglCreateWindowSurface(display, config, window, 0))) { LOGE(\"eglCreateWindowSurface() returned error %d\", eglGetError()); } //4 context 创建关联的上下文 const EGLint ctxAttr[] = { EGL_CONTEXT_CLIENT_VERSION, 2, EGL_NONE }; EGLContext context = eglCreateContext(display, config, EGL_NO_CONTEXT, ctxAttr); if(context == EGL_NO_CONTEXT){ LOGE(\"eglCreateContext failed!\"); return; } //5. 进行绑定 if(EGL_TRUE != eglMakeCurrent(display, surface, surface, context)){ LOGE(\"eglMakeCurrent failed！\"); return; } //此处为处理以上OpenGL相关 //在OpenGL执行渲染之后，最后通知window进行显示： eglSwapBuffers(display,surface); 实现显示RGB24（Android） 1）在CMakeList.txt引入相关库: target_link_libraries( native-lib android log EGL GLESv3) 2）申请读取文件权限；（因为本demo需要把文件拷贝到手机的Download目录下）。 3）在java层通过SurfaceView.getHolder().getSurface()函数把Surface传递给底层，开启子线程。 4）在C++处理OpenGL以及窗口显示等： VideoPlayer::VideoPlayer(ANativeWindow* window,const char *filename) { // 1 EGL dispaly创建和初始化 EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); if(display == EGL_NO_DISPLAY){ LOGE(\"eglGetDisplay failed\"); return; } //初始化这个显示设备，该方法会返回一个布尔型变量来代表执行状态，后面两个参数则代表Major和Minor的版本。 if (EGL_FALSE == eglInitialize(display, 0, 0)) { LOGE(\"NativeEngine: failed to init display, error %d\", eglGetError()); return; } //一旦EGL有了Display之后，它就可以将OpenGL ES的输出和设备的屏幕桥接起来，但是需要指定一些配置项，类似于色彩格式、像素格式、RGBA的表示以及SurfaceType等 //2 surface窗口配置 EGLConfig config; EGLint configNum; EGLint attribs[] = { EGL_SURFACE_TYPE,EGL_WINDOW_BIT, EGL_BUFFER_SIZE, 24, EGL_RED_SIZE,8, EGL_GREEN_SIZE, 8, EGL_BLUE_SIZE, 8, EGL_DEPTH_SIZE, 16, EGL_NONE }; if(EGL_TRUE != eglChooseConfig(display, attribs, &config, 1, &configNum)){ LOGE(\"eglChooseConfig failed!\"); return; } //3. 创建surface，将EGL和设备的屏幕连接起来 EGLSurface surface = NULL; EGLint format; if (!eglGetConfigAttrib(display, config, EGL_NATIVE_VISUAL_ID, &format)) { LOGE(\"eglGetConfigAttrib() returned error %d\", eglGetError()); return; } ANativeWindow_setBuffersGeometry(window, 0, 0, format); if (!(surface = eglCreateWindowSurface(display, config, window, 0))) { LOGE(\"eglCreateWindowSurface() returned error %d\", eglGetError()); } //4 context 创建关联的上下文 const EGLint ctxAttr[] = { EGL_CONTEXT_CLIENT_VERSION, 2, EGL_NONE }; EGLContext context = eglCreateContext(display, config, EGL_NO_CONTEXT, ctxAttr); if(context == EGL_NO_CONTEXT){ LOGE(\"eglCreateContext failed!\"); return; } //5. 进行绑定 if(EGL_TRUE != eglMakeCurrent(display, surface, surface, context)){ LOGE(\"eglMakeCurrent failed！\"); return; } LOGD(\"NativeEngine: EGL finish, and creating OpenGL.\"); /*********************************************************/ /******************* OpenGL ********************/ /*********************************************************/ // 版本查看NDK Demo const GLchar *VERTEX_SHADER = \"#version 300 es\\n\" \"layout (location = 0) in vec3 position;\\n\" \"layout (location = 1) in vec3 color;\\n\" \"layout (location = 2) in vec2 texCoord;\\n\" \"out vec3 ourColor;\\n\" \"out vec2 TexCoord;\\n\" \"void main(){\\n\" \" gl_Position = vec4(position, 1.0f);\\n\" \" ourColor = color;\\n\" \" TexCoord = texCoord;\\n\" \"}\\n\"; //precision mediump float; 是从报错日志查出来的 const GLchar *FRAGMENT_SHADER = \"#version 300 es\\n\" \"precision mediump float;\\n\" \"in vec3 ourColor;\\n\" \"in vec2 TexCoord;\\n\" \"out vec4 color;\\n\" \"uniform sampler2D ourTexture;\\n\" \"void main(){\\n\" \" color = texture(ourTexture, TexCoord);\\n\" \"}\\n\"; const int pixel_w = 384, pixel_h = 216; unsigned char *buffer = new unsigned char[pixel_w * pixel_h * 3]; FILE *fp = fopen(filename, \"rb+\"); fread(buffer, pixel_w * pixel_h * 3, 1, fp); // Define the viewport dimensions //此处应该感觉屏幕实际大小进行处理 glViewport(80, 300, pixel_w*2, pixel_h*2); // Build and compile our shader program GLuint vertexShader = glCreateShader(GL_VERTEX_SHADER); glShaderSource(vertexShader, 1, &VERTEX_SHADER, NULL); glCompileShader(vertexShader); //以下为异常情况处理，打印编译错误（如果有的话） GLint success; GLchar infoLog[512]; glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(vertexShader, 512, NULL, infoLog); LOGE(\"ERROR::SHADER::VERTEX::COMPILATION_FAILED1 %s\\n\",infoLog); } GLuint fragmentShader = glCreateShader(GL_FRAGMENT_SHADER); glShaderSource(fragmentShader, 1, &FRAGMENT_SHADER, NULL); glCompileShader(fragmentShader); //以下为异常情况处理，打印编译错误（如果有的话） glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &success); if(!success){ glGetShaderInfoLog(fragmentShader, 512, NULL, infoLog); LOGE(\"ERROR::SHADER::VERTEX::COMPILATION_FAILED2 %s\\n\",infoLog); } GLuint shaderProgram = glCreateProgram(); glAttachShader(shaderProgram, vertexShader); glAttachShader(shaderProgram, fragmentShader); glLinkProgram(shaderProgram); glDeleteShader(vertexShader); glDeleteShader(fragmentShader); // Set up vertex data (and buffer(s)) and attribute pointers GLfloat vertices[] = { // ---- 位置 ---- ---- 颜色 ---- - 纹理坐标 - 1.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 0.0f, // 右上 1.0f, -1.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 1.0f, // 右下 -1.0f, -1.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, // 左下 -1.0f, 1.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, 0.0f // 左上 }; //从0开始，画两个三角形，组成矩形（从0开始，画两个三角形的索引即可） GLuint indices[] = { // Note that we start from 0! 0, 1, 3, // First Triangle 1, 2, 3 // Second Triangle }; GLuint VBO, VAO, EBO; // 1. 绑定顶点数组对象 glGenVertexArrays(1, &VAO); glGenBuffers(1, &VBO); glGenBuffers(1, &EBO); glBindVertexArray(VAO); glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW); // Position attribute glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid *) 0); glEnableVertexAttribArray(0); // Color attribute glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid *) (3 * sizeof(GLfloat))); glEnableVertexAttribArray(1); // TexCoord attribute glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (GLvoid *) (6 * sizeof(GLfloat))); glEnableVertexAttribArray(2); glBindVertexArray(0); // Unbind VAO // Load and create a texture GLuint texture; glGenTextures(1, &texture); glBindTexture(GL_TEXTURE_2D, texture); // All upcoming GL_TEXTURE_2D operations now have effect on this texture object // Set the texture wrapping parameters glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); // Set texture wrapping to GL_REPEAT (usually basic wrapping method) glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); // Set texture filtering parameters glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); // Load image, create texture and generate mipmaps //就让他一直显示吧 while (1){ // Render // Clear the colorbuffer glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // Bind Texture glBindTexture(GL_TEXTURE_2D, texture); glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, pixel_w, pixel_h, 0, GL_RGB, GL_UNSIGNED_BYTE, buffer); // Activate shader glUseProgram(shaderProgram); // Draw container glBindVertexArray(VAO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); glBindVertexArray(0); eglSwapBuffers(display,surface); sleep(0.5); } // Properly de-allocate all resources once they've outlived their purpose glDeleteVertexArrays(1, &VAO); glDeleteBuffers(1, &VBO); glDeleteBuffers(1, &EBO); //省略释放内存等..... } Android项目地址：OpenGLESDemo Mac项目地址：OpenGLMacTextureDemo 参考 https://en.wikipedia.org/wiki/OpenSL_ES https://blog.csdn.net/leixiaohua1020/article/details/40379845 https://github.com/android/ndk-samples/tree/main/audio-echo https://blog.csdn.net/ywl5320/article/details/78503768 http://web.cse.ohio-state.edu/~shen.94/781/Site/Slides_files/pipeline.pdf Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/03_mediacodec.html":{"url":"AFPlayer/03_mediacodec.html","title":"FFmpeg实现MediaCodec解码","keywords":"","body":"FFmpeg实现MediaCodec解码简述实现流程一、编译的FFmpeg二、导入三、代码实现输出验证FFmpeg实现MediaCodec解码 简述 MediaCodec是Android平台的硬件编解码，FFmpeg从3.1版本开始支持，但是，目前支持实现解码功能。官网硬件编解码支持结束 实现流程 一、编译的FFmpeg 为了快速编译，以及减少体积，这里关闭了大部分功能。 #!/bin/bash API=21 NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 function build_android() { ./configure \\ --prefix=$PREFIX \\ --disable-opencl \\ --disable-doc \\ --disable-everything \\ --disable-htmlpages \\ --disable-podpages \\ --disable-debug \\ --disable-programs \\ --disable-demuxers \\ --disable-muxers \\ --disable-decoders \\ --disable-encoders \\ --disable-bsfs \\ --disable-indevs \\ --disable-outdevs \\ --disable-filters \\ --disable-protocols \\ --disable-hwaccels \\ --disable-avdevice \\ --disable-postproc \\ --disable-devices \\ --disable-symver \\ --disable-stripping \\ --disable-asm \\ --disable-w32threads \\ --disable-parsers \\ --disable-shared \\ --enable-static \\ --enable-swscale \\ --enable-protocol=file \\ --enable-protocol=hls \\ --enable-protocol=rtmp \\ --enable-protocol=http \\ --enable-protocol=tls \\ --enable-protocol=https \\ --enable-demuxer=aac \\ --enable-demuxer=h264 \\ --enable-demuxer=mov \\ --enable-demuxer=flv \\ --enable-demuxer=avi \\ --enable-demuxer=hevc \\ --enable-demuxer=gif \\ --enable-parser=h264 \\ --enable-parser=hevc \\ --enable-parser=aac \\ --enable-decoder=aac \\ --enable-decoder=h264 \\ --enable-decoder=flv \\ --enable-decoder=gif \\ --enable-decoder=mpeg4 \\ --enable-bsf=aac_adtstoasc \\ --enable-bsf=h264_mp4toannexb \\ --enable-jni \\ --enable-mediacodec \\ --enable-decoder=h264_mediacodec \\ --enable-decoder=hevc_mediacodec \\ --enable-decoder=mpeg4_mediacodec \\ --enable-hwaccel=h264_mediacodec \\ --enable-neon \\ --enable-gpl \\ --cross-prefix=$CROSS_PREFIX \\ --target-os=android \\ --arch=$ARCH \\ --cpu=$CPU \\ --cc=$CC \\ --cxx=$CXX \\ --enable-cross-compile \\ --sysroot=$SYSROOT \\ --extra-cflags=\"-Os -fpic $OPTIMIZE_CFLAGS\" \\ --extra-ldflags=\"$ADDI_LDFLAGS\" || exit 1 make clean make -j8 make install } #armv8-a ARCH=arm64 CPU=armv8-a CC=$TOOLCHAIN/bin/aarch64-linux-android$API-clang CXX=$TOOLCHAIN/bin/aarch64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/aarch64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU\" #build_android #armv7-a ARCH=arm CPU=armv7-a CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU \" build_android #x86 ARCH=x86 CPU=x86 CC=$TOOLCHAIN/bin/i686-linux-android$API-clang CXX=$TOOLCHAIN/bin/i686-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/i686-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=i686 -mtune=intel -mssse3 -mfpmath=sse -m32\" #build_android #x86_64 ARCH=x86_64 CPU=x86-64 CC=$TOOLCHAIN/bin/x86_64-linux-android$API-clang CXX=$TOOLCHAIN/bin/x86_64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/x86_64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU -msse4.2 -mpopcnt -m64 -mtune=intel\" #build_android 二、导入 将FFmpeg到Android Studio项目中，编辑CMakeList.txt如下： #1、添加版本 cmake_minimum_required(VERSION 3.4.1) include_directories(include) file(GLOB core_srcs ${CMAKE_SOURCE_DIR}/core/*.cpp) #2、编译库，把native-lib.cpp编译成以native-lib为名称的动态库 add_library( native-lib SHARED player_main.cpp ${core_srcs}) #3、设置C++编译参数（CMAKE_CXX_FLAGS是全局变量） #添加其他预编译库还可以使用这种方式 #使用-L指导编译时库文件的查找路径 #cxx_flags \"cxx_flags -L目录\" set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -L${CMAKE_SOURCE_DIR}/libs/${CMAKE_ANDROID_ARCH_ABI}\") #4、链接到库文件，jni/c/c++可以引入链接到 target_link_libraries( native-lib log # 先把有依赖的库，先依赖进来；注意顺序。 avformat avcodec avfilter avutil swresample swscale z ) 目录结构见底部项目Demo。 三、代码实现 1）给FFmpeg设置JavaVM extern \"C\" JNIEXPORT jint JNI_OnLoad(JavaVM *vm, void *res) { av_jni_set_java_vm(vm, 0); // 返回jni版本 return JNI_VERSION_1_4; } 2）API流程 3）关键代码 /** * description: * @author 秦城季 * @email xhunmon@126.com * @blog https://qincji.gitee.io * @date 2021/2/1 */ #include \"hw_mediacodec.h\" static AVBufferRef *hw_device_ctx = NULL; static enum AVPixelFormat hw_pix_fmt; static FILE *output_file = NULL; static int hw_decoder_init(AVCodecContext *ctx, const enum AVHWDeviceType type) { int err = 0; if ((err = av_hwdevice_ctx_create(&hw_device_ctx, type, NULL, NULL, 0)) hw_device_ctx = av_buffer_ref(hw_device_ctx); return err; } static enum AVPixelFormat get_hw_format(AVCodecContext *ctx, const enum AVPixelFormat *pix_fmts) { const enum AVPixelFormat *p; for (p = pix_fmts; *p != -1; p++) { if (*p == hw_pix_fmt) return *p; } HWLOGE(\"Failed to get HW surface format.\\n\"); return AV_PIX_FMT_NONE; } static int decode_write(AVCodecContext *avctx, AVPacket *packet) { AVFrame *frame = NULL, *sw_frame = NULL; AVFrame *tmp_frame = NULL; uint8_t *buffer = NULL; int size; int ret = 0; ret = avcodec_send_packet(avctx, packet); if (ret format == hw_pix_fmt) { if ((ret = av_hwframe_transfer_data(sw_frame, frame, 0)) (tmp_frame->format), tmp_frame->width, tmp_frame->height, 1); buffer = static_cast(av_malloc(size)); if (!buffer) { HWLOGE(\"Can not alloc buffer\\n\"); ret = AVERROR(ENOMEM); goto fail; } //本次测试所得：AV_PIX_FMT_NV12 ret = av_image_copy_to_buffer(buffer, size, (const uint8_t *const *) tmp_frame->data, (const int *) tmp_frame->linesize, static_cast(tmp_frame->format), tmp_frame->width, tmp_frame->height, 1); if (ret name, av_hwdevice_get_type_name(type)); return; } if (config->methods & AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX && config->device_type == type) { hw_pix_fmt = config->pix_fmt; break; } } if (!(decoder_ctx = avcodec_alloc_context3(decoder))) { HWLOGE(\"Cannot find a video stream in the input file%d\", AVERROR(ENOMEM)); return; } //将参数赋值给新的编码器 if ((ret = avcodec_parameters_to_context(decoder_ctx, input_ctx->streams[video_stream]->codecpar)) get_format = get_hw_format; if (hw_decoder_init(decoder_ctx, type) = 0) { if ((ret = av_read_frame(input_ctx, &packet)) 输出验证 上面顺利输出后的文件格式是nv12的，我们使用ffplay进行播放如下： ffplay -f rawvideo -video_size 384x216 -pixel_format nv12 -framerate 15 Kobe-384x216.yuv Demo地址：FFmpegMediaCodecDemo Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"AFPlayer/04_build_for_android.html":{"url":"AFPlayer/04_build_for_android.html","title":"FFmpeg、x264、fdk-aac、openssl的交叉编译","keywords":"","body":"FFmpeg、x264、fdk-aac、openssl的交叉编译简述X264重要参数X264在NDK17使用gcc编译X264在NDK21使用clang编译Fdk-aac编译前的操作Fdk-aac在NDK17使用gcc编译Fdk-aac在NDK21使用clang编译OpenSSL重要参数OpenSSL在NDK17使用gcc编译OpenSSL在NDK21使用clang编译FFmpeg方案一：使用gcc进行编译FFmpeg在NDK21使用clang编译导入到Android Studio中进行验证参考- 0基础学习音视频路线，以及重磅音视频资料下载 求赞。FFmpeg、x264、fdk-aac、openssl的交叉编译 简述 以前在Android进行交叉编译都是零零散散的，内容也不是很全，现在重新整理一遍，以便以后参考和移植。本文FFmpeg将集成x264、fdk aac以及openssl第三方库，以及开启mediacodec。 Android NDK在版本r17c之后弃用了gcc编译器，改用clang编译器。所以如果ndk在r17c及以下则是使用gcc进行编译。所以以下有两种ndk版本编译： NDK17使用gcc编译（FFmpeg最低支持API=21） NDK21使用clang编译（FFmpeg最低支持API更低，推荐使用） X264 在ffmpeg中使用的h264编码通常是使用x264库内置进去，因为x264的算法更优秀。所以需要把x264编译生成静态库后进行移植。 重要参数 --prefix：安装的目录 --cross-prefix：交叉编译的前缀路径，后面会拼接gcc、ld等 --sysroot：编译过程会去这个目录下找依赖相关的类等 --enable-pic：生成与路径无关静/动态库 --extra-cflags：编译过程需要传递的参数，相关参数参考 X264在NDK17使用gcc编译 ndk在r17c及以下则是使用gcc进行编译，编译脚本如下： #!/bin/sh NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c API=17 build_x264() { ./configure \\ --prefix=${PREFIX} \\ --cross-prefix=${CROSS_PREFIX} \\ --sysroot=${SYSROOT} \\ --host=${HOST} \\ --enable-shared \\ --disable-static \\ --enable-pic \\ --disable-cli \\ --extra-cflags=\"${EXTRA_CFLAGS}\" || exit 0 make clean make install } PREFIX=$(pwd)/android_r17c/armeabi-v7a HOST=arm-linux CROSS_PREFIX=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi- SYSROOT=$NDK/platforms/android-$API/arch-arm EXTRA_CFLAGS=\"-D__ANDROID_API__=$API -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" build_x264 注意：darwin-x86_64这是平台相关，这里表示MacOS系统，如果是Linux则是linux-x86_64，自己的请看NDK路径的。 X264在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 1）修改x264库跟目录下的configure文件 a. 找到CC=\"${CC-${cross_prefix}gcc}\"修改成CC=\"${CC-$CC_PATH}\"；这里的CC_PATH是我们需要添加到环境变量的clang编译器路径，居然看编译脚本。 b. 把全局的${cross_prefix}gcc-改成${cross_prefix}-；也就是把拼接中的gcc去掉。 2）看脚本文件： #!/bin/bash NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 API=21 build_x264() { ./configure \\ --prefix=${PREFIX} \\ --cross-prefix=${CROSS_PREFIX} \\ --sysroot=$SYSROOT \\ --host=${HOST} \\ --disable-asm \\ --enable-shared \\ --disable-static \\ --disable-opencl \\ --enable-pic \\ --disable-cli \\ --extra-cflags=\"$EXTRA_CFLAGS\" || exit 0 make clean make install } #armeabi-v7a PREFIX=$(pwd)/android_r21/armeabi-v7a HOST=arm-linux #把CC_PATH设置成环境变量，然后在configure中直接读取 export CC_PATH=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- #EXTRA_CFLAGS=\"-D__ANDROID_API__=$API -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" build_x264 Fdk-aac 重要参数跟x264的差不过的。 编译前的操作 将libfdkaac/libSBRdec/src/lpp_tran.cpp中的__ANDROID__改成__ANDROID_OFF__；也就是去掉log/log.h，因为ndk中没有这玩意。 Fdk-aac在NDK17使用gcc编译 ndk在r17c及以下则是使用gcc进行编译，编译脚本如下： #!/bin/bash NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c TOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 API=17 build_aac() { ./configure \\ --prefix=${PREFIX} \\ --host=${HOST} \\ --with-pic \\ --target=android \\ --disable-debug \\ --disable-asm \\ --enable-static=no \\ --enable-shared=yes \\ || exit 0 make clean make install } #armeabi-v7a PREFIX=$(pwd)/android_r17c/armeabi-v7a HOST=arm-linux-androideabi SYSROOT=$NDK/platforms/android-${API}/arch-arm CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- #以下都是设置环境变量形式就行传递的，查看方式：configure --help export CC=${CROSS_COMPILE}gcc export CPP=${CROSS_COMPILE}cpp export CXX=${CROSS_COMPILE}g++ export CFLAGS=\"-D__ANDROID_API__=$API --sysroot=${SYSROOT} -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" #-D__ANDROID__=OFF 取消宏定义都没用，也会被覆盖 export CPPFLAGS=\"${CFLAGS}\" export CXXFLAGS=\"${CFLAGS}\" export AR=${CROSS_COMPILE}ar export AS=${CROSS_COMPILE}as export LD=${CROSS_COMPILE}ld export RANLIB=${CROSS_COMPILE}ranlib export STRIP=${CROSS_COMPILE}strip build_aac Fdk-aac在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 #!/bin/bash NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 API=21 build_aac() { ./configure \\ --prefix=${PREFIX} \\ --host=${HOST} \\ --with-pic \\ --target=android \\ --disable-debug \\ --disable-asm \\ --enable-static=no \\ --enable-shared=yes \\ CPPFLAGS=\"-fPIC\" || exit 0 make clean make install } #armeabi-v7a PREFIX=$(pwd)/android_r21/armeabi-v7a HOST=arm-linux-androideabi export AR=$TOOLCHAIN/bin/arm-linux-androideabi-ar export AS=$TOOLCHAIN/bin/arm-linux-androideabi-as export LD=$TOOLCHAIN/bin/arm-linux-androideabi-ld export RANLIB=$TOOLCHAIN/bin/arm-linux-androideabi-ranlib export STRIP=$TOOLCHAIN/bin/arm-linux-androideabi-strip export CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang export CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ build_aac OpenSSL 重要参数 --prefix：安装的目录 ANDROID_NDK：设置环境变量，名称是不能改变的 -D__ANDROID_API__=$API：指定android版本 android-arm：生成架构类型，看官网 生成动态库时需要把.so.xx中的.xx放到前面来，因为android加载动态库时不支持；操作方式：直接全局搜索.so.，然后把后面的放到前面来，如： '.so.$(SHLIB_VERSION_NUMBER)' --> '.$(SHLIB_VERSION_NUMBER).so' 注意：其他的环境变量没有过多的验证 OpenSSL在NDK17使用gcc编译 ndk在r17c及以下则是使用gcc进行编译，编译脚本如下： #!/bin/sh API=17 export ANDROID_NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c export PATH=$ANDROID_NDK/toolchains/llvm/prebuilt/darwin-x86_64/bin:$ANDROID_NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin:$PATH export SYSROOT=$ANDROID_NDK/platforms/android-${API}/arch-arm export TOOLCHAIN=$ANDROID_NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- #以下都是设置环境变量形式就行传递的，查看方式：configure --help export CC=\"${CROSS_COMPILE}gcc\" export CFLAGS=\"-D__ANDROID_API__=$API -isysroot $ANDROID_NDK/sysroot -I$ANDROID_NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" export STRIP=\"${CROSS_COMPILE}strip\" export RANLIB=\"${CROSS_COMPILE}ranlib\" export AR=\"${CROSS_COMPILE}ar\" export LD=\"${CROSS_COMPILE}ld\" export AS=\"${CROSS_COMPILE}as\" ./Configure android-arm -D__ANDROID_API__=$API --prefix=$(pwd)/android_r17c/armeabi-v7a make clean make -j8 make install OpenSSL在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 API=21 export ANDROID_NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 export PATH=$ANDROID_NDK/toolchains/llvm/prebuilt/darwin-x86_64/bin:$ANDROID_NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin:$PATH export SYSROOT=$ANDROID_NDK/platforms/android-${API}/arch-arm export TOOLCHAIN=$ANDROID_NDK/toolchains/llvm/prebuilt/darwin-x86_64/bin CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- #以下都是设置环境变量形式就行传递的，查看方式：configure --help export CC=\"${CROSS_COMPILE}gcc\" export CFLAGS=\"-D__ANDROID_API__=$API -isysroot $ANDROID_NDK/sysroot -I$ANDROID_NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC -marm\" export STRIP=\"${CROSS_COMPILE}strip\" export RANLIB=\"${CROSS_COMPILE}ranlib\" export AR=\"${CROSS_COMPILE}ar\" export LD=\"${CROSS_COMPILE}ld\" export AS=\"${CROSS_COMPILE}as\" ./Configure android-arm -D__ANDROID_API__=$API --prefix=$(pwd)/android_r21/armeabi-v7a make clean make -j8 make install FFmpeg 为了减少编译的时间以及减少包的体积，这里关闭了很多的参数。 注意： 1）启用openssl时需要开启--pkg-config=pkg-config，不然会找不着库。 2）使用动态库形式进行引入，不然会报找不到第三库的文件，原因未知。 方案一：使用gcc进行编译 #!/bin/bash #报错了，哈哈 #libavcodec/v4l2_buffers.c:434:44: error: call to 'mmap' declared with attribute error: mmap is not available with _FILE_OFFSET_BITS=64 when using GCC until android-21. Either raise your minSdkVersion, disable _FILE_OFFSET_BITS=64, or switch to Clang. API=21 export NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r17c export SYSROOT=$NDK/platforms/android-$API/arch-arm export TOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 X264_PREFIX=$(pwd)/libx264/android_r17c/armeabi-v7a X264_INCLUDE=${X264_PREFIX}/include X264_LIB=${X264_PREFIX}/lib FDK_AAC_PREFIX=$(pwd)/libfdkaac/android_r17c/armeabi-v7a FDK_AAC_INCLUDE=${FDK_AAC_PREFIX}/include FDK_AAC_LIB=${FDK_AAC_PREFIX}/lib OPENSSL_PREFIX=$(pwd)/libopenssl/android_r17c/armeabi-v7a OPENSSL_INCLUDE=${OPENSSL_PREFIX}/include OPENSSL_LIB=${OPENSSL_PREFIX}/lib //这里参照下面即可 ARCH=arm CPU=armv7-a CROSS_COMPILE=${TOOLCHAIN}/bin/arm-linux-androideabi- CC=${CROSS_COMPILE}gcc CXX=${CROSS_COMPILE}g++ CROSS_PREFIX=${CROSS_COMPILE} PREFIX=$(pwd)/android_r17c/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU\" CFLAG=\"-I${X264_INCLUDE} -I${FDK_AAC_INCLUDE} -I${OPENSSL_INCLUDE} -D__ANDROID_API__=$API --sysroot=${SYSROOT} -isysroot $NDK/sysroot -I$NDK/sysroot/usr/include/arm-linux-androideabi -Os -fPIC $OPTIMIZE_CFLAGS\" build_android FFmpeg在NDK21使用clang编译 这是针对NDK在版本r17c之后的。 #!/bin/bash API=21 NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk/android-ndk-r21 TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 X264_PREFIX=$(pwd)/libx264/android_r21/armeabi-v7a X264_INCLUDE=${X264_PREFIX}/include X264_LIB=${X264_PREFIX}/lib FDK_AAC_PREFIX=$(pwd)/libfdkaac/android_r21/armeabi-v7a FDK_AAC_INCLUDE=${FDK_AAC_PREFIX}/include FDK_AAC_LIB=${FDK_AAC_PREFIX}/lib OPENSSL_PREFIX=$(pwd)/libopenssl/android_r21/armeabi-v7a OPENSSL_INCLUDE=${OPENSSL_PREFIX}/include OPENSSL_LIB=${OPENSSL_PREFIX}/lib function build_android() { ./configure \\ --prefix=$PREFIX \\ --disable-opencl \\ --disable-doc \\ --disable-everything \\ --disable-htmlpages \\ --disable-podpages \\ --disable-debug \\ --disable-programs \\ --disable-demuxers \\ --disable-muxers \\ --disable-decoders \\ --disable-encoders \\ --disable-bsfs \\ --disable-indevs \\ --disable-outdevs \\ --disable-filters \\ --disable-protocols \\ --disable-hwaccels \\ --disable-avdevice \\ --disable-postproc \\ --disable-devices \\ --disable-symver \\ --disable-stripping \\ --disable-asm \\ --disable-w32threads \\ --disable-parsers \\ --disable-static \\ --enable-shared \\ --enable-openssl \\ --pkg-config=pkg-config \\ --enable-gpl \\ --enable-nonfree \\ --enable-libx264 \\ --enable-encoder=libx264 \\ --enable-swscale \\ --enable-protocol=file \\ --enable-protocol=hls \\ --enable-protocol=rtmp \\ --enable-protocol=http \\ --enable-protocol=tls \\ --enable-protocol=https \\ --enable-demuxer=aac \\ --enable-demuxer=h264 \\ --enable-demuxer=mov \\ --enable-demuxer=flv \\ --enable-demuxer=avi \\ --enable-demuxer=hevc \\ --enable-demuxer=gif \\ --enable-parser=h264 \\ --enable-parser=hevc \\ --enable-parser=aac \\ --enable-decoder=aac \\ --enable-decoder=h264 \\ --enable-decoder=flv \\ --enable-decoder=gif \\ --enable-decoder=mpeg4 \\ --enable-bsf=aac_adtstoasc \\ --enable-bsf=h264_mp4toannexb \\ --enable-jni \\ --enable-mediacodec \\ --enable-decoder=h264_mediacodec \\ --enable-hwaccel=h264_mediacodec \\ --enable-decoder=hevc_mediacodec \\ --enable-decoder=mpeg4_mediacodec \\ --enable-decoder=vp8_mediacodec \\ --enable-decoder=vp9_mediacodec \\ --enable-neon \\ --enable-gpl \\ --cross-prefix=$CROSS_PREFIX \\ --target-os=android \\ --arch=$ARCH \\ --cpu=$CPU \\ --cc=$CC \\ --cxx=$CXX \\ --enable-cross-compile \\ --sysroot=$SYSROOT \\ --extra-cflags=\"-I${X264_INCLUDE} -I${FDK_AAC_INCLUDE} -I${OPENSSL_INCLUDE} -Os -fpic $OPTIMIZE_CFLAGS\" \\ --extra-ldflags=\"-L${X264_LIB} -L${FDK_AAC_LIB} -L${OPENSSL_LIB} $ADDI_LDFLAGS\" || exit 1 make clean make -j8 make install } #armv8-a ARCH=arm64 CPU=armv8-a CC=$TOOLCHAIN/bin/aarch64-linux-android$API-clang CXX=$TOOLCHAIN/bin/aarch64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/aarch64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU\" #build_android #armv7-a ARCH=arm CPU=armv7-a CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- PREFIX=$(pwd)/android-so/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU \" build_android #x86 ARCH=x86 CPU=x86 CC=$TOOLCHAIN/bin/i686-linux-android$API-clang CXX=$TOOLCHAIN/bin/i686-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/i686-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=i686 -mtune=intel -mssse3 -mfpmath=sse -m32\" #build_android #x86_64 ARCH=x86_64 CPU=x86-64 CC=$TOOLCHAIN/bin/x86_64-linux-android$API-clang CXX=$TOOLCHAIN/bin/x86_64-linux-android$API-clang++ SYSROOT=$NDK/toolchains/llvm/prebuilt/darwin-x86_64/sysroot CROSS_PREFIX=$TOOLCHAIN/bin/x86_64-linux-android- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-march=$CPU -msse4.2 -mpopcnt -m64 -mtune=intel\" #build_android 导入到Android Studio中进行验证 1）在app级的build.gradle中设置动态库目录： android { defaultConfig { externalNativeBuild { cmake { cppFlags \"\" abiFilters 'armeabi-v7a' } } } externalNativeBuild { cmake { path \"src/main/cpp/CMakeLists.txt\" // version \"3.10.2\" //版本过高导致无日志输出 version \"3.6.0\" } } //配置第三方so路径 sourceSets { main { jniLibs.srcDirs = ['src/main/cpp/libs'] } } } dependencies { } 2）以下为CMakeList.txt导入方式： #1、添加版本 cmake_minimum_required(VERSION 3.4.1) include_directories(include) file(GLOB core_srcs ${CMAKE_SOURCE_DIR}/core/*.cpp) #2、编译库，把native-lib.cpp编译成以native-lib为名称的动态库 add_library( native-lib SHARED player_main.cpp ${core_srcs}) #3、设置C++编译参数（CMAKE_CXX_FLAGS是全局变量） #添加其他预编译库还可以使用这种方式 #使用-L指导编译时库文件的查找路径 #cxx_flags \"cxx_flags -L目录\" set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -L${CMAKE_SOURCE_DIR}/libs/${CMAKE_ANDROID_ARCH_ABI}\") #4、链接到库文件，jni/c/c++可以引入链接到 target_link_libraries( native-lib log # 先把有依赖的库，先依赖进来；注意顺序。 avformat avcodec avfilter avutil swresample swscale x264.161 fdk-aac ssl.3 crypto.3 #openssl z ) 3）在java类中进行加载 static { //动态库的操作 avformat avcodec avfilter avutil swresample swscale System.loadLibrary(\"avformat\"); System.loadLibrary(\"avcodec\"); System.loadLibrary(\"avfilter\"); System.loadLibrary(\"avutil\"); System.loadLibrary(\"swresample\"); System.loadLibrary(\"swscale\"); System.loadLibrary(\"x264.161\"); System.loadLibrary(\"fdk-aac\"); System.loadLibrary(\"ssl.3\"); System.loadLibrary(\"crypto.3\"); System.loadLibrary(\"native-lib\"); } 具体的请看Demo地址：FFmpegImportDemo 参考 https://github.com/taoliuh/ffmpeg-build-tools https://github.com/ShikinChen/android_ffmpeg https://github.com/hilive/build-script https://github.com/lllkey/android-openssl-build https://www.jianshu.com/p/f98db1d84d93 https://www.jianshu.com/p/f98db1d84d93 - 0基础学习音视频路线，以及重磅音视频资料下载 求赞。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/":{"url":"FFmpeg/","title":"FFmpeg","keywords":"","body":"FFmpeg简述FFmpeg的重要知识点FFmpeg 简述 音视频开发是绕不开FFmpeg的，因为它是一个\"集大成者\"，里面已经包含或可集成现代几乎所有的音视频技术（库）。 FFmpeg的重要知识点 Configure配置选项 (转载) Shell脚本 编译FFmpeg4.2.2 FFmpeg导入到Clion（MacOS） 使用Clion阅读FFmpeg源码（支持跳转） FFmpeg重要结构体（转载） Demuxing（解封装） Muxing（封装） Remuxing（重新封装） Decode（解码） Encode（编码） 简单实现转码 Filter和SDL（Video） Filter和SDL（Audio） Transcode(转码) 音视频同步处理 FFmpeg命令使用指南 Swscale（图像转换） FFmpeg添加字幕的详细操作 如何深入学习解封装？ Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-25 11:38:10 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/01_configure_help.html":{"url":"FFmpeg/01_configure_help.html","title":"Configure配置选项 (转载)","keywords":"","body":"Configure配置选项 (转载)帮助选项（Help options）标准选项（Standard options）许可证选项（Licensing options）配置选项（Configuration options）程序选项（Program options）文档选项（Documentation options）组件选项（Component options）个别组件选项（Individual component options）扩展库支持（External library support）硬件加速功能（hardware acceleration features）工具链选项（Toolchain options）高级选项（Advanced options）优化选项（Optimization options）开发者选项（Developer options）Configure配置选项 (转载) 帮助选项Help options 标准选项Standard options 许可证选项Licensing options 配置选项Configuration options 程序选项Program options 文档选项Documentation options 组件选项Component options 个别组件选项Individual component options 扩展库支持External library support 硬件加速功能hardware acceleration features 工具链选项Toolchain options 高级选项Advanced options 优化选项Optimization options 开发者选项Developer options 帮助选项（Help options） 用于查看ffmpeg的能力 选项 说明 –help print this message –list-decoders 显示所有可用的解码器（h264/mjpeg等） –list-encoders 显示所有可用的编码器（h264/mjpeg等） –list-hwaccels 显示所有支持的硬编解码器（h264_videotoolbox/h264_mediacodec等） –list-demuxers 显示所有支持解复用的容器（mp4/h264等） –list-muxers 显示所有支持复用的容器（mp4/h264等） –list-parsers show all available parsers –list-protocols 显示所有支持的传输协议（rtmp/rtp等） –list-bsfs 显示所有可用的格式转换（h264_mp4toannexb/aac_adtstoasc等） –list-indevs 显示所有支持的输入设备（alsa/v4l2等） –list-outdevs 显示所有支持的输出设备（alsa/opengl等） –list-filters 显示支持的所有过滤器（scale/volume/fps/allyuv等） 标准选项（Standard options） 编译配置 选项 说明 –logfile=FILE 配置过程中的log输出文件，默认输出到当前位置的ffbuild/config.log文件 –disable-logging 配置过程中不输出日志 –fatal-warnings 把配置过程中的任何警告当做致命的错误处理 –prefix=PREFIX 设定安装的跟目录，如果不指定默认是/usr –bindir=DIR 设置可执行程序的安装位置，默认是[PREFIX/bin] –datadir=DIR 设置测试程序以及数据的安装位置，默认是[PREFIX/share/ffmpeg] –docdir=DIR 设置文档的安装目录，默认是[PREFIX/share/doc/ffmpeg] –libdir=DIR 设置静态库的安装位置，默认是[PREFIX/lib] –shlibdir=DIR 设置动态库的安装位置，默认是[LIBDIR] –incdir=DIR 设置头文件的安装位置，默认是[PREFIX/include]；一般来说用于依赖此头文件来开发就够了 –mandir=DIR 设置man文件的安装目录，默认是[PREFIX/share/man] –pkgconfigdir=DIR 设置pkgconfig的安装目录，默认是[LIBDIR/pkgconfig]，只有--enable-shared使能的时候这个选项才有效 –enable-rpath use rpath to allow installing libraries in paths not part of the dynamic linker search path use rpath when linking programs [USE WITH CARE] –install-name-dir=DIR Darwin directory name for installed targets 许可证选项（Licensing options） 选择许可证，FFMPEG默认许可证LGPL 2.1，如果需要加gpl的库需要使用gpl的许可证，例如libx264就是gpl的，如果需要加入libx264则需要--enable-gpl。 选项 说明 –enable-gpl allow use of GPL code, the resulting libs and binaries will be under GPL [no] –enable-version3 upgrade (L)GPL to version 3 [no] –enable-nonfree allow use of nonfree code, the resulting libs and binaries will be unredistributable [no] 配置选项（Configuration options） 编译选项的配置 选项 说明 –disable-static 不生产静态库，默认生成静态库 –enable-shared 生成动态库，默认不生成动态库 –enable-small optimize for size instead of speed，默认开启 –disable-runtime-cpudetect disable detecting cpu capabilities at runtime (smaller binary)，默认开启 –enable-gray enable full grayscale support (slower color) –disable-swscale-alpha disable alpha channel support in swscale –disable-all 禁止编译所有库和可执行程序 –enable-raise-major 增加主版本号 程序选项（Program options） 可执行程序开启选项，默认编译ffmpeg中的所有可执行程序，包括ffmpeg、ffplay、ffprobe、ffserver，不过Mac平台默认情况下不生成ffplay，目前暂未知道啥原因。 选项 说明 –disable-programs do not build command line programs –disable-ffmpeg disable ffmpeg build –disable-ffplay disable ffplay build –disable-ffprobe disable ffprobe build –disable-ffserver disable ffserver build 文档选项（Documentation options） 离线文档选择 选项 说明 –disable-doc do not build documentation –disable-htmlpages do not build HTML documentation pages –disable-manpages do not build man documentation pages –disable-podpages do not build POD documentation pages –disable-txtpages do not build text documentation pages 组件选项（Component options） 除了avresample模块，默认编译所有模块。一般来说用于轻量化ffmpeg库的大小，可以仅仅开启指定某些组件的某些功能。 选项 说明 –disable-avdevice disable libavdevice build –disable-avcodec disable libavcodec build –disable-avformat disable libavformat build –disable-swresample disable libswresample build –disable-swscale disable libswscale build –disable-postproc disable libpostproc build –disable-avfilter disable libavfilter build –enable-avresample enable libavresample build [no] –disable-pthreads disable pthreads [autodetect] –disable-w32threads disable Win32 threads [autodetect] –disable-os2threads disable OS/2 threads [autodetect] –disable-network disable network support [no] –disable-dct disable DCT code –disable-dwt disable DWT code –disable-error-resilience disable error resilience code –disable-lsp disable LSP code –disable-lzo disable LZO decoder code –disable-mdct disable MDCT code –disable-rdft disable RDFT code –disable-fft disable FFT code –disable-faan disable floating point AAN (I)DCT code –disable-pixelutils disable pixel utils in libavutil 个别组件选项（Individual component options） 可以用于设定开启指定功能，例如禁止所有encoders，在这里可以开启特定的encoders（x264、aac等） 选项 说明 –disable-everything disable all components listed below –disable-encoder=NAME disable encoder NAME –enable-encoder=NAME enable encoder NAME –disable-encoders disable all encoders –disable-decoder=NAME disable decoder NAME –enable-decoder=NAME enable decoder NAME –disable-decoders disable all decoders –disable-hwaccel=NAME disable hwaccel NAME –enable-hwaccel=NAME enable hwaccel NAME –disable-hwaccels disable all hwaccels –disable-muxer=NAME disable muxer NAME –enable-muxer=NAME enable muxer NAME –disable-muxers disable all muxers –disable-demuxer=NAME disable demuxer NAME –enable-demuxer=NAME enable demuxer NAME –disable-demuxers disable all demuxers –enable-parser=NAME enable parser NAME –disable-parser=NAME disable parser NAME –disable-parsers disable all parsers –enable-bsf=NAME enable bitstream filter NAME –disable-bsf=NAME disable bitstream filter NAME –disable-bsfs disable all bitstream filters –enable-protocol=NAME enable protocol NAME –disable-protocol=NAME disable protocol NAME –disable-protocols disable all protocols –enable-indev=NAME enable input device NAME –disable-indev=NAME disable input device NAME –disable-indevs disable input devices –enable-outdev=NAME enable output device NAME –disable-outdev=NAME disable output device NAME –disable-outdevs disable output devices –disable-devices disable all devices –enable-filter=NAME enable filter NAME –disable-filter=NAME disable filter NAME –disable-filters disable all filters 扩展库支持（External library support） ffmpeg提供的一些功能是由其他扩展库支持的，如果需要使用需要明确声明，确定编译的第三方库的目标架构--arch相同就好了，在编译ffmpeg的时候需加入第三方库的头文和库搜索路径（通过extra-cflags和extra-ldflags指定即可），剩下的事ffmpeg都给你做好了。 libx264例子 ffmpeg集成libx264官方文档 $ git clone http://source.ffmpeg.org/git/ffmpeg.git $ cd x264 $ ./configure --prefix=$FFMPEG_PREFIX --enable-static --enable-shared $ make -j8 && make install 选项 说明 –enable-avisynth enable reading of AviSynth script files [no] –disable-bzlib disable bzlib [autodetect] –enable-chromaprint enable audio fingerprinting with chromaprint [no] –enable-frei0r enable frei0r video filtering [no] –enable-gcrypt enable gcrypt, needed for rtmp(t)e support if openssl, librtmp or gmp is not used [no] –enable-gmp enable gmp, needed for rtmp(t)e support if openssl or librtmp is not used [no] –enable-gnutls enable gnutls, needed for https support if openssl is not used [no] –disable-iconv disable iconv [autodetect] –enable-jni enable JNI support [no] –enable-ladspa enable LADSPA audio filtering [no] –enable-libass enable libass subtitles rendering, needed for subtitles and ass filter [no] –enable-libbluray enable BluRay reading using libbluray [no] –enable-libbs2b enable bs2b DSP library [no] –enable-libcaca enable textual display using libcaca [no] –enable-libcelt enable CELT decoding via libcelt [no] –enable-libcdio enable audio CD grabbing with libcdio [no] –enable-libdc1394 enable IIDC-1394 grabbing using libdc1394 and libraw1394 [no] –enable-libebur128 enable libebur128 for EBU R128 measurement, needed for loudnorm filter [no] –enable-libfdk-aac enable AAC de/encoding via libfdk-aac [no] –enable-libflite enable flite (voice synthesis) support via libflite [no] –enable-libfontconfig enable libfontconfig, useful for drawtext filter [no] –enable-libfreetype enable libfreetype, needed for drawtext filter [no] –enable-libfribidi enable libfribidi, improves drawtext filter [no] –enable-libgme enable Game Music Emu via libgme [no] –enable-libgsm enable GSM de/encoding via libgsm [no] –enable-libiec61883 enable iec61883 via libiec61883 [no] –enable-libilbc enable iLBC de/encoding via libilbc [no] –enable-libkvazaar enable HEVC encoding via libkvazaar [no] –enable-libmodplug enable ModPlug via libmodplug [no] –enable-libmp3lame enable MP3 encoding via libmp3lame [no] –enable-libnut enable NUT (de)muxing via libnut, native (de)muxer exists [no] –enable-libopencore-amrnb enable AMR-NB de/encoding via libopencore-amrnb [no] –enable-libopencore-amrwb enable AMR-WB decoding via libopencore-amrwb [no] –enable-libopencv enable video filtering via libopencv [no] –enable-libopenh264 enable H.264 encoding via OpenH264 [no] –enable-libopenjpeg enable JPEG 2000 de/encoding via OpenJPEG [no] –enable-libopenmpt enable decoding tracked files via libopenmpt [no] –enable-libopus enable Opus de/encoding via libopus [no] –enable-libpulse enable Pulseaudio input via libpulse [no] –enable-librubberband enable rubberband needed for rubberband filter [no] –enable-librtmp enable RTMP[E] support via librtmp [no] –enable-libschroedinger enable Dirac de/encoding via libschroedinger [no] –enable-libshine enable fixed-point MP3 encoding via libshine [no] –enable-libsmbclient enable Samba protocol via libsmbclient [no] –enable-libsnappy enable Snappy compression, needed for hap encoding [no] –enable-libsoxr enable Include libsoxr resampling [no] –enable-libspeex enable Speex de/encoding via libspeex [no] –enable-libssh enable SFTP protocol via libssh [no] –enable-libtesseract enable Tesseract, needed for ocr filter [no] –enable-libtheora enable Theora encoding via libtheora [no] –enable-libtwolame enable MP2 encoding via libtwolame [no] –enable-libv4l2 enable libv4l2/v4l-utils [no] –enable-libvidstab enable video stabilization using vid.stab [no] –enable-libvo-amrwbenc enable AMR-WB encoding via libvo-amrwbenc [no] –enable-libvorbis enable Vorbis en/decoding via libvorbis, native implementation exists [no] –enable-libvpx enable VP8 and VP9 de/encoding via libvpx [no] –enable-libwavpack enable wavpack encoding via libwavpack [no] –enable-libwebp enable WebP encoding via libwebp [no] –enable-libx264 enable H.264 encoding via x264 [no] –enable-libx265 enable HEVC encoding via x265 [no] –enable-libxavs enable AVS encoding via xavs [no] –enable-libxcb enable X11 grabbing using XCB [autodetect] –enable-libxcb-shm enable X11 grabbing shm communication [autodetect] –enable-libxcb-xfixes enable X11 grabbing mouse rendering [autodetect] –enable-libxcb-shape enable X11 grabbing shape rendering [autodetect] –enable-libxvid enable Xvid encoding via xvidcore, native MPEG-4/Xvid encoder exists [no] –enable-libzimg enable z.lib, needed for zscale filter [no] –enable-libzmq enable message passing via libzmq [no] –enable-libzvbi enable teletext support via libzvbi [no] –disable-lzma disable lzma [autodetect] –enable-decklink enable Blackmagic DeckLink I/O support [no] –enable-mediacodec enable Android MediaCodec support [no] –enable-netcdf enable NetCDF, needed for sofalizer filter [no] –enable-openal enable OpenAL 1.1 capture support [no] –enable-opencl enable OpenCL code –enable-opengl enable OpenGL rendering [no] –enable-openssl enable openssl, needed for https support if gnutls is not used [no] –disable-schannel disable SChannel SSP, needed for TLS support on Windows if openssl and gnutls are not used [autodetect] –disable-sdl2 disable sdl2 [autodetect] –disable-securetransport disable Secure Transport, needed for TLS support on OSX if openssl and gnutls are not used [autodetect] –enable-x11grab enable X11 grabbing (legacy) [no] –disable-xlib disable xlib [autodetect] –disable-zlib disable zlib [autodetect] 硬件加速功能（hardware acceleration features） ffmpeg默认实现了移动端（Android和IOS）的硬编解码，可以选择disable的都是默认开启的，可以关闭，可以选择enable的都是需要自己解决依赖的。 选项 说明 –disable-audiotoolbox disable Apple AudioToolbox code [autodetect] –enable-cuda enable dynamically linked Nvidia CUDA code [no] –enable-cuvid enable Nvidia CUVID support [autodetect] –disable-d3d11va disable Microsoft Direct3D 11 video acceleration code [autodetect] –disable-dxva2 disable Microsoft DirectX 9 video acceleration code [autodetect] –enable-libmfx enable Intel MediaSDK (AKA Quick Sync Video) code via libmfx [no] –enable-libnpp enable Nvidia Performance Primitives-based code [no] –enable-mmal enable Broadcom Multi-Media Abstraction Layer (Raspberry Pi) via MMAL [no] –disable-nvenc disable Nvidia video encoding code [autodetect] –enable-omx enable OpenMAX IL code [no] –enable-omx-rpi enable OpenMAX IL code for Raspberry Pi [no] –disable-vaapi disable Video Acceleration API (mainly Unix/Intel) code [autodetect] –disable-vda disable Apple Video Decode Acceleration code [autodetect] –disable-vdpau disable Nvidia Video Decode and Presentation API for Unix code [autodetect] –disable-videotoolbox disable VideoToolbox code [autodetect] 工具链选项（Toolchain options） ffmpeg代码本身是支持跨平台的，要编译不同的平台需要配置不同平台的交叉编译工具链。ffmpeg都是c代码，所以不需要配置c++的sysroot。常用的就几个arch,cpu,cross-prefix,enable-cross-compile,sysroot,target-os,extra-cflags,extra-ldflags,enable-pic。现在Android和IOS几乎没有armv5的设备了，所以如果编译这两个平台配置armv7和armv8就好了。 选项 说明 –arch=ARCH 选择目标架构[armv7a/aarch64/x86/x86_64等] –cpu=CPU 选择目标cpu[armv7-a/armv8-a/x86/x86_64] –cross-prefix=PREFIX 设定交叉编译工具链的前缀,不算gcc/nm/as命令，例如android 32位的交叉编译链$ndk_dir/toolchains/arm-linux-androideabi-$toolchain_version/prebuilt/linux-$host_arch/bin/arm-linux-androideabi- –progs-suffix=SUFFIX program name suffix [] –enable-cross-compile 如果目标平台和编译平台不同则需要使能它 –sysroot=PATH 交叉工具链的头文件和库位，例如Android 32位位置$ndk_dir/platforms/android-14/arch-arm –sysinclude=PATH location of cross-build system headers –target-os=OS 设置目标系统 –target-exec=CMD command to run executables on target –target-path=DIR path to view of build directory on target –target-samples=DIR path to samples directory on target –tempprefix=PATH force fixed dir/prefix instead of mktemp for checks –toolchain=NAME set tool defaults according to NAME –nm=NM use nm tool NM [nm -g] –ar=AR use archive tool AR [ar] –as=AS use assembler AS [] –ln_s=LN_S use symbolic link tool LN_S [ln -s -f] –strip=STRIP use strip tool STRIP [strip] –windres=WINDRES use windows resource compiler WINDRES [windres] –yasmexe=EXE use yasm-compatible assembler EXE [yasm] –cc=CC use C compiler CC [gcc] –cxx=CXX use C compiler CXX [g++] –objcc=OCC use ObjC compiler OCC [gcc] –dep-cc=DEPCC use dependency generator DEPCC [gcc] –ld=LD use linker LD [] –pkg-config=PKGCONFIG use pkg-config tool PKGCONFIG [pkg-config] –pkg-config-flags=FLAGS pass additional flags to pkgconf [] –ranlib=RANLIB use ranlib RANLIB [ranlib] –doxygen=DOXYGEN use DOXYGEN to generate API doc [doxygen] –host-cc=HOSTCC use host C compiler HOSTCC –host-cflags=HCFLAGS use HCFLAGS when compiling for host –host-cppflags=HCPPFLAGS use HCPPFLAGS when compiling for host –host-ld=HOSTLD use host linker HOSTLD –host-ldflags=HLDFLAGS use HLDFLAGS when linking for host –host-libs=HLIBS use libs HLIBS when linking for host –host-os=OS compiler host OS [] –extra-cflags=ECFLAGS 设置cflags，如果是Android平台可以根据ndk内的设定,arm-linux-androideabi-4.6/setup.mk，建议参考你当前的setup来配置 –extra-cxxflags=ECFLAGS add ECFLAGS to CXXFLAGS [] –extra-objcflags=FLAGS add FLAGS to OBJCFLAGS [] –extra-ldflags=ELDFLAGS 参考cflags –extra-ldexeflags=ELDFLAGS add ELDFLAGS to LDEXEFLAGS [] –extra-ldlibflags=ELDFLAGS add ELDFLAGS to LDLIBFLAGS [] –extra-libs=ELIBS add ELIBS [] –extra-version=STRING version string suffix [] –optflags=OPTFLAGS override optimization-related compiler flags –build-suffix=SUFFIX library name suffix [] –enable-pic build position-independent code –enable-thumb compile for Thumb instruction set –enable-lto use link-time optimization –env=”ENV=override” override the environment variables 高级选项（Advanced options） 选项 说明 –malloc-prefix=PREFIX prefix malloc and related names with PREFIX –custom-allocator=NAME use a supported custom allocator –disable-symver disable symbol versioning –enable-hardcoded-tables use hardcoded tables instead of runtime generation –disable-safe-bitstream-reader disable buffer boundary checking in bitreaders (faster, but may crash) –enable-memalign-hack emulate memalign, interferes with memory debuggers –sws-max-filter-size=N the max filter size swscale uses [256] 优化选项（Optimization options） 默认开启各个平台的汇编优化，有些嵌入式平台可能并不能完整的支持架构的所有汇编指令，所以需要关闭。（自己理解的，没有实战） 选项 说明 –disable-asm disable all assembly optimizations –disable-altivec disable AltiVec optimizations –disable-vsx disable VSX optimizations –disable-power8 disable POWER8 optimizations –disable-amd3dnow disable 3DNow! optimizations –disable-amd3dnowext disable 3DNow! extended optimizations –disable-mmx disable MMX optimizations –disable-mmxext disable MMXEXT optimizations –disable-sse disable SSE optimizations –disable-sse2 disable SSE2 optimizations –disable-sse3 disable SSE3 optimizations –disable-ssse3 disable SSSE3 optimizations –disable-sse4 disable SSE4 optimizations –disable-sse42 disable SSE4.2 optimizations –disable-avx disable AVX optimizations –disable-xop disable XOP optimizations –disable-fma3 disable FMA3 optimizations –disable-fma4 disable FMA4 optimizations –disable-avx2 disable AVX2 optimizations –disable-aesni disable AESNI optimizations –disable-armv5te disable armv5te optimizations –disable-armv6 disable armv6 optimizations –disable-armv6t2 disable armv6t2 optimizations –disable-vfp disable VFP optimizations –disable-neon disable NEON optimizations –disable-inline-asm disable use of inline assembly –disable-yasm disable use of nasm/yasm assembly –disable-mipsdsp disable MIPS DSP ASE R1 optimizations –disable-mipsdspr2 disable MIPS DSP ASE R2 optimizations –disable-msa disable MSA optimizations –disable-mipsfpu disable floating point MIPS optimizations –disable-mmi disable Loongson SIMD optimizations –disable-fast-unaligned consider unaligned accesses slow 开发者选项（Developer options） 调试用的一些开关 选项 说明 –disable-debug disable debugging symbols –enable-debug=LEVEL set the debug level [] –disable-optimizations disable compiler optimizations –enable-extra-warnings enable more compiler warnings –disable-stripping disable stripping of executables and shared libraries –assert-level=level 0(default), 1 or 2, amount of assertion testing, 2 causes a slowdown at runtime. –enable-memory-poisoning fill heap uninitialized allocated space with arbitrary data –valgrind=VALGRIND run “make fate” tests through valgrind to detect memory leaks and errors, using the specified valgrind binary. Cannot be combined with –target-exec –enable-ftrapv Trap arithmetic overflows –samples=PATH location of test samples for FATE, if not set use $FATE_SAMPLES at make invocation time. –enable-neon-clobber-test check NEON registers for clobbering (should be used only for debugging purposes) –enable-xmm-clobber-test check XMM registers for clobbering (Win64-only; should be used only for debugging purposes) –enable-random randomly enable/disable components –disable-random –enable-random=LIST randomly enable/disable specific components or –disable-random=LIST component groups. LIST is a comma-separated list of NAME[:PROB] entries where NAME is a component (group) and PROB the probability associated with –random-seed=VALUE seed value for –enable/disable-random –disable-valgrind-backtrace do not print a backtrace under Valgrind (only applies to –disable-optimizations builds) 转自：https://blog.csdn.net/momo0853/article/details/78043903 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/02_shell.html":{"url":"FFmpeg/02_shell.html","title":"Shell脚本","keywords":"","body":"Shell脚本简介Shell脚本权限Shell基本语法Shell其他关键词Shell脚本 简介 Shell脚本（Shell Script）是一种为Shell编写的脚本程序。而Shell是一个用C语言编写的程序，它是用户使用Linux的桥梁。在Linux中Shell程序又有很多种，如： sh - 即 Bourne Shell。sh 是 Unix 标准默认的 shell。 bash - 即 Bourne Again Shell。bash 是 Linux 标准默认的 shell。 …… 其中，bash由于易用和免费，被广泛使用。每种Shell程序都有其对应的解释器，在脚本文件第一行会告诉系统当前程序属于那种类型，如： #!/bin/bash echo \"这是bash类型的shell程序\" 本文主要介绍bash类型的Shell脚本的基本使用。 Shell脚本权限 Shell脚本被执行前需要可执行权限。如刚创建的test.sh脚本，需chmod +x test.sh方能执行： qincji:giteeblog mac$ ./test.sh -bash: ./test.sh: Permission denied qincji:giteeblog mac$ chmod +x test.sh qincji:giteeblog mac$ ./test.sh Hellow Shell 注：执行脚本文件的命令使用sh或.，如sh test.sh或./test.sh。 Shell基本语法 注释 单行注释 - 以 # 开头，到行尾结束。 多行注释 - 以 : 开头，到 EOF 结束。 如编辑test.sh脚本文件 #!/bin/bash echo \"Hellow Shell\" #echo \"使用#单行注释了，不能输出\" : 执行sh test.sh查看输出： qincji:giteeblog mac$ sh test.sh Hellow Shell echo输出 echo用于字符串的输出。编辑test.sh进行介绍： #!/bin/bash echo \"1-You\" #1.输出普通字符串 echo \"2-You \\\"are\\\"\" #2.输出使用\\转义 age=18 echo \"3-You \\\"are\\\" ${age}\" #3.使用${变量名}输出包含变量的字符串 echo \"4-You \\\"are\\\" \"$age\"\" #4.使用\"$变量名\"输出包含变量的字符串\" echo -e \"5-YES\\nNO\" #5.使用 -e 开启（对\\n）转义 echo \"6-test\" > test.txt #6.输出重定向至文件 执行后输出结果： qincji:giteeblog mac$ ./test.sh 1-You 2-You \"are\" 3-You \"are\" 18 4-You \"are\" 18 5-YES NO qincji:giteeblog mac$ cat test.txt 6-test 变量 Bash 中没有数据类型，bash 中的变量可以保存一个数字、一个字符、一个字符串等等。定义变量名的规则： 1.命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 2.中间不能有空格，可以使用下划线（_）。 3.不能使用标点符号。 4.不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 如，编辑test.sh如下： #!/bin/bash #有效变量名 var=\"0 字符串\" VAR=1 _var=2 _var2=3 _3var=4 #以下为无效变量名 4var=5 ?var=6 var*2=7 #使用表达式命令赋值，输出看看命令的结果 files=`du -sh .` echo ${files} #删除变量 unset var echo ${var} 执行输出结果： qincji:giteeblog mac$ ./test.sh ./test.sh: line 9: 4var=5: command not found ./test.sh: line 10: ?var=6: command not found ./test.sh: line 11: var*2=7: command not found 138M . 字符串 使用'或者\"来定义字符串，也可以不用引号。但是'中不能使用变量和转义符。所以记住用\"就行了。字符串常规操作，有： 1.字符串拼接，格式：'hello，'${串}''或\"hello，${串}\"。 2.获取字符串长度，格式：${#串}。 3.截取字符串，格式${串:2:3}，其中2为起点，3为截取的个数。 编辑test.sh如下： #!/bin/bash name=\"123456\" full1='1-hello，'${name}'' #1.单引号字符串拼接 full2=\"1-hello，${name}\" #1.双引号字符串拼接 echo ${full1} echo ${full2} echo \"2-长度：${#name}\" #2.获取字符串长度 echo \"3-截取：${name:2:3}\" #3.截取字符串 执行输出结果： qincji:giteeblog mac$ ./test.sh 1-hello，123456 1-hello，123456 2-长度：6 3-截取：345 数组 Bash只支持一维数组，下标从 0 开始，下标可以是整数或算术表达式，其值应大于或等于 0。数组常规操作，有： 1.创建数组，格式：array=(value0 value1 value2)或指定下标array=([1]=value0 [1]=value1 [2]=value2)。 2.访问元素，格式：获取单个：\"${array[下标]}\"；获取所有：\"${array[@]}\"。 3.取得个数，格式：\"${#array[@]}\"或\"${#array[*]}\"。 4.增加元素，格式：array=(value3 \"${array[@]}\" value4)。 5.删除元素，格式：unset array[0]。 编辑test.sh如下： #!/bin/bash array=([1]=1 [0]=\"a\" [3]=\"￥\") #1.创建数组 echo \"2-访问：v1=${array[1]}，v2=${array[@]}\" #2.访问元素 echo \"3-获取：len1=${#array[@]}，len2=${#array[*]}\" #3.取得个数 array=(\"NO\" \"${array[@]}\" 33) #4.增加元素 echo \"4-增加：${array[@]}\" unset array[0] #5.删除元素 echo \"5-删除：${array[@]}\" 执行输出结果： qincji:giteeblog mac$ ./test.sh 2-访问：v1=1，v2=a 1 ￥ 3-获取：len1=3，len2=3 4-增加：NO a 1 ￥ 33 5-删除：a 1 ￥ 33 运算符 算术运算符 + - / %（加减乘除取余），格式：expr value0 符号 value1，特殊符号需要转义，如`expr 3 \\ 4`结果为12。 =（赋值），格式：value=value1，把value1赋值给value。 ==（比较相等），格式：[ value1 == value2 ]，如[ 2 == 5 ]，返回false。 !=（比较不相等），格式：[ value1 != value2 ]，如[ 2 != 5 ]，返回true。 注意：[ value1 != 'value2' ]是带空格的，如错误写法：[value1!='value2']。在循环体测试中使用[]不起作用，需要用(())，如： a=1 while (($a 编辑test.sh如下： #!/bin/bash a=3 b=3 o1=`expr ${a} \\* ${b}` echo \"o1=${o1}\" if [ $a == $b ] then echo \"a 等于 b\" else echo \"a 不等于 b\" fi 执行输出结果： qincji:giteeblog mac$ ./test.sh o1=9 a 等于 b 关系运算符 通用格式为：[ value1 符号 value2 ]，用于比较结构中，同上==和!=用法一样。 -eq 是否相等，格式：[ value1 -eq value2 ]，如[ 2 -eq 5 ]，返回false。 -ne 是否不相等，格式：同上。 -gt >，格式：同上。 -lt -ge >=，格式：同上。 -le 布尔运算符 ! 非，格式：[ ! 表达式 ]。与表示式结果相反。。如[ ! 1 == 1 ]返回 false。 -o 或 (与逻辑运算符||相同)，格式：[ 表达式1 -o 表达式2 ]。有一个表达式为 true 则返回 true。如[ 1 == 1 -o 2 != 2 ]返回 true。 -a 与 (与逻辑运算符&&相同)，格式：[ 表达式1 -o 表达式2 ]。两个表达式都为 true 才返回 true。如[ 1 == 1 -a 2 != 2 ]返回 false。 逻辑运算符 ||和&&参考布尔运算符。 字符串运算符 = 字符串相等，格式：[ value1 = value2 ]，如a=\"a\",b=\"b\"，[ ${a} = ${b} ]，返回true。 != 字符串不相等，格式：[ value1 != value2 ]，如a=\"a\",b=\"b\"，[ ${a} != ${b} ]，返回false。 -z 字符串长度为0为true，格式：[ -z value ]，如a=\"a\"，[ -z ${a} ]，返回false。 -n 字符串长度不为0为true， 格式：[ -n = value ]，如a=\"a\"，[ -n = ${a} ]，返回true。 文件运算符 通用格式为：[符号 文件路径 ]，用于检查文件的属性，如何符合为true。 -d 目录存在为true，格式：[ -d filePath ]，如[ -d \"./\" ]，为true。 -f 文件存在为true，格式：[ -f filePath ]，如[ -f \"./test.sh\" ]，为true。 -r 可读为true，格式：同上。 -w 可写为true，格式：同上。 -x 可执行 可执行为true，格式：同上。 -s 文件不为空(有内容) true，格式：同上。 -e 文件/目录存在，格式：同上。 编辑test.sh如下： #!/bin/bash if [ -d \"./\" ] then echo \"1-输出为true\" else echo \"1-输出为false\" fi if [ -w \"./test.sh\" ] then echo \"2-输出为true\" else echo \"2-输出为false\" fi 执行输出结果： qincji:giteeblog mac$ ./test.sh 1-输出为true 2-输出为true 流程控制 条件语句： ①if..elif..else，使用then..fi来控制命中的范围。单行格式：if [ 条件1 ]; then 执行体1; elif [ 条件2 ]; then 执行体2; else 执行体2; fi，多行格式：直接;去掉即可。 ②case，在exec..esac范围内。格式如下 exec case 输入 in 条件1) 执行体1;; 条件2) 执行体2;; *) 其他执行体;; esac 循环语句：for、while和until。循环体在do..done范围内，在循环中遇到continue会进入下个循环，遇到break跳出循环。 编辑test.sh如下： #!/bin/bash #1.注意单行的写法需要用 ； 把语句段落隔开 if [ \"111\" = \"abc\" ]; then echo \"输出if-1\" elif [ \"abc\" = \"abc\" ]; then echo \"输出elif-1\" else echo \"输出else-1\" fi #2.注意单行的写法需要用 ； 把语句段落隔开 if [ \"111\" = \"abc\" ]; then echo \"输出if-2\"; elif [ \"abc2\" = \"abc\" ]; then echo \"输出elif-2\"; else echo \"输出else-2\"; fi #3.case语句 exec case \"4\" in \"a\") echo \"case输出a\" ;; \"b\") echo \"case输出b\" ;; *) echo \"case输出*\" ;; esac #4.for语句 for var in \"red\" 2 \"yes\"; do echo \"for语句：${var}\"; done #5.while语句 a=1 #while [ $a -le 10 ] while (($a = 6)); do echo \"until语句：$a\" let \"a++\" done 执行输出结果： qincji:giteeblog mac$ ./test.sh 输出elif-1 输出else-2 case输出* for语句：red for语句：2 for语句：yes while语句：1 while语句：continue while语句：3 while语句：break until语句：4 until语句：5 函数 编辑test.sh如下： #!/bin/bash #function关键字可省了，fun_name随便定的函数名称。 function fun_name(){ echo $0 #$0：文件名称 echo $1 #$1：传递的第一个参数 echo $2 #$2：传递的第二个参数 } fun_name 1 \"Yes\" 执行输出结果： qincji:giteeblog mac$ ./test.sh ./test.sh 1 Yes Shell其他关键词 eval 语法：eval cmdLine，eval会对后面的cmdLine进行两遍扫描，如果在第一遍扫面后cmdLine是一个普通命令，则执行此命令；如果cmdLine中含有变量的间接引用，则保证简介引用的语义。示例如下： 编辑test.sh如下： #!/bin/bash #文件传递的长度，输出最后一个数 function test() { echo \"\\{% math %}#\" #这才是我们想要的结果 eval echo \"\\{% endmath %}#\" } test 11 22 33 44 执行输出结果： qincji:giteeblog mac$ ./test.sh $4 44 shift 语法：shift number，位置参数可以用shift命令左移。比如shift 3表示原来的$4现在变成$1，原来的$5现在变成$2等等，原来的$1、$2、$3丢弃，$0不移动。不带参数的shift命令相当于shift 1。示例如下： 编辑test.sh如下： #!/bin/bash #文件传递的长度，输出最后一个数 function test() { until [ $# -eq 0 ]; do echo \"第一个参数为: $1 参数个数为: $#\" shift 2 done } test 11 22 33 44 55 66 执行输出结果： qincji:giteeblog mac$ ./test.sh 第一个参数为: 11 参数个数为: 6 第一个参数为: 33 参数个数为: 4 第一个参数为: 55 参数个数为: 2 参考 https://www.cnblogs.com/jingmoxukong/p/7867397.html Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/03_build_ffmpeg.html":{"url":"FFmpeg/03_build_ffmpeg.html","title":"编译FFmpeg4.2.2","keywords":"","body":"编译FFmpeg4.2.2前言FFmpeg编译流程FFmpeg基本组成结构选择模块进行编译准备测试1：仅仅编译出ffplay程序测试2：根据测试1中去掉支持rtmp协议测试3：集成第三方库：x264测试4：android交叉编译总结编译FFmpeg4.2.2 前言 在编译FFmpeg之前，我们得先知道FFmpeg包含了那些内容（组件），我们应该要如何查看并选择？这里我们就简单来说说FFmpeg编译的过程，以及集成x264，编译android平台所需要的动态库等。（这里的测试使用的是Mac系统） FFmpeg编译流程 编译过程主要分为两步（下图）： (1)configure：通过configure --help查看我们所能选择的配置。前往查看configure配置选项注释。这一步后会生成许多Makefile编译所需要的东西。其中在 ffbuild/config.log 可查看当前执行的日志。 (2)make install：编译生成我们所配置的东西。如果之前编译过需要make clean清除之前编译过的数据再执行make install。 注：生成的执行文件或可移植类库时根据系统会不一样，比如生成ffplay时：Unix系统会是ffplay，windows会是ffplay.exe。 FFmpeg基本组成结构 从configure配置文件中阅读可知，FFmpeg基本组成结构可以下部分（下图）： 主要由AVCodec（编解码相关）、AVDevice（输入/输出设备相关）、AVFilter（滤波处理相关）和AVFormat（数据格式处理相关）四大基本模块构成。在这四大模块下又细分了一些小模块，这里对小模块作用简单做一下说明： bsfs：格式转换。通过./configure --list-bsfs查看所有能支持转换的格式。 decoders：解码器。通过./configure --list-decoders查看所有能支持解码器。 encoders：编码器。通过./configure --list-encoders查看所有能支持编码器。 hwaccels：硬件编解码器。通过./configure --list-hwaccels查看所有能支持硬件编解码器。 parsers：解析器。通过./configure --list-parsers查看所有能支持解析器。 indevs：输入设备（如安卓摄像头）。通过./configure --list-indevs查看所有能支持输入设备。 outdevs：输出设备（如opengl）。通过./configure --list-outdevs查看所有能支持输出设备。 filters：滤镜处理器（如gblur高斯模糊）。通过./configure --list-filters查看所有能支持滤镜处理器。 muxers：封装（如把flv容器格式视频拆解出视频流、音频流等）。通过./configure --list-muxers查看所有能支持封装。 demuxers：解封装（对应封装）。通过./configure --list-demuxers查看所有能支持解封装。 protocols：协议（如rtmp）。通过./configure --list-protocols查看所有能支持的协议。 选择模块进行编译 上面简单介绍了ffmpeg的编译流程以及ffmpeg库到底包含了那些类库。知道了这些，我们就可以选择我们所需要的类库进行编译了。如果默认编译出来的ffmpeg将会是相当庞大的，像一下动态库移植，库太大就很烦恼了。而且像x264库等一些第三方库也是需要知道如何进行配置的。这里我们的就探讨一下如何选择我们所需要的类库进行编译。 准备 (1)下载ffmpeg源文件 ，根据自己的系统进行选择，下载回来后进行解压。 (2)参考官方文档 ，如：Mac需要按照Xcode等，编译也应该先看官网文档介绍。（最靠谱的方式吧？） (3)掌握Shell基础知识 ，为了方便修改与编译。 (4)下载x264 ，《测试3》需要用。 (5)下载NDK ，注意系统的类型。《测试4》需要用。 测试1：仅仅编译出ffplay程序 configure配置文件中默认是输出所有程序的（ffmpeg、ffplay和ffprobe）。但是因为ffplay需要sdl2库，所以这里讲一下如何配置单单输出ffplay程序。 (1)安装sdl2 从configure配置文件中得知，ffplay程序需要依赖sdl2库。Mac可通过执行brew install sdl2命令进行安装。 (2)编译验证ffplay 在项目的根目录编写build.sh脚本文件并授权，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash ./configure \\ --disable-programs \\ --enable-ffplay make clean make install 然后在控制台中看到输出信息，发现只有ffplay程序将要被编译。 等待十几分钟后，我们发现在项目的根目录仅仅生成ffplay可执行文件，我们在控制台执行du -sh ./ffplay命令看看生成文件的大小有20M。 qincji:ffmpeg mac$ du -sh ./ffplay 20M ./ffplay 执行命令播放一个rtmp：./ffplay -window_title \"版本1\" rtmp://slive.ytn.co.kr/dmb/ydlive_20140419_1，结果如下： 好了，我们编译的ffplay播放器能正常使用了。使用./ffplay --help查看具体用法。 (3)查找这样配置的原因 上面我们已经知道了编译结果，这里需要找一下原因，这到底为何？我们从configure文件找到一些蛛丝马迹： #略…………………… #program队列 PROGRAM_LIST=\" ffplay ffprobe ffmpeg \" #略…………………… #disable方法，将调用set_all，并传值为no，以及将要设置disable的名称($*)。 disable(){ set_all no $* } #略…………………… set_all(){ value=$1 shift for var in $*; do eval $var=$value done } : 测试2：根据测试1中去掉支持rtmp协议 在测试1中我们是把程序模块的先全部取消编译，然后再选择其中一个。这里我们选择仅仅去掉一个模块中的单个组件。我们举一个例子，取消支持protocols中的rtmp协议： (1)如何入手？ 我们知道一切编译皆来自configure配置文件，所以我们得从这个文件找我们需要的信息。我刚开始找rtmp相关信息，只是找到librtmp库的启用或取消，然后编译了一下，发现并不是！ 然后我就搜索protocols，发现有一条关键信息，如下： #单个组件配置 Individual component options: #略………… #这就是我想要的！！后面NAME是rtmp！如何知道？通过 ./configure --list-protocols 看到所有支持协议的名称 --disable-protocol=NAME disable protocol NAME --disable-protocols disable all protocols (2)编译验证结果 修改build.sh脚本，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash ./configure \\ --disable-programs \\ --enable-ffplay \\ --disable-protocol=rtmp make clean make install 然后在控制台中看到输出信息，发现Enabled protocols:中已经没有了rtmp协议了！ 等待编译完成，执行命令播放一个rtmp：./ffplay -window_title \"版本2\" rtmp://slive.ytn.co.kr/dmb/ydlive_20140419_1，结果如下： 开启或者禁用其他组件的道理也是一样哦。 测试3：集成第三方库：x264 ffmpeg官方mac编译指南集成libx264 ，但是给出的信息不是很明确（有些我不太懂），几经波折后发现，是需要指定CFLAGS和LDFLAGS（也就是--extra-cflags和--extra-ldflags）才编译通过的。这里给出编译脚本，在编写前我们把下载下来解压后的x264库改名为libx264然后放进ffmpeg项目根目录。修改build.sh脚本，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash function build_x264() { cd libx264 ./configure \\ --prefix=${X264_LIBS} \\ --enable-static make clean make install } function build_ffmpeg() { cd .. ./configure \\ --disable-programs \\ --enable-ffmpeg \\ --enable-gpl \\ --extra-cflags=\"-I${X264_LIBS}/include\" \\ --extra-ldflags=\"-L${X264_LIBS}/lib\" \\ --enable-libx264 make clean make install } #设置x264变成出来的静态库保存路径，然后编译ffmpeg时，链接进去 X264_LIBS=$(pwd)/libx264/libouput build_x264 build_ffmpeg 编译过程中，我们留意到控制台的到输出信息，发现libx264协议被启用了： 等待编译成功后，验证当前生成的ffmpeg程序是否已经集成了libx264，找个mp4视频放到当前目录，命名为input.mp4。然后执行以下命令，如果没报错说了已成功集成： ./ffmpeg -re -i input.mp4 -vcodec libx264 -an output.mp4 测试4：android交叉编译 ffmpeg官网android编译指南 ， 测试了一下，最后发现这几个参数比较关键： --enable-cross-compile : 开启交叉编译。 --cross-prefix : gcc的前缀。（如果使用clang编译则可以不给） --target-os : 指定android使用平台。 --arch : 处理器类型。 --cpu : cpu类型。 --cc : c语言编译器（给当前指定的绝对路径）。 --cxx : c++语言编译器（给当前指定的绝对路径）。 --extra-cflags : 给传递给编译器的参数。 修改build.sh脚本，如下。然后在控制台中直接执行./build.sh。 #!/bin/bash API=21 export NDK=/Users/Qincji/Desktop/develop/android/source/sdk/ndk-bundle export SYSROOT=$NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot export TOOLCHAIN=$NDK/toolchains/llvm/prebuilt/darwin-x86_64 function build_android() { ./configure \\ --prefix=$PREFIX \\ --disable-programs \\ --disable-static \\ --enable-shared \\ --cross-prefix=$CROSS_PREFIX \\ --target-os=android \\ --arch=$ARCH \\ --cpu=$CPU \\ --cc=$CC \\ --cxx=$CXX \\ --enable-cross-compile \\ --extra-cflags=\"$CFLAG\" || exit 0 make clean make install } ARCH=arm CPU=armv7-a CC=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang CXX=$TOOLCHAIN/bin/armv7a-linux-androideabi$API-clang++ CROSS_PREFIX=$TOOLCHAIN/bin/arm-linux-androideabi- PREFIX=$(pwd)/android/$CPU OPTIMIZE_CFLAGS=\"-mfloat-abi=softfp -mfpu=vfp -marm -march=$CPU\" CFLAG=\"-Os -fpic $OPTIMIZE_CFLAGS\" build_android 等待编译完成，我们发现能成功编译出动态库： 总结 好了，ffmpeg编译已经基本讲述完了。这里总结一下我所编译过程的一些想法： 使用IDE管理项目，自己处理的方便就行。我这里用clion，github有破解方式。 configure 文件非常重要，shell语法还是要懂一点。 报错了注意查看日志ffbuild/config.log。 路径一定要看是否真实存在，如$CC可能在一些API版本上xxx-clang是没有的。 参考 https://blog.csdn.net/leixiaohua1020/article/details/44587465 https://blog.csdn.net/leixiaohua1020/article/details/44556525 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/04_import_ffmpeg.html":{"url":"FFmpeg/04_import_ffmpeg.html","title":"FFmpeg导入到Clion（MacOS）","keywords":"","body":"FFmpeg导入到Clion（MacOS）编译MacOS平台的FFmpeg动态库将生成的库导入到Clion进行验证FFmpeg导入到Clion（MacOS） 继如何编译FFmpeg后，本章也是基于前面来讲解如何把编译后的库文件移入到Clion。 在编译ffmpeg前，有一点需要提前说明的，Mac系统并不支持链接静态库，详细请看初步认识c/c++编译 这篇文章。针对于平台的差异性，CLion工具在Mac系统中使用CMakelist.txt进行交叉编译并不友好，我试过集成Android平台的ffmpeg，最终以失败而告终，主要报错原因是：无法指向NDK中的 ld 链接器（要是哪位朋友尝试编译通过还望告知）。所以后续如果是Android项目还是老老实实使用Android Studio吧。 编译MacOS平台的FFmpeg动态库 在ffmpeg项目根目录编写build.sh脚本文件如下： #!/bin/bash make clean function build_macosx() { ./configure \\ --prefix=$PREFIX \\ --disable-programs \\ --target-os=darwin \\ --disable-static \\ --pkg-config=$(which pkg-config) \\ --enable-shared || exit 0 make clean make install } PREFIX=$(pwd)/macox build_macosx 然后在控制台上执行编译脚本./build.sh，建议使用IDE，如我使用CLion导入ffmpeg源码就行操作如下： 等待编译成功后，会在当前目录的macox文件夹生成动态库，如下： 将生成的库导入到Clion （1）在Clion新建C++项目，项目名称为VAFFmpeg。 （2）将生成的include和lib复制到项目跟目录。如图下图所示： （3）编写CMakeLists.txt文件，分别引入刚刚复制过来的头文件和动态库，如下： cmake_minimum_required(VERSION 3.17) project(VAFFmpeg) set(CMAKE_CXX_STANDARD 11) set(SOURCE_FILES main.cpp) link_directories(./lib/) include_directories(./include/) add_executable(VAFFmpeg ${SOURCE_FILES}) target_link_libraries( VAFFmpeg avcodec avdevice avfilter avformat ) 进行验证 编写main.cpp文件如下： #include extern \"C\" { #include } using namespace std; int main() { cout \" \" 执行能正常输出，如下： Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/05_source.html":{"url":"FFmpeg/05_source.html","title":"使用Clion阅读FFmpeg源码（支持跳转）","keywords":"","body":"使用Clion阅读FFmpeg源码（支持跳转）前言方案一（简单）步骤1步骤2步骤3简单查看方案二（需要编译源码）步骤1（同方案一）步骤2（同方案一）步骤3步骤4使用Clion阅读FFmpeg源码（支持跳转） 前言 本方案仅仅适用于阅读FFmpeg，配置方式及其简单，能支持方法间的跳转，但由于配置原因部分无法识别或跳转，介意者勿入！！ 方案一（简单） 此方案非常简单，能阅读绝大部分源码了，但仍会缺失一些头文件，导致无法小小部分无法查阅。 步骤1 在Clion新建c++项目项目File->New Project->Create，如下图： 步骤2 将FFmpeg源码复制到根目录，如下图： 步骤3 编写CMakeLists.txt文件，如下： cmake_minimum_required(VERSION 3.17) project(SourceFFmpeg) set(CMAKE_CXX_STANDARD 11) file(GLOB EX_DIR ./*.cpp ./*.c ./*/*.c ./*/*/*.c ./*/*/*/*.c) set(INCLUDE_DIR ./ ./*/ ./*/*/ ./*/*/*/ ./*/*/*/*/) include_directories(${INCLUDE_DIR}) add_executable(SourceFFmpeg ${EX_DIR}) 简单查看 到这已经完成了，这里我们查看一下源码之间的跳转，以及一些方法的被调用，可以从doc/examples例子看看源码能不能正常跳转等，我这如下： 方案二（需要编译源码） 解决方案一中一些头文件缺失问题。 步骤1（同方案一） 步骤2（同方案一） 步骤3 编译源码生成动态库/静态库，目的需要头文件，然后把生成的头文件复制到项目中，如下： 步骤4 编写CMakeLists.txt文件，如下： cmake_minimum_required(VERSION 3.17) project(SourceFFmpeg) set(CMAKE_CXX_STANDARD 11) include_directories(./include/) file(GLOB EX_DIR ./*.cpp ./*.c ./*/*.c ./*/*/*.c ./*/*/*/*.c) add_executable(SourceFFmpeg ${EX_DIR}) 到这里方案二的也完成了，可自行查阅。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/06_struct.html":{"url":"FFmpeg/06_struct.html","title":"FFmpeg重要结构体（转载）","keywords":"","body":"FFmpeg重要结构体（转载）结构体间联系a)解协议（http,rtsp,rtmp,mms）b)解封装（flv,avi,rmvb,mp4）c)解码（h264,mpeg2,aac,mp3）d)存数据AVFrame （libavutil/frame.h）AVFormatContext （libavformat/avformat.h）AVStream （libavformat/avformat.h）AVIOContext （libavformat/avio.h）AVCodecContext （libavcodec/avcodec.h）AVCodec （libavcodec/avcodec.h）AVPacket （libavcodec/avcodec.h）FFmpeg重要结构体（转载） 本文主要整合FFMPEG中最关键的结构体之间的关系 系列文章，以便于后面学习以及资料的查阅。 结构体间联系 a)解协议（http,rtsp,rtmp,mms） AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”） b)解封装（flv,avi,rmvb,mp4） AVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装格式都对应一个AVInputFormat 结构。 c)解码（h264,mpeg2,aac,mp3） 每个AVStream存储一个视频/音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频/音频流使用解码方式的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频/音频对应的解码器。每种解码器都对应一个AVCodec结构。 d)存数据 视频的话，每个结构一般是存一帧；音频可能有好几帧; 解码前数据：AVPacket 解码后数据：AVFrame AVFrame （libavutil/frame.h） AVFrame是包含码流参数较多的结构体。AVFrame结构体一般用于存储原始数据（即非压缩数据，例如对视频来说是YUV，RGB，对音频来说是PCM），此外还包含了一些相关的信息。比如说，解码的时候存储了宏块类型表，QP表，运动矢量表等数据。编码的时候也存储了相关的数据。因此在使用FFMPEG进行码流分析的时候，AVFrame是一个很重要的结构体。 #define AV_NUM_DATA_POINTERS 8 uint8_t *data[AV_NUM_DATA_POINTERS]：解码后原始数据（对视频来说是YUV、RGB，对音频来说是PCM） int linesize[AV_NUM_DATA_POINTERS]：data中“一行”数据的大小。注意：未必等于图像的宽，一般大于图像的宽。 int width, height：视频帧宽和高（1920x1080,1280x720...） int nb_samples：音频的一个AVFrame中可能包含多个音频帧，在此标记包含了几个 int format：解码后原始数据类型（YUV420，YUV422，RGB24...），未知或未设置为-1。 int key_frame：是否是关键帧 enum AVPictureType pict_type：帧类型（I,B,P...） AVRational sample_aspect_ratio：宽高比（16:9，4:3...） int64_t pts：显示时间戳 int coded_picture_number：编码帧序号 int display_picture_number：显示帧序号 int8_t *qscale_table：QP表 int channels：音频通道数 int interlaced_frame：是否是隔行扫描 其中，sample_aspect_ratio宽高比是一个分数，AVRational结构体： typedef struct AVRational{ int num; /// QP表 qscale_table : QP表指向一块内存，里面存储的是每个宏块的QP值。宏块的标号是从左往右，一行一行的来的。每个宏块对应1个QP。qscale_table[0]就是第1行第1列宏块的QP值；qscale_table[1]就是第1行第2列宏块的QP值；qscale_table[2]就是第1行第3列宏块的QP值。以此类推... 宏块的个数用下式计算(注：宏块大小是16x16的)，每行宏块数： int mb_stride = pCodecCtx->width/16+1 宏块的总数： int mb_sum = ((pCodecCtx->height+15)>>4)*(pCodecCtx->width/16+1) AVFormatContext （libavformat/avformat.h） 在使用FFMPEG进行开发的时候，AVFormatContext是一个贯穿始终的数据结构，很多函数都要用到它作为参数。它是FFMPEG解封装（flv，mp4，rmvb，avi）功能的结构体。（在这里考虑解码的情况） AVInputFormat *iformat：输入容器格式数据 AVOutputFormat *oformat：输出容器格式数据 AVIOContext *pb：输入数据的缓存 AVIOContext *pb：输入数据的缓存 unsigned int nb_streams：视音频流的个数 AVStream **streams：视音频流 char filename[1024]：文件名 char *url：输入/输出URL int64_t duration：时长（单位：微秒us，转换为秒需要除以1000000） int bit_rate：比特率（单位bps，转换为kbps需要除以1000） int packet_size：packet的长度 AVDictionary *metadata：元数据 其中，通过av_dict_get()函数获得视频的元数据。封装在了AVDictionary和AVDictionaryEntry： struct AVDictionary { int count; AVDictionaryEntry *elems; }; typedef struct AVDictionaryEntry { char *key; char *value; } AVDictionaryEntry; AVStream （libavformat/avformat.h） AVStream是存储每一个视频/音频流信息的结构体。 int index：标识该视频/音频流(AVFormatContext中) AVCodecContext *codec：指向该视频/音频流的AVCodecContext（它们是一一对应的关系） AVRational time_base：时基。通过该值可以把PTS，DTS转化为真正的时间。FFMPEG其他结构体中也有这个字段，但是根据我的经验，只有AVStream中的time_base是可用的。PTS*time_base=真正的时间 int64_t duration：该视频/音频流长度 int64_t nb_frames：该流中已知时的帧数 AVDictionary *metadata：元数据信息 AVRational avg_frame_rate：帧率（注：对视频来说，这个挺重要的） AVPacket attached_pic：附带的图片。比如说一些MP3，AAC音频文件附带的专辑封面。 AVIOContext （libavformat/avio.h） AVIOContext是FFMPEG管理输入输出数据的结构体 unsigned char *buffer：缓存开始位置 int buffer_size：缓存大小（默认32768） unsigned char *buf_ptr：当前指针读取到的位置 unsigned char *buf_end：缓存结束的位置 void *opaque：URLContext结构体 其中，opaque指向的URLContext： typedef struct URLContext { const AVClass *av_class; /** URLContext结构体中还有一个结构体URLProtocol。注：每种协议（rtp，rtmp，file等）对应一个URLProtocol。这个结构体也不在FFMPEG提供的头文件中。从FFMPEG源代码中翻出其的定义： typedef struct URLProtocol { const char *name; int (*url_open)( URLContext *h, const char *url, int flags); int (*url_open2)(URLContext *h, const char *url, int flags, AVDictionary **options); int (*url_accept)(URLContext *s, URLContext **c); int (*url_handshake)(URLContext *c); int (*url_read)( URLContext *h, unsigned char *buf, int size); int (*url_write)(URLContext *h, const unsigned char *buf, int size); int64_t (*url_seek)( URLContext *h, int64_t pos, int whence); int (*url_close)(URLContext *h); int (*url_read_pause)(URLContext *h, int pause); int64_t (*url_read_seek)(URLContext *h, int stream_index, int64_t timestamp, int flags); int (*url_get_file_handle)(URLContext *h); int (*url_get_multi_file_handle)(URLContext *h, int **handles, int *numhandles); int (*url_get_short_seek)(URLContext *h); int (*url_shutdown)(URLContext *h, int flags); int priv_data_size; const AVClass *priv_data_class; int flags; int (*url_check)(URLContext *h, int mask); int (*url_open_dir)(URLContext *h); int (*url_read_dir)(URLContext *h, AVIODirEntry **next); int (*url_close_dir)(URLContext *h); int (*url_delete)(URLContext *h); int (*url_move)(URLContext *h_src, URLContext *h_dst); const char *default_whitelist; } URLProtocol; AVCodecContext （libavcodec/avcodec.h） AVCodecContext是包含变量较多的结构体（感觉差不多是变量最多的结构体）。本文将会大概分析一下该结构体里每个变量的含义和作用。（这里只考虑解码） enum AVMediaType codec_type：编解码器的类型（视频，音频，字幕...） struct AVCodec *codec：采用的解码器AVCodec（H.264,MPEG2...） int bit_rate：平均比特率 uint8_t *extradata; int extradata_size：针对特定编码器包含的附加信息（例如对于H.264解码器来说，存储SPS，PPS等） AVRational time_base：根据该参数，可以把PTS转化为实际的时间（单位为秒s） int width, height：如果是视频的话，代表宽和高 int refs：运动估计参考帧的个数（H.264的话会有多帧，MPEG2这类的一般就没有了） int sample_rate：采样率（音频） int channels：声道数（音频） enum AVSampleFormat sample_fmt：采样格式 int frame_size：帧中每个通道的采样率（音频） int profile：型（H.264里面就有，其他编码标准应该也有） int level：级（和profile差不太多） 其中，1.编解码器类型：codec_type enum AVMediaType { AVMEDIA_TYPE_UNKNOWN = -1, /// 2.在FFMPEG中音频采样格式：sample_fmt enum AVSampleFormat { AV_SAMPLE_FMT_NONE = -1, AV_SAMPLE_FMT_U8, /// 3.FFMPEG中型：profile #define FF_PROFILE_UNKNOWN -99 #define FF_PROFILE_RESERVED -100 #define FF_PROFILE_AAC_MAIN 0 #define FF_PROFILE_AAC_LOW 1 #define FF_PROFILE_AAC_SSR 2 #define FF_PROFILE_AAC_LTP 3 #define FF_PROFILE_AAC_HE 4 #define FF_PROFILE_AAC_HE_V2 28 #define FF_PROFILE_AAC_LD 22 #define FF_PROFILE_AAC_ELD 38 #define FF_PROFILE_DTS 20 #define FF_PROFILE_DTS_ES 30 #define FF_PROFILE_DTS_96_24 40 #define FF_PROFILE_DTS_HD_HRA 50 #define FF_PROFILE_DTS_HD_MA 60 #define FF_PROFILE_MPEG2_422 0 #define FF_PROFILE_MPEG2_HIGH 1 #define FF_PROFILE_MPEG2_SS 2 #define FF_PROFILE_MPEG2_SNR_SCALABLE 3 #define FF_PROFILE_MPEG2_MAIN 4 #define FF_PROFILE_MPEG2_SIMPLE 5 #define FF_PROFILE_H264_CONSTRAINED (1 AVCodec （libavcodec/avcodec.h） AVCodec是存储编解码器信息的结构体。 const char *name：编解码器的名字，比较短 const char *long_name：编解码器的名字，全称，比较长 enum AVMediaType type：指明了类型，是视频，音频，还是字幕 enum AVCodecID id：ID，不重复 const AVRational *supported_framerates：支持的帧率（仅视频） const enum AVPixelFormat *pix_fmts：支持的像素格式（仅视频） const int *supported_samplerates：支持的采样率（仅音频） const enum AVSampleFormat *sample_fmts：支持的采样格式（仅音频） const uint64_t *channel_layouts：支持的声道数（仅音频） int priv_data_size：私有数据的大小 其中，AVMediaType结构体： enum AVMediaType { AVMEDIA_TYPE_UNKNOWN = -1, /// AVCodecID结构体： enum AVCodecID { AV_CODEC_ID_NONE, /* video codecs */ AV_CODEC_ID_MPEG1VIDEO, AV_CODEC_ID_MPEG2VIDEO, /// AVPixelFormat结构体： enum AVPixelFormat { AV_PIX_FMT_NONE = -1, AV_PIX_FMT_YUV420P, /// AVPacket （libavcodec/avcodec.h） AVPacket是存储压缩编码数据相关信息的结构体。例如对于H.264来说。1个AVPacket的data通常对应一个NAL。注意：在这里只是对应，而不是一模一样。他们之间有微小的差别：使用FFMPEG类库分离出多媒体文件中的H.264码流。因此在使用FFMPEG进行视音频处理的时候，常常可以将得到的AVPacket的data数据直接写成文件，从而得到视音频的码流文件。 int64_t pts：显示时间戳 int64_t dts：解码时间戳 uint8_t *data：压缩编码的数据 int size：data的大小 int stream_index：标识该AVPacket所属的视频/音频流。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/08_demuxing.html":{"url":"FFmpeg/08_demuxing.html","title":"Demuxing（解封装）","keywords":"","body":"Demuxing（解封装）FFmpeg解封装流程（1）avformat_open_input（2）avformat_find_stream_info（3）av_read_frame（4）avformat_close_input例子测试文件下载地址Demuxing（解封装） FFmpeg解封装流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转自雷神） 。 （1）avformat_open_input 创建并初始化AVFormatContext结构体，并把输入文件信息赋值到AVFormatContext中。 （2）avformat_find_stream_info 检索流信息，这个过程会检查输入流中信息是否存在异常，如：AVCodecContext中的extradata是否存在。 （3）av_read_frame 从文件中读取每一帧的数据到AVPacket中，得到解封装之前的数据。有些（很多吧？）解封装后的数据输出到一个文件中并不支持播放，如FLV。因为FLV解封装后的数据并不是完整一个H264格式视频数据和AAC格式音频数据，需要重新进行封装后再输出到文件中才能正常播放。（或者使用FFmpeg中的工具） （4）avformat_close_input 关闭并释放资源。 例子 参考官方例子doc/examples/demuxing_decoding.c。 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/03 * description: FFmpeg Demuxing（解封装） */ extern \"C\" { #include } int main() { const char *src_filename = \"source/Kobe.flv\"; const char *out_filename_v = \"output/kobe3.h264\";//Output file URL const char *out_filename_a = \"output/kobe.mp3\"; AVFormatContext *fmt_ctx; /**(1)*/ if (avformat_open_input(&fmt_ctx, src_filename, NULL, NULL) streams[video_idx]->codec->extradata包含了sps和pps的数据，需要组装成NALU /**(3)*/ while (av_read_frame(fmt_ctx, &pkt) >= 0) { AVPacket orig_pkt = pkt; //针对每一个AVPacket进行处理。 if(orig_pkt.stream_index == video_idx){ //封装成 NALU 形式的 av_bitstream_filter_filter(h264bsfc, fmt_ctx->streams[video_idx]->codec, NULL, &orig_pkt.data, &orig_pkt.size, orig_pkt.data, orig_pkt.size, 0); fprintf(stderr,\"Write Video Packet. size:%d\\tpts:%lld\\n\", orig_pkt.size, orig_pkt.pts); fwrite(orig_pkt.data, 1, orig_pkt.size, fp_video); } else if(orig_pkt.stream_index == audio_idx){ fprintf(stderr,\"Write Audio Packet. size:%d\\tpts:%lld\\n\", orig_pkt.size, orig_pkt.pts); } av_packet_unref(&orig_pkt); } av_bitstream_filter_close(h264bsfc); /**(4)*/ avformat_close_input(&fmt_ctx); } 测试文件下载地址 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/09_muxing.html":{"url":"FFmpeg/09_muxing.html","title":"Muxing（封装）","keywords":"","body":"Muxing（封装）FFmpeg解封装流程（1）avformat_alloc_output_context2（2）avformat_new_stream（3）avformat_write_header（4）av_interleaved_write_frame（5）av_write_trailer代码实现测试文件下载地址: Kobe.aac 、 Kobe.h264 。Muxing（封装） 本文主要参考雷神的最简单的基于FFmpeg的封装格式处理：视音频复用器（muxer） 与官方例子doc/examples/muxing.c 。 FFmpeg解封装流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 以上步骤主要方法的简单说明。 （1）avformat_alloc_output_context2 构造输出的AVFormatContext。 （2）avformat_new_stream 将新的流添加到AVFormatContext中。 （3）avformat_write_header 写入文件头 （4）av_interleaved_write_frame 通过解封装文件（或者创建）得到的AVPacket数据写入到文件中。 （5）av_write_trailer 写入文件尾 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/04 * description: FFmpeg 封装 */ #include #define __STDC_CONSTANT_MACROS #ifdef _WIN32 //Windows extern \"C\" { #include \"libavformat/avformat.h\" }; #else //Linux... #ifdef __cplusplus extern \"C\" { #endif #include #ifdef __cplusplus }; #endif #endif /* FIX: H.264 in some container format (FLV, MP4, MKV etc.) need \"h264_mp4toannexb\" bitstream filter (BSF) *Add SPS,PPS in front of IDR frame *Add start code (\"0,0,0,1\") in front of NALU H.264 in some container (MPEG2TS) don't need this BSF. */ //'1': Use H.264 Bitstream Filter #define USE_H264BSF 0 /* FIX:AAC in some container format (FLV, MP4, MKV etc.) need \"aac_adtstoasc\" bitstream filter (BSF) */ //'1': Use AAC Bitstream Filter #define USE_AACBSF 0 int main(int argc, char *argv[]) { AVOutputFormat *ofmt = NULL; //Input AVFormatContext and Output AVFormatContext AVFormatContext *ifmt_ctx_v = NULL, *ifmt_ctx_a = NULL, *ofmt_ctx = NULL; AVPacket pkt; int ret, i; int videoindex_v = -1, videoindex_out = -1; int audioindex_a = -1, audioindex_out = -1; int frame_index = 0; int64_t cur_pts_v = 0, cur_pts_a = 0; int writing_v = 1, writing_a = 1; const char *in_filename_v = \"source/kobe.h264\"; const char *in_filename_a = \"source/kobe.aac\"; const char *out_filename = \"output/kobe.flv\";//Output file URL remove(out_filename); //Input if ((ret = avformat_open_input(&ifmt_ctx_v, in_filename_v, 0, 0)) oformat; for (i = 0; i nb_streams; i++) { //Create output AVStream according to input AVStream if (ifmt_ctx_v->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO) { AVStream *out_stream = avformat_new_stream(ofmt_ctx, ifmt_ctx_v->streams[i]->codec->codec); videoindex_v = i; if (!out_stream) { printf(\"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } videoindex_out = out_stream->index; //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, ifmt_ctx_v->streams[i]->codec) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; break; } } for (i = 0; i nb_streams; i++) { //Create output AVStream according to input AVStream if (ifmt_ctx_a->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO) { AVStream *out_stream = avformat_new_stream(ofmt_ctx, ifmt_ctx_a->streams[i]->codec->codec); audioindex_a = i; if (!out_stream) { printf(\"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } audioindex_out = out_stream->index; //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, ifmt_ctx_a->streams[i]->codec) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; break; } } /* open the output file, if needed */ if (!(ofmt->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", out_filename, av_err2str(ret)); goto end; } } //Write file header if (avformat_write_header(ofmt_ctx, NULL) streams[videoindex_v]->time_base, cur_pts_a, ifmt_ctx_a->streams[audioindex_a]->time_base) = 0) { do { in_stream = ifmt_ctx->streams[pkt.stream_index]; out_stream = ofmt_ctx->streams[stream_index]; if (pkt.stream_index == videoindex_v) { //FIX：No PTS (Example: Raw H.264) //Simple Write PTS if (pkt.pts == AV_NOPTS_VALUE) { //Write PTS AVRational time_base1 = in_stream->time_base; //Duration between 2 frames (us) int64_t calc_duration = (double) AV_TIME_BASE / av_q2d(in_stream->r_frame_rate); //Parameters pkt.pts = (double) (frame_index * calc_duration) / (double) (av_q2d(time_base1) * AV_TIME_BASE); pkt.dts = pkt.pts; pkt.duration = (double) calc_duration / (double) (av_q2d(time_base1) * AV_TIME_BASE); frame_index++; } cur_pts_v = pkt.pts; break; } } while (av_read_frame(ifmt_ctx, &pkt) >= 0); } else { writing_v = 0; continue; } } else { ifmt_ctx = ifmt_ctx_a; stream_index = audioindex_out; if (av_read_frame(ifmt_ctx, &pkt) >= 0) { do { in_stream = ifmt_ctx->streams[pkt.stream_index]; out_stream = ofmt_ctx->streams[stream_index]; if (pkt.stream_index == audioindex_a) { //FIX：No PTS //Simple Write PTS if (pkt.pts == AV_NOPTS_VALUE) { //Write PTS AVRational time_base1 = in_stream->time_base; //Duration between 2 frames (us) int64_t calc_duration = (double) AV_TIME_BASE / av_q2d(in_stream->r_frame_rate); //Parameters pkt.pts = (double) (frame_index * calc_duration) / (double) (av_q2d(time_base1) * AV_TIME_BASE); pkt.dts = pkt.pts; pkt.duration = (double) calc_duration / (double) (av_q2d(time_base1) * AV_TIME_BASE); frame_index++; } cur_pts_a = pkt.pts; break; } } while (av_read_frame(ifmt_ctx, &pkt) >= 0); } else { writing_a = 0; continue; } } //Convert PTS/DTS pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding) (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding) (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base); pkt.pos = -1; pkt.stream_index = stream_index; printf(\"Write 1 Packet. size:%5d\\tpts:%lld\\n\", pkt.size, pkt.pts); //Write if (av_interleaved_write_frame(ofmt_ctx, &pkt) flags & AVFMT_NOFILE)) avio_close(ofmt_ctx->pb); avformat_free_context(ofmt_ctx); if (ret 测试文件下载地址: Kobe.aac 、 Kobe.h264 。 参考 https://blog.csdn.net/leixiaohua1020/article/details/39802913 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/10_remuxing.html":{"url":"FFmpeg/10_remuxing.html","title":"Remuxing（重新封装）","keywords":"","body":"Remuxing（重新封装）FFmpeg解封装流程官方例子测试文件下载地址: Kobe.flv 。Remuxing（重新封装） 本文来自官方例子doc/examples/remuxing.c 。 FFmpeg解封装流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 重新封装的原理就是把输入文件 解封装 之后，再进行 封装 输出到新的文件中。 官方例子 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/04 * description: FFmpeg 重新封装 */ #define __STDC_CONSTANT_MACROS extern \"C\" { #include #include } static void log_packet(const AVFormatContext *fmt_ctx, const AVPacket *pkt, const char *tag) { AVRational *time_base = &fmt_ctx->streams[pkt->stream_index]->time_base; printf(\"%s: pts:%s pts_time:%s dts:%s dts_time:%s duration:%s duration_time:%s stream_index:%d\\n\", tag, av_ts2str(pkt->pts), av_ts2timestr(pkt->pts, time_base), av_ts2str(pkt->dts), av_ts2timestr(pkt->dts, time_base), av_ts2str(pkt->duration), av_ts2timestr(pkt->duration, time_base), pkt->stream_index); } int main(int argc, char **argv) { AVOutputFormat *ofmt = NULL; AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL; AVPacket pkt; const char *in_filename, *out_filename; int ret, i; int stream_index = 0; int *stream_mapping = NULL; int stream_mapping_size = 0; in_filename = \"source/Kobe.flv\"; out_filename = \"output/Kobe.mp4\"; if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) nb_streams; stream_mapping = static_cast(av_mallocz_array(stream_mapping_size, sizeof(*stream_mapping))); if (!stream_mapping) { ret = AVERROR(ENOMEM); goto end; } ofmt = ofmt_ctx->oformat; for (i = 0; i nb_streams; i++) { AVStream *out_stream; AVStream *in_stream = ifmt_ctx->streams[i]; AVCodecParameters *in_codecpar = in_stream->codecpar; if (in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO && in_codecpar->codec_type != AVMEDIA_TYPE_VIDEO && in_codecpar->codec_type != AVMEDIA_TYPE_SUBTITLE) { stream_mapping[i] = -1; continue; } stream_mapping[i] = stream_index++; out_stream = avformat_new_stream(ofmt_ctx, NULL); if (!out_stream) { fprintf(stderr, \"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar); if (ret codecpar->codec_tag = 0; } av_dump_format(ofmt_ctx, 0, out_filename, 1); if (!(ofmt->flags & AVFMT_NOFILE)) { ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE); if (ret streams[pkt.stream_index]; if (pkt.stream_index >= stream_mapping_size || stream_mapping[pkt.stream_index] streams[pkt.stream_index]; log_packet(ifmt_ctx, &pkt, \"in\"); /* copy packet */ pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, static_cast(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, static_cast(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX)); pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base); pkt.pos = -1; log_packet(ofmt_ctx, &pkt, \"out\"); ret = av_interleaved_write_frame(ofmt_ctx, &pkt); if (ret flags & AVFMT_NOFILE)) avio_closep(&ofmt_ctx->pb); avformat_free_context(ofmt_ctx); av_freep(&stream_mapping); if (ret 测试文件下载地址: Kobe.flv 。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/11_decode.html":{"url":"FFmpeg/11_decode.html","title":"Decode（解码）","keywords":"","body":"Decode（解码）FFmpeg解码流程官方例子【Audio】官方例子【Video】Decode（解码） 本文来自官方例子doc/examples/decode_audio.c 和 doc/examples/decode_video.c。 FFmpeg解码流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 官方例子【Audio】 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/06 * description: 来自官方例子：doc/examples/decode_audio.c */ #include #include #include extern \"C\"{ #include #include #include } #define AUDIO_INBUF_SIZE 20480 #define AUDIO_REFILL_THRESH 4096 static void decode(AVCodecContext *dec_ctx, AVPacket *pkt, AVFrame *frame, FILE *outfile) { int i, ch; int ret, data_size; //把AVPacket数据送去解码器 ret = avcodec_send_packet(dec_ctx, pkt); if (ret = 0) { //接到解码后的，读取到AVFrame中 ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret sample_fmt); if (data_size nb_samples; i++) for (ch = 0; ch channels; ch++) fwrite(frame->data[ch] + data_size*i, 1, data_size, outfile); } } int main(int argc, char **argv) { const char *outfilename, *filename; const AVCodec *codec; AVCodecContext *c= NULL; AVCodecParserContext *parser = NULL; int len, ret; FILE *f, *outfile; uint8_t inbuf[AUDIO_INBUF_SIZE + AV_INPUT_BUFFER_PADDING_SIZE]; uint8_t *data; size_t data_size; AVPacket *pkt; AVFrame *decoded_frame = NULL; filename = \"source/Kobe.aac\"; outfilename = \"output/Kebe.pcm\"; pkt = av_packet_alloc(); //通过AVCodecID查找到相应的AVCodec，类型需要输入文件解码类型保持一致 codec = avcodec_find_decoder(AV_CODEC_ID_AAC); if (!codec) { fprintf(stderr, \"Codec not found\\n\"); exit(1); } //获取解析器上下文 parser = av_parser_init(codec->id); if (!parser) { fprintf(stderr, \"Parser not found\\n\"); exit(1); } //获取解码器器上下文 c = avcodec_alloc_context3(codec); if (!c) { fprintf(stderr, \"Could not allocate audio codec context\\n\"); exit(1); } //初始化码器器上下文 if (avcodec_open2(c, codec, NULL) 0) { if (!decoded_frame) { if (!(decoded_frame = av_frame_alloc())) { fprintf(stderr, \"Could not allocate audio frame\\n\"); exit(1); } } //把输入的文件流进行解析，输出到AVPacket中 ret = av_parser_parse2(parser, c, &pkt->data, &pkt->size, data, data_size, AV_NOPTS_VALUE, AV_NOPTS_VALUE, 0); if (ret size){ //这部分主要是把上面AVPacket数据进行解码，最后得到裸流数据输出到AVFrame中 decode(c, pkt, decoded_frame, outfile); } if (data_size 0) data_size += len; } } /* flush the decoder */ pkt->data = NULL; pkt->size = 0; decode(c, pkt, decoded_frame, outfile); fclose(outfile); fclose(f); avcodec_free_context(&c); av_parser_close(parser); av_frame_free(&decoded_frame); av_packet_free(&pkt); return 0; } 测试文件下载地址: Kobe.aac 。 解析后的pcm原始数据可以使用ffplay或者audition等工具播放，而播放前必须先知道该原始数据的初始化信息，如通道数、位宽等，可以使用ffprobe进行查看解码前的aac文件，如：qincji:usr mac$ ffprobe ./VAFFmpeg/source/Kobe.aac ffprobe version 4.2.2 Copyright (c) 2007-2019 the FFmpeg developers built with Apple clang version 11.0.0 (clang-1100.0.33.12) configuration: libavutil 56. 31.100 / 56. 31.100 libavcodec 58. 54.100 / 58. 54.100 libavformat 58. 29.100 / 58. 29.100 libavdevice 58. 8.100 / 58. 8.100 libavfilter 7. 57.100 / 7. 57.100 libswscale 5. 5.100 / 5. 5.100 libswresample 3. 5.100 / 3. 5.100 [aac @ 0x7fec2b005200] Estimating duration from bitrate, this may be inaccurate Input #0, aac, from '/Users/Qincji/Desktop/develop/android/project/va/VAFFmpeg/source/Kobe.aac': Duration: 00:00:29.90, bitrate: 34 kb/s Stream #0:0: Audio: aac (HE-AAC), 44100 Hz, stereo, fltp, 34 kb/s 使用audition输入出文件实例： 官方例子【Video】 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/06 * description: 来自官方例子：doc/examples/decode_video.c */ #include #include #include extern \"C\"{ #include } #define INBUF_SIZE 4096 static void decode(AVCodecContext *dec_ctx, AVFrame *frame, AVPacket *pkt, FILE *outfile) { int ret; ret = avcodec_send_packet(dec_ctx, pkt); if (ret = 0) { ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret frame_number); fflush(stdout); //根据自己解码出来的数据格式进行处理 fwrite(frame->data[0],1,frame->width*frame->height,outfile);//Y fwrite(frame->data[1],1,frame->width*frame->height/4,outfile);//U fwrite(frame->data[2],1,frame->width*frame->height/4,outfile);//V } } int main(int argc, char **argv) { const char *filename, *outfilename; const AVCodec *codec; AVCodecParserContext *parser; AVCodecContext *c= NULL; FILE *f; AVFrame *frame; uint8_t inbuf[INBUF_SIZE + AV_INPUT_BUFFER_PADDING_SIZE]; uint8_t *data; size_t data_size; int ret; AVPacket *pkt; filename = \"source/Kobe.h264\"; outfilename = \"output/Kobe.yuv\"; FILE *outfile = fopen(outfilename, \"wb\"); pkt = av_packet_alloc(); if (!pkt) exit(1); /* set end of buffer to 0 (this ensures that no overreading happens for damaged MPEG streams) */ memset(inbuf + INBUF_SIZE, 0, AV_INPUT_BUFFER_PADDING_SIZE); /* find the MPEG-1 video decoder */ codec = avcodec_find_decoder(AV_CODEC_ID_H264); if (!codec) { fprintf(stderr, \"Codec not found\\n\"); exit(1); } parser = av_parser_init(codec->id); if (!parser) { fprintf(stderr, \"parser not found\\n\"); exit(1); } c = avcodec_alloc_context3(codec); if (!c) { fprintf(stderr, \"Could not allocate video codec context\\n\"); exit(1); } /* For some codecs, such as msmpeg4 and mpeg4, width and height MUST be initialized there because this information is not available in the bitstream. */ /* open it */ if (avcodec_open2(c, codec, NULL) 0) { ret = av_parser_parse2(parser, c, &pkt->data, &pkt->size, data, data_size, AV_NOPTS_VALUE, AV_NOPTS_VALUE, 0); if (ret size) decode(c, frame, pkt, outfile); } } /* flush the decoder */ decode(c, frame, NULL, outfile); fclose(f); av_parser_close(parser); avcodec_free_context(&c); av_frame_free(&frame); av_packet_free(&pkt); return 0; } 测试文件下载地址: Kobe.h264 。 解析后的yuv原始数据可以使用ffplay等工具播放，而播放前必须先知道该原始数据的宽高等信息，可以使用ffprobe进行查看解码前的h264文件，如：qincji:usr mac$ ffprobe /Users/Qincji/Desktop/develop/android/project/va/VAFFmpeg/source/Kobe.h264 ffprobe version 4.2.2 Copyright (c) 2007-2019 the FFmpeg developers built with Apple clang version 11.0.0 (clang-1100.0.33.12) configuration: libavutil 56. 31.100 / 56. 31.100 libavcodec 58. 54.100 / 58. 54.100 libavformat 58. 29.100 / 58. 29.100 libavdevice 58. 8.100 / 58. 8.100 libavfilter 7. 57.100 / 7. 57.100 libswscale 5. 5.100 / 5. 5.100 libswresample 3. 5.100 / 3. 5.100 Input #0, h264, from '/Users/Qincji/Desktop/develop/android/project/va/VAFFmpeg/source/Kobe.h264': Duration: N/A, bitrate: N/A Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 15 fps, 15 tbr, 1200k tbn, 30 tbc 使用ffplay播放： ffplay -f rawvideo -video_size 384x216 output/Kobe.yuv 输入出文件实例： Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/12_encode.html":{"url":"FFmpeg/12_encode.html","title":"Encode（编码）","keywords":"","body":"Encode（编码）FFmpeg编码流程官方例子【Audio】官方例子【Video】Encode（编码） 本文来自官方例子doc/examples/encode_audio.c 和 doc/examples/encode_video.c。 FFmpeg编码流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体（转载） 。 官方例子【Audio】 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/07 * description: 来自官方例子：doc/examples/encode_audio.c * 主要思路： * 1、初始化编码器信息 * 2、从文件（或者生成）读取裸流数据到每个AVFrame中 * 3、把每个AVFrame通过编码器生成编码后数据的AVPacket * 4、使用AVFormatContext封装到文件中进行输出 * * */ #include #include #include extern \"C\"{ #include #include #include #include #include #include } /* check that a given sample format is supported by the encoder */ static int check_sample_fmt(const AVCodec *codec, enum AVSampleFormat sample_fmt) { const enum AVSampleFormat *p = codec->sample_fmts; while (*p != AV_SAMPLE_FMT_NONE) { if (*p == sample_fmt) return 1; p++; } return 0; } /* just pick the highest supported samplerate */ static int select_sample_rate(const AVCodec *codec) { const int *p; int best_samplerate = 0; if (!codec->supported_samplerates) return 44100; p = codec->supported_samplerates; while (*p) { if (!best_samplerate || abs(44100 - *p) channel_layouts) return AV_CH_LAYOUT_STEREO; p = codec->channel_layouts; while (*p) { int nb_channels = av_get_channel_layout_nb_channels(*p); if (nb_channels > best_nb_channels) { best_ch_layout = *p; best_nb_channels = nb_channels; } p++; } return best_ch_layout; } static int f_index = 0; static void encode(AVCodecContext *ctx, AVFrame *frame, AVPacket *pkt, FILE *output,AVFormatContext *ofmt_ctx) { int ret; /* send the frame for encoding */ ret = avcodec_send_frame(ctx, frame); if (ret = 0) { ret = avcodec_receive_packet(ctx, pkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret pts = f_index *100; pkt->dts = f_index *100; pkt->pos = -1; pkt->stream_index = 0; printf(\"Write 1 Packet. size:%5d\\tpts:%lld\\n\", pkt->size, pkt->pts); //Write if (av_interleaved_write_frame(ofmt_ctx, pkt) bit_rate = 34000; c->sample_fmt = AV_SAMPLE_FMT_FLTP; //音频：检测当前编码器是否支持采样格式 if (!check_sample_fmt(codec, c->sample_fmt)) { fprintf(stderr, \"Encoder does not support sample format %s\", av_get_sample_fmt_name(c->sample_fmt)); exit(1); } /* select other audio parameters supported by the encoder */ c->sample_rate = select_sample_rate(codec); c->channel_layout = select_channel_layout(codec); c->channels = av_get_channel_layout_nb_channels(c->channel_layout); //初始化AVCodecContext if (avcodec_open2(c, codec, NULL) nb_samples = c->frame_size; frame->format = c->sample_fmt; frame->channel_layout = c->channel_layout; /* allocate the data buffers */ ret = av_frame_get_buffer(frame, 0); if (ret oformat; AVStream *out_stream = avformat_new_stream(ofmt_ctx, codec); //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, c) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; /* open the output file, if needed */ if (!(ofmt->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, outFilename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", outFilename, av_err2str(ret)); return -1; } } //Write file header //（封装——文件头） if (avformat_write_header(ofmt_ctx, NULL) sample_fmt); while (!feof(inFile)){ /* make sure the frame is writable -- makes a copy if the encoder * kept a reference internally */ ret = av_frame_make_writable(frame); if (ret frame_size; i++) for (j = 0; j channels; j++) fread(frame->data[j] + data_size*i, 1, data_size, inFile); encode(c, frame, pkt, outFile,ofmt_ctx); } /* flush the encoder */ encode(c, NULL, pkt, outFile,ofmt_ctx); printf(\"Write file trailer.\\n\"); //Write file trailer//（封装——文件尾） av_write_trailer(ofmt_ctx); fclose(outFile); av_frame_free(&frame); av_packet_free(&pkt); avcodec_free_context(&c); avformat_free_context(ofmt_ctx); return 0; } 官方例子【Video】 改动的本例子，使用x264库进行解码。所以需要集成x264库，请参考 编译ffmpeg4.2.2 。 我这里使用在MacOS上面跑的例子，以下是我本例编译ffmpeg的脚本： #!/bin/bash make clean function build_x264() { cd libx264 ./configure \\ --prefix=${X264_LIBS} \\ --disable-static \\ --enable-shared || exit 0 make clean make install } function build_macosx() { cd .. ./configure \\ --prefix=$PREFIX \\ --disable-programs \\ --enable-ffmpeg \\ --target-os=darwin \\ --disable-static \\ --enable-shared \\ --extra-cflags=\"-fPIC\" \\ --ln_s=\"ln -s\" \\ --pkg-config=$(which pkg-config) \\ --enable-gpl \\ --extra-cflags=\"-I${X264_LIBS}/include\" \\ --extra-ldflags=\"-L${X264_LIBS}/lib\" \\ --enable-libx264 || exit 0 make clean make install } PREFIX=$(pwd)/macox X264_LIBS=$(pwd)/libx264/libouput build_x264 build_macosx 编译完成后，会在项目根目录上传ffmpeg工具，可以通过以下操作查看是否正常集成： ./ffmpeg -re -i input.mp4 -vcodec libx264 -an output.mp4 编码例子： /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/07 * description: 来自官方例子：doc/examples/encode_video.c * 主要思路： * 1、初始化编码器信息 * 2、从文件（或者生成）读取裸流数据到每个AVFrame中 * 3、把每个AVFrame通过编码器生成编码后数据的AVPacket * 4、使用AVFormatContext封装到文件中进行输出 * * */ #include #include #include extern \"C\" { #include #include #include #include #include #include #include #include } static int f_index = 0; static void encode(AVCodecContext *ctx, AVFrame *frame, AVPacket *pkt, FILE *output, AVFormatContext *ofmt_ctx) { int ret; //注意：需要在送去编码器前指定 pts frame->pts = f_index++; /* send the frame for encoding */ ret = avcodec_send_frame(ctx, frame); if (ret = 0) { ret = avcodec_receive_packet(ctx, pkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) return; else if (ret stream_index = 0; printf(\"Write 1 Packet. size:%5d\\tpts:%lld\\n\", pkt->size, pkt->pts); //Write if (av_interleaved_write_frame(ofmt_ctx, pkt) bit_rate = 120000; /* resolution must be a multiple of two */ c->width = 384; c->height = 216; /* frames per second */ c->time_base = (AVRational) {1, 15}; c->framerate = (AVRational) {15, 1}; /* emit one intra frame every ten frames * check frame pict_type before passing frame * to encoder, if frame->pict_type is AV_PICTURE_TYPE_I * then gop_size is ignored and the output of encoder * will always be I frame irrespective to gop_size */ c->gop_size = 10; c->max_b_frames = 1; c->pix_fmt = AV_PIX_FMT_YUV420P; if (codec->id == AV_CODEC_ID_H264) av_opt_set(c->priv_data, \"preset\", \"slow\", 0); //初始化AVCodecContext if (avcodec_open2(c, codec, NULL) format = c->pix_fmt; frame->width = c->width; frame->height = c->height; /* allocate the data buffers */ ret = av_frame_get_buffer(frame, 32); if (ret oformat; AVStream *out_stream = avformat_new_stream(ofmt_ctx, codec); //Copy the settings of AVCodecContext if (avcodec_copy_context(out_stream->codec, c) codec->codec_tag = 0; if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; /* open the output file, if needed */ if (!(ofmt->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, outFilename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", outFilename, av_err2str(ret)); return -1; } } //Write file header //（封装——文件头） if (avformat_write_header(ofmt_ctx, NULL) data[0], 1, frame->height * frame->width, inFile); //Y fread(frame->data[1], 1, frame->height * frame->width/4, inFile); //U fread(frame->data[2], 1, frame->height * frame->width/4, inFile); //V encode(c, frame, pkt, outFile, ofmt_ctx); } /* flush the encoder */ encode(c, NULL, pkt, outFile, ofmt_ctx); printf(\"Write file trailer.\\n\"); //Write file trailer//（封装——文件尾） av_write_trailer(ofmt_ctx); fclose(outFile); av_frame_free(&frame); av_packet_free(&pkt); avcodec_free_context(&c); avformat_free_context(ofmt_ctx); return 0; } 测试的输入数据从 Decode（解码） 生成。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/13_transfer.html":{"url":"FFmpeg/13_transfer.html","title":"简单实现转码","keywords":"","body":"简单实现转码转码原理代码实现简单实现转码 本文汇总前面几篇文章，把所有流程合并到一块，简单实现转码的流程。其中有些异常不做处理。 转码原理 先看雷神的一张图： 上图描述的很明白，上完图发现已经不需要语言描述了[Dog]。 但还是画个来说明一下这块之间的联系： 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/08 * description: 转码 * 主要思路： * 格式1-->解封装--[解封装数据AVPacket]-->解码--[原始数据AVFrame]-->编码-->[编码后数据AVPacket]-->封装-->格式2 * * 2021/01/11 增加音频重采样变换 * */ #include #include extern \"C\" { #include #include #include #include #include #include #include #include #include } int main(int argc, char **argv) { // const char *in_filename = \"source/lol.mp4\"; // const char *out_filename = \"output/lol.wma\"; // const char *in_filename = \"source/Kobe.flv\"; // const char *out_filename = \"output/Kobe.avi\"; // const char *in_filename = \"source/lol.mp4\"; // const char *out_filename = \"output/lol.avi\"; const char *in_filename = \"source/Kobe.flv\"; const char *out_filename = \"output/Kobe2.avi\"; remove(out_filename); SwrContext *swr_ctx;//当有需要时，需要重采样 AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL; AVPacket *ipkt = NULL, *opkt = NULL; AVFrame *iframe = NULL, *oframe_a = NULL; AVCodecContext *ic_v = NULL, *ic_a = NULL, *oc_v = NULL, *oc_a = NULL; AVCodec *ocodec_v = NULL, *ocodec_a = NULL; int iindex_v = -1, iindex_a = -1, oindex_v = -1, oindex_a = -1, ret, dst_nb_samples; ipkt = av_packet_alloc(); opkt = av_packet_alloc(); /**（解封装 1.1）：创建并初始化AVFormatContext*/ if (avformat_open_input(&ifmt_ctx, in_filename, NULL, NULL) streams[iindex_v]->codec; if (!(ic_v->codec = avcodec_find_decoder(ic_v->codec_id))) { fprintf(stderr, \"Could find in video AVCodec\\n\"); goto end; } ic_a = ifmt_ctx->streams[iindex_a]->codec; if (!(ic_a->codec = avcodec_find_decoder(ic_a->codec_id))) { fprintf(stderr, \"Could find in audio AVCodec\\n\"); goto end; } /**（解码 2.2）：初始化码器器上下文*/ if (avcodec_open2(ic_v, ic_v->codec, NULL) codec, NULL) oformat->video_codec))) { fprintf(stderr, \"Could find out video AVCodec\\n\"); goto end; } if (!(ocodec_a = avcodec_find_encoder(ofmt_ctx->oformat->audio_codec))) { fprintf(stderr, \"Could find out audio AVCodec\\n\"); goto end; } /**（封装 4.3）：创建输出视频和音频AVStream，并与关联上AVFormatContext。（编码 3.1）：创建我们所需的编码器*/ for (int i = 0; i nb_streams; i++) { if (i != iindex_v && i != iindex_a) { continue; } AVStream *out_stream = avformat_new_stream(ofmt_ctx, NULL); if (!out_stream) { fprintf(stderr, \"Failed allocating output stream\\n\"); goto end; } out_stream->codecpar->codec_tag = 0; /*********************************************************************************************/ /** 这一块是编码器的参数信息设置，ffmpeg生产的默认编码器信息可能不是我们想要的，需要根据具体情况进行设置 **/ /*********************************************************************************************/ if (i == iindex_v) { oindex_v = out_stream->index; // oc_v = ofmt_ctx->streams[oindex_v]->codec; //这是旧版的写法 if (!(oc_v = avcodec_alloc_context3(ocodec_v))) { fprintf(stderr, \"Could not allow out AVCodecContext v\\n\"); goto end; } oc_v->height = ic_v->height; oc_v->width = ic_v->width; oc_v->sample_aspect_ratio = ic_v->sample_aspect_ratio; oc_v->pix_fmt = ocodec_v->pix_fmts[0]; oc_v->time_base = ic_v->time_base; oc_v->has_b_frames = ic_v->has_b_frames; //输出将相对于输入延迟max_b_frames + 1-->但是输入的为0！ // oc_v->max_b_frames = ic_v->max_b_frames + 1; oc_v->max_b_frames = 2; oc_v->bit_rate = ic_v->bit_rate; oc_v->codec_type = ic_v->codec_type; if ((ret = avcodec_parameters_from_context(out_stream->codecpar, oc_v)) index; // oc_a = ofmt_ctx->streams[oindex_a]->codec; if (!(oc_a = avcodec_alloc_context3(ocodec_a))) { fprintf(stderr, \"Could not allow out AVCodecContext v\\n\"); goto end; } oc_a->sample_rate = ic_a->sample_rate; oc_a->channel_layout = ic_a->channel_layout; oc_a->channels = av_get_channel_layout_nb_channels(ic_a->channel_layout); oc_a->sample_fmt = ocodec_a->sample_fmts[0]; oc_a->time_base = {1, ic_a->sample_rate}; oc_a->bit_rate = ic_a->bit_rate; oc_a->codec_type = ic_a->codec_type; if ((ret = avcodec_parameters_from_context(out_stream->codecpar, oc_a)) oformat->flags & AVFMT_NOFILE)) { if (avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE)) { fprintf(stderr, \"Could not open '%s': %s\\n\", out_filename, av_err2str(ret)); goto end; } } /**（重采样 5.1）：申请内存*/ if (!(swr_ctx = swr_alloc())) { fprintf(stderr, \"Could not allocate resampler context\\n\"); goto end; } /**（重采样 5.2）：设置参数*/ av_opt_set_int(swr_ctx, \"in_channel_count\", ic_a->channels, 0); av_opt_set_int(swr_ctx, \"in_sample_rate\", ic_a->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"in_sample_fmt\", ic_a->sample_fmt, 0); av_opt_set_int(swr_ctx, \"out_channel_count\", oc_a->channels, 0); av_opt_set_int(swr_ctx, \"out_sample_rate\", oc_a->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"out_sample_fmt\", oc_a->sample_fmt, 0); /**（重采样 5.3）：初始化SwrContext*/ if ((ret = swr_init(swr_ctx)) = 0) { if (!iframe) { if (!(iframe = av_frame_alloc())) { fprintf(stderr, \"Could not allocate iframe\\n\"); goto end; } } if (!oframe_a) { if (!(oframe_a = av_frame_alloc())) { fprintf(stderr, \"Could not allocate oframe_a\\n\"); goto end; } oframe_a->format = oc_a->sample_fmt; oframe_a->channel_layout = oc_a->channel_layout; oframe_a->sample_rate = oc_a->sample_rate; oframe_a->nb_samples = oc_a->frame_size; if (oframe_a->nb_samples) { ret = av_frame_get_buffer(oframe_a, 0); if (ret stream_index == iindex_v) { ic_temp = ic_v; oc_temp = oc_v; oindex_temp = oindex_v; } else if (ipkt->stream_index == iindex_a) { ic_temp = ic_a; oc_temp = oc_a; oindex_temp = oindex_a; } else { fprintf(stderr, \"has other streams?\\n\"); continue; } in_stream = ifmt_ctx->streams[ipkt->stream_index]; out_stream = ofmt_ctx->streams[oindex_temp]; /**（解码 2.3）：将得到的AVPacket送去解码器*/ ret = avcodec_send_packet(ic_temp, ipkt); if (ret = 0) { //接到解码后的，读取到AVFrame中 /**（解码 2.4）：从解码器中得到的数据到AVFrame*/ ret = avcodec_receive_frame(ic_temp, iframe); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret stream_index == iindex_a && iframe->nb_samples != oc_temp->frame_size) { oframe_a->nb_samples = oc_temp->frame_size; // dst_nb_samples = av_rescale_rnd(swr_get_delay(swr_ctx, ic_a->sample_rate) + // ic_a->frame_size, oc_a->sample_rate, ic_a->sample_rate, AV_ROUND_UP); /**（重采样 5.4）：重新采样*/ ret = swr_convert(swr_ctx, oframe_a->data, oc_a->frame_size, (const uint8_t **) iframe->data, ic_a->frame_size); if (ret = 0) { /**（编码 3.3）：从编码器中得到编码后数据，放入AVPacket中*/ ret = avcodec_receive_packet(oc_temp, opkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break; else if (ret 从输入的信息转换成输出的信息；参考官方例子：doc/examples/remuxing.c opkt->stream_index = oindex_temp; opkt->pts = av_rescale_q_rnd(ipkt->pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); opkt->dts = av_rescale_q_rnd(ipkt->dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX)); opkt->duration = av_rescale_q(ipkt->duration, in_stream->time_base, out_stream->time_base); opkt->pos = -1; // printf(\"in Read 1 Packet. size:%5d\\tstream_index:% d\\tdts:%lld\\tpts:%lld\\tduration:%lld\\tcur_dts:%lld\\n\", // ipkt->size, ipkt->stream_index, ipkt->dts, ipkt->pts, ipkt->duration, // ofmt_ctx->streams[oindex_temp]->cur_dts); printf(\"out Write 1 Packet. size:%5d\\tstream_index:% d\\tdts:%lld\\tpts:%lld\\tduration:%lld\\tcur_dts:%lld\\n\", opkt->size, opkt->stream_index, opkt->dts, opkt->pts, opkt->duration, ofmt_ctx->streams[oindex_temp]->cur_dts); /**（封装 4.6）：写入数据*/ if (av_interleaved_write_frame(ofmt_ctx, opkt) 参考 https://blog.csdn.net/leixiaohua1020/article/details/26838535 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/14_filter_v.html":{"url":"FFmpeg/14_filter_v.html","title":"Filter和SDL（Video）","keywords":"","body":"Filter和SDL（Video）使用滤镜流程代码实现CMakeList配置测试文件下载地址Filter和SDL（Video） 本文主要来自官方例子doc/examples/filtering_video.c 。 滤镜官方语法 , 推荐参考《FFmpeg从入门到精通》。 使用滤镜流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/09 * description: 来自官方例子：doc/examples/filtering_video.c * */ #include #include #include #define _XOPEN_SOURCE 600 /* for usleep */ #include extern \"C\" { #include #include #include #include #include #include }; //swapuv //const char *filter_descr = \"scale=78:24,transpose=cclock\"; //const char *filter_descr = \"movie=logo.png[wm];[in][wm]overlay=30:10[out]\"; const char *filter_descr = \"scale=iw/2:ih/2[in_tmp];[in_tmp]split=4[in_1][in_2][in_3][in_4];[in_1]pad=iw*2:ih*2[a];[a][in_2]overlay=w[b];[b][in_3]overlay=0:h[d];[d][in_4]overlay=w:h[out]\"; /* other way: scale=78:24 [scl]; [scl] transpose=cclock // assumes \"[in]\" and \"[out]\" to be input output pads respectively */ static AVFormatContext *fmt_ctx; static AVCodecContext *dec_ctx; AVFilterContext *buffersink_ctx; AVFilterContext *buffersrc_ctx; AVFilterGraph *filter_graph; static int video_stream_index = -1; static int64_t last_pts = AV_NOPTS_VALUE; static int open_input_file(const char *filename) { int ret; AVCodec *dec; if ((ret = avformat_open_input(&fmt_ctx, filename, NULL, NULL)) streams[video_stream_index]->codecpar); /* init the video decoder */ if ((ret = avcodec_open2(dec_ctx, dec, NULL)) streams[video_stream_index]->time_base; enum AVPixelFormat pix_fmts[] = {AV_PIX_FMT_YUV420P, AV_PIX_FMT_NONE};//注意输入的格式类型pix_fmts[0] filter_graph = avfilter_graph_alloc(); if (!outputs || !inputs || !filter_graph) { ret = AVERROR(ENOMEM); goto end; } /* buffer video source: the decoded frames from the decoder will be inserted here. */ snprintf(args, sizeof(args), \"video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\", dec_ctx->width, dec_ctx->height, dec_ctx->pix_fmt, time_base.num, time_base.den, dec_ctx->sample_aspect_ratio.num, dec_ctx->sample_aspect_ratio.den); //创建和初始化过滤器实例并将其添加到现有图形中。【输入】 ret = avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, \"in\", args, NULL, filter_graph); if (ret name = av_strdup(\"in\"); outputs->filter_ctx = buffersrc_ctx; outputs->pad_idx = 0; outputs->next = NULL; /* * The buffer sink input must be connected to the output pad of * the last filter described by filters_descr; since the last * filter output label is not specified, it is set to \"out\" by * default. */ inputs->name = av_strdup(\"out\"); inputs->filter_ctx = buffersink_ctx; inputs->pad_idx = 0; inputs->next = NULL; //将字符串描述的图形添加到图形中。 if ((ret = avfilter_graph_parse_ptr(filter_graph, filters_descr, &inputs, &outputs, NULL)) width, dec_ctx->height, SDL_WINDOW_OPENGL ); if (!screen){ printf(\"SDL_CreateWindow() failed: %s\\n\", SDL_GetError()); goto end; } // B3. 创建SDL_Renderer // SDL_Renderer：渲染器 sdl_renderer = SDL_CreateRenderer(screen, -1, 0); // B4. 创建SDL_Texture // 一个SDL_Texture对应一帧YUV数据，同SDL 1.x中的SDL_Overlay // 此处第2个参数使用的是SDL中的像素格式，对比参考注释A7 // FFmpeg中的像素格式AV_PIX_FMT_YUV420P对应SDL中的像素格式SDL_PIXELFORMAT_IYUV sdl_texture = SDL_CreateTexture(sdl_renderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, dec_ctx->width, dec_ctx->height); sdl_rect.x = 0; sdl_rect.y = 0; sdl_rect.w = dec_ctx->width; sdl_rect.h = dec_ctx->height; ret = 1; end: return ret; } static void sdl_play(const AVFrame *frame){ // B5. 使用新的YUV像素数据更新SDL_Rect SDL_UpdateYUVTexture(sdl_texture, // sdl texture &sdl_rect, // sdl rect &sdl_rect frame->data[0], // y plane frame->linesize[0], // y pitch frame->data[1], // u plane frame->linesize[1], // u pitch frame->data[2], // v plane frame->linesize[2] // v pitch ); // B6. 使用特定颜色清空当前渲染目标 SDL_RenderClear(sdl_renderer); // B7. 使用部分图像数据(texture)更新当前渲染目标 SDL_RenderCopy(sdl_renderer, // sdl renderer sdl_texture, // sdl texture NULL, // src rect, if NULL copy texture &sdl_rect // dst rect ); // B8. 执行渲染，更新屏幕显示 SDL_RenderPresent(sdl_renderer); // B9. 控制帧率为25FPS，此处不够准确，未考虑解码消耗的时间 //Delay 40ms SDL_Event e; while (SDL_PollEvent(&e)) { SDL_Delay(40); if (e.type == SDL_QUIT) { break; } } } static void display_frame(const AVFrame *frame, AVRational time_base) { int x, y; uint8_t *p0, *p; int64_t delay; if (frame->pts != AV_NOPTS_VALUE) { if (last_pts != AV_NOPTS_VALUE) { /* sleep roughly the right amount of time; * usleep is in microseconds, just like AV_TIME_BASE. */ delay = av_rescale_q(frame->pts - last_pts, time_base, AV_TIME_BASE_Q); if (delay > 0 && delay pts; } sdl_play(frame); /* Trivial ASCII grayscale display. */ // p0 = frame->data[0]; // puts(\"\\033c\"); /*for (y = 0; y height; y++) { p = p0; for (x = 0; x width; x++) putchar(\" .-+#\"[*(p++) / 52]); putchar('\\n'); p0 += frame->linesize[0]; } fflush(stdout);*/ } int main(int argc, char **argv) { int ret; AVPacket packet; AVFrame *frame; AVFrame *filt_frame; const char *filename = \"source/Kobe.flv\"; // const char *filename = \"source/lol.mp4\"; frame = av_frame_alloc(); filt_frame = av_frame_alloc(); if (!frame || !filt_frame) { perror(\"Could not allocate frame\"); exit(1); } //获取解封装上下文（AVFormatContext）和解码器，并查找视频通道 if ((ret = open_input_file(filename)) = 0) { //获取解码后数据 ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret pts = frame->best_effort_timestamp; /**将解码后得到的数据通过滤镜处理*/ /* push the decoded frame into the filtergraph */ if (av_buffersrc_add_frame_flags(buffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF) inputs[0]->time_base); av_frame_unref(filt_frame); } av_frame_unref(frame); } } av_packet_unref(&packet); } end: SDL_Quit(); avfilter_graph_free(&filter_graph); avcodec_free_context(&dec_ctx); avformat_close_input(&fmt_ctx); av_frame_free(&frame); av_frame_free(&filt_frame); if (ret CMakeList配置 cmake_minimum_required(VERSION 3.17) project(VAFFmpeg) set(CMAKE_CXX_STANDARD 11) set(SOURCE_FILES main.cpp) link_directories(lib) include_directories(include) #引入sdl2库 find_package(SDL2 REQUIRED) include_directories(VAFFmpeg ${SDL2_INCLUDE_DIRS}) add_executable(VAFFmpeg ${SOURCE_FILES}) target_link_libraries( VAFFmpeg ${SDL2_LIBRARIES} -lavcodec -lavdevice -lavfilter -lavformat -lswresample -lswscale # -lx264 ) 测试文件下载地址 参考 https://blog.csdn.net/leixiaohua1020/article/details/29368911 https://blog.csdn.net/leixiaohua1020/article/details/40876089 FFmpeg从入门到精通 其他：忘记留地址了[尴尬] Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/15_filter_a.html":{"url":"FFmpeg/15_filter_a.html","title":"Filter和SDL（Audio）","keywords":"","body":"Filter和SDL（Audio）使用滤镜流程代码实现测试文件下载地址Filter和SDL（Audio） 本文主要来自官方例子doc/examples/filtering_video.c 。 滤镜官方语法 , 推荐参考《FFmpeg从入门到精通》。 使用滤镜流程 参考上一篇视频滤镜使用流程 。注意以下一点： 获取滤镜器的名称输入：avfilter_get_by_name(\"buffer\") -> avfilter_get_by_name(\"abuffer\") 输出：avfilter_get_by_name(\"buffersink\") -> avfilter_get_by_name(\"abuffersink\") 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/10 * description: 来自官方例子：doc/examples/filtering_audio.c * */ #include #include #include #define _XOPEN_SOURCE 600 /* for usleep */ #include extern \"C\" { #include #include #include #include #include #include }; //fltp //static const char *filter_descr = \"aresample=44100,aformat=sample_fmts=fltp:channel_layouts=mono\"; static const char *filter_descr = \"aecho=0.8:0.88:60:0.4\";//参考：http://ffmpeg.org/ffmpeg-filters.html#aecho static const char *player = \"ffplay -f s16le -ar 8000 -ac 1 -\"; static AVFormatContext *fmt_ctx; static AVCodecContext *dec_ctx; AVFilterContext *buffersink_ctx; AVFilterContext *buffersrc_ctx; AVFilterGraph *filter_graph; static int audio_stream_index = -1; static int open_input_file(const char *filename) { int ret; AVCodec *dec; if ((ret = avformat_open_input(&fmt_ctx, filename, NULL, NULL)) streams[audio_stream_index]->codecpar); /* init the audio decoder */ if ((ret = avcodec_open2(dec_ctx, dec, NULL)) streams[audio_stream_index]->time_base; filter_graph = avfilter_graph_alloc(); if (!outputs || !inputs || !filter_graph) { ret = AVERROR(ENOMEM); goto end; } /* buffer audio source: the decoded frames from the decoder will be inserted here. */ if (!dec_ctx->channel_layout) dec_ctx->channel_layout = av_get_default_channel_layout(dec_ctx->channels); snprintf(args, sizeof(args), \"time_base=%d/%d:sample_rate=%d:sample_fmt=%s:channel_layout=0x%lld\", time_base.num, time_base.den, dec_ctx->sample_rate, av_get_sample_fmt_name(dec_ctx->sample_fmt), dec_ctx->channel_layout); ret = avfilter_graph_create_filter(&buffersrc_ctx, abuffersrc, \"in\", args, NULL, filter_graph); if (ret name = av_strdup(\"in\"); outputs->filter_ctx = buffersrc_ctx; outputs->pad_idx = 0; outputs->next = NULL; /* * The buffer sink input must be connected to the output pad of * the last filter described by filters_descr; since the last * filter output label is not specified, it is set to \"out\" by * default. */ inputs->name = av_strdup(\"out\"); inputs->filter_ctx = buffersink_ctx; inputs->pad_idx = 0; inputs->next = NULL; if ((ret = avfilter_graph_parse_ptr(filter_graph, filters_descr, &inputs, &outputs, NULL)) inputs[0]; av_get_channel_layout_string(args, sizeof(args), -1, outlink->channel_layout); av_log(NULL, AV_LOG_INFO, \"Output: srate:%dHz fmt:%s chlayout:%s\\n\", (int) outlink->sample_rate, (char *) av_x_if_null(av_get_sample_fmt_name(static_cast(outlink->format)), \"?\"), args); end: avfilter_inout_free(&inputs); avfilter_inout_free(&outputs); return ret; } static Uint8 *audio_chunk; static Uint32 audio_len; static Uint8 *audio_pos; void fill_audio(void *udata, Uint8 *stream, int len) { //SDL 2.0 SDL_memset(stream, 0, len); if (audio_len == 0) return; len = (len > audio_len ? audio_len : len); SDL_MixAudio(stream, audio_pos, len, SDL_MIX_MAXVOLUME); audio_pos += len; audio_len -= len; } //https://blog.csdn.net/leixiaohua1020/article/details/40544521 static int init_sdl(AVCodecContext *dec_ctx) { int ret = -1; // B1. 初始化SDL子系统：缺省(事件处理、文件IO、线程)、视频、音频、定时器 if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) { printf(\"SDL_Init() failed: %s\\n\", SDL_GetError()); goto end; } //注意：这里设置的参数会算出 audio_chunk 所使用的长度 //audio_chunk = 采样数 * 通道数 * 位宽 SDL_AudioSpec wanted_spec; wanted_spec.freq = dec_ctx->sample_rate; // wanted_spec.format = dec_ctx->sample_fmt; wanted_spec.format = AUDIO_F32SYS;//位宽=4 wanted_spec.channels = dec_ctx->channels;//通道数 wanted_spec.silence = 0; wanted_spec.samples = dec_ctx->frame_size;//采样数 wanted_spec.callback = fill_audio; if (SDL_OpenAudio(&wanted_spec, NULL) data[0][0] == '\\0') {//没有数据？ return; } int i, ch, data_size; data_size = av_get_bytes_per_sample(dec_ctx->sample_fmt);//每一个采样点所占的字节数 Uint32 len = data_size * frame->nb_samples * dec_ctx->channels;//所有通道采样数所占字节长度（一帧大小） Uint8 *all_channels_buf = (Uint8 *) malloc(len); int index = 0; //把所有通道采样数据重新排列 for (i = 0; i nb_samples; i++) { for (ch = 0; ch channels; ch++) { memcpy(all_channels_buf + index * data_size, frame->data[ch] + data_size * i, data_size); ++index; } } //把一帧数据设置给SDL播放器 audio_chunk = all_channels_buf; audio_len = len; audio_pos = audio_chunk; while (audio_len > 0)//Wait until finish SDL_Delay(1); free(all_channels_buf); } static void print_frame(const AVFrame *frame) { sdl_play(frame); /*const int n = frame->nb_samples * av_get_channel_layout_nb_channels(frame->channel_layout); const uint16_t *p = (uint16_t *) frame->data[0]; const uint16_t *p_end = p + n; while (p > 8 & 0xff, stdout); p++; } fflush(stdout);*/ } int main(int argc, char **argv) { int ret; AVPacket packet; AVFrame *frame = av_frame_alloc(); AVFrame *filt_frame = av_frame_alloc(); if (!frame || !filt_frame) { perror(\"Could not allocate frame\"); exit(1); } const char *filename = \"source/Kobe.flv\"; if ((ret = open_input_file(filename)) = 0) { ret = avcodec_receive_frame(dec_ctx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret = 0) { /* push the audio data from decoded frame into the filtergraph */ if (av_buffersrc_add_frame_flags(buffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF) 测试文件下载地址 参考 https://blog.csdn.net/leixiaohua1020/article/details/40544521 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/16_transcode.html":{"url":"FFmpeg/16_transcode.html","title":"Transcode(转码)","keywords":"","body":"Transcode(转码)流程与FFmpeg 简单实现转码 差异性如下：1.增加Filter（滤镜）处理2.优化SwrContext（重采样）处理3.增加AVAudioFifo缓存处理4.视频和音频同步代码实现测试文件下载地址Transcode(转码) 本文基于官方例子：doc/examples/transcoding.c 。 流程 其中，AVFormatContext、AVPacket等重要的结构体请看：FFmpeg重要结构体 。 与FFmpeg 简单实现转码 差异性如下： 1.增加Filter（滤镜）处理 具体请看Filter和SDL（Video） 和 Filter和SDL（Audio） 2.优化SwrContext（重采样）处理 关键理解是这一段： //1.swr_get_out_samples if (swr_get_out_samples(swr_ctx, 0) >= enc_ctx->frame_size) {//当上次缓存足够时，取缓存 //2.swr_convert if ((ret = swr_convert(swr_ctx, reasampling_frame->data, enc_ctx->frame_size, NULL, 0)) data, enc_ctx->frame_size, (const uint8_t **) filt_frame->data, filt_frame->nb_samples)) swr_get_out_samples 含义：获取当前采样缓存的大小。 swr_convert 含义：从采样缓存中采样enc_ctx->frame_size个长度数据到reasampling_frame->data中。 swr_convert 含义：从有个filt_frame->nb_samples数据的filt_frame->data中采样enc_ctx->frame_size个长度数据到reasampling_frame->data中。 3.增加AVAudioFifo缓存处理 主要有三个方法av_audio_fifo_size 、 av_audio_fifo_write 和av_audio_fifo_read ，详细见代码。 4.视频和音频同步 创建流时(avformat_new_stream)，初始化时间基out_stream->time_base = enc_ctx->time_base 。 送去解码前，需要重新计算pts、dts和duration，代码如下：av_packet_rescale_ts(de_pkt, ifmt_ctx->streams[stream_index]->time_base, ifmt_ctx->streams[stream_index]->codec->time_base); 送去编码前，计算pts、dts和duration，代码如下：av_packet_rescale_ts(enc_pkt, ofmt_ctx->streams[stream_index]->codec->time_base, ofmt_ctx->streams[stream_index]->time_base); 注意：当音频编码器的frame_size和需要被编码的AVFrame中的nb_samples 大小不相等时，需要使用重采样或使用AVAudioFifo缓存，使得frame_size == nb_samples后再送去编码，这种情况会导致pts、dts和duration的值与其他帧相同问题。if (stream_index == AVMEDIA_TYPE_AUDIO && filt_frame->nb_samples != enc_ctx->frame_size){ ...... } 代码实现 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/12 * description: 来着官方例子：doc/examples/transcoding.c * */ extern \"C\" { #include #include #include #include #include #include #include #include \"libavutil/audio_fifo.h\" } //将每一组滤镜组合放到一起，filter_stm[i]和ifmt_ctx->streams[i]下标保持一致，表示对音频或者视频的滤镜 typedef struct FilterStream { AVFilterContext *buffersink_ctx; AVFilterContext *buffersrc_ctx; AVFilterGraph *filter_graph; } FilterStream; static FilterStream *filter_stm; static AVFormatContext *ifmt_ctx; static AVFormatContext *ofmt_ctx; static int audio_index, video_index; static SwrContext *swr_ctx;//音频：当有需要时，需要重采样 static int64_t resample_pos = 0;//重采样时，会有缓存，这时候要另外计算dts和pts static AVAudioFifo *fifo = NULL;;//重采样时，如果输入nb_sample比输出的nb_sample小时，需要缓存 int open_input_file(const char *in_filename) { int ret = -1, i; /**（解封装 1.1）：创建并初始化AVFormatContext*/ if (avformat_open_input(&ifmt_ctx, in_filename, NULL, NULL) nb_streams; i++) { AVStream *stream = ifmt_ctx->streams[i]; AVCodec *dec; AVCodecContext *dec_ctx; //dec_ctx = stream->codec; //-->通过这种形式获取的解码器上下文已过时 /**（解码 2.1）：查找解码器(AVCodec)*/ if (!(dec = avcodec_find_decoder(stream->codecpar->codec_id))) { fprintf(stderr, \"Failed to find decoder for stream #%u\\n\", i); ret = AVERROR_DECODER_NOT_FOUND; goto end; } /**（解码 2.2）：通过解码器(AVCodec)生成解码器上下文(AVCodecContext)*/ if (!(dec_ctx = avcodec_alloc_context3(dec))) { fprintf(stderr, \"Failed to allocate the decoder context for stream #%u\\n\", i); ret = AVERROR(ENOMEM); goto end; } /**（解码 2.3）：将AVCodecParameters参数赋值给AVCodecContext*/ if ((ret = avcodec_parameters_to_context(dec_ctx, stream->codecpar)) codec_type == AVMEDIA_TYPE_VIDEO || dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO) { dec_ctx->framerate = av_guess_frame_rate(ifmt_ctx, stream, NULL); video_index = i; } else if (dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { audio_index = i; } /**（解码 2.4）：初始化码器器上下文*/ if ((ret = avcodec_open2(dec_ctx, dec, NULL)) codec = dec_ctx; } //打印输出日志 // fprintf(stderr, \"---------------------------ifmt_ctx----------------\\n\"); // av_dump_format(ifmt_ctx, 0, in_filename, 0); ret = 0; end: return ret; } int open_output_file(const char *out_filename) { int ret = -1, i; AVStream *out_stream, *in_stream; AVCodecContext *dec_ctx, *enc_ctx; AVCodec *encoder; AVCodecID encodec_id; /**（封装 4.1）：根据文件格式初始化封装器上下文AVFormatContext*/ avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, out_filename); if (!ofmt_ctx) { fprintf(stderr, \"Could not create output context\\n\"); ret = AVERROR_UNKNOWN; goto end; } for (i = 0; i nb_streams; i++) { /**（封装 4.2）：创建输出视频和音频AVStream*/ if (!(out_stream = avformat_new_stream(ofmt_ctx, NULL))) { fprintf(stderr, \"Failed allocating output stream\\n\"); ret = AVERROR_UNKNOWN; goto end; } out_stream->codecpar->codec_tag = 0; in_stream = ifmt_ctx->streams[i]; dec_ctx = in_stream->codec; if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO || dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO) { encodec_id = ofmt_ctx->oformat->video_codec; } else if (dec_ctx->codec_type == AVMEDIA_TYPE_AUDIO) { encodec_id = ofmt_ctx->oformat->audio_codec; } /* in this example, we choose transcoding to same codec */ /**（编码 3.1）：获取对应的编码器AVCodec*/ if (!(encoder = avcodec_find_encoder(encodec_id))) { fprintf(stderr, \"Necessary encoder not found\\n\"); ret = AVERROR_INVALIDDATA; goto end; } /**（编码 3.2）：通过编码器(AVCodec)获取编码器上下文(AVCodecContext)*/ if (!(enc_ctx = avcodec_alloc_context3(encoder))) { fprintf(stderr, \"Failed to allocate the encoder context\\n\"); ret = AVERROR(ENOMEM); goto end; } /**给编码器初始化信息*/ /* In this example, we transcode to same properties (picture size, * sample rate etc.). These properties can be changed for output * streams easily using filters */ if (dec_ctx->codec_type == AVMEDIA_TYPE_VIDEO) { enc_ctx->height = dec_ctx->height; enc_ctx->width = dec_ctx->width; enc_ctx->sample_aspect_ratio = dec_ctx->sample_aspect_ratio; //take first format from list of supported formats if (encoder->pix_fmts) enc_ctx->pix_fmt = encoder->pix_fmts[0]; else enc_ctx->pix_fmt = dec_ctx->pix_fmt; //video time_base can be set to whatever is handy and supported by encoder enc_ctx->time_base = dec_ctx->time_base; enc_ctx->has_b_frames = dec_ctx->has_b_frames; //输出将相对于输入延迟max_b_frames + 1-->但是输入的为0！ //enc_ctx->max_b_frames = dec_ctx->max_b_frames + 1; enc_ctx->max_b_frames = 2; enc_ctx->bit_rate = dec_ctx->bit_rate; enc_ctx->codec_type = dec_ctx->codec_type; //不支持B帧 if (enc_ctx->max_b_frames && enc_ctx->codec_id != AV_CODEC_ID_MPEG4 && enc_ctx->codec_id != AV_CODEC_ID_MPEG1VIDEO && enc_ctx->codec_id != AV_CODEC_ID_MPEG2VIDEO) { enc_ctx->has_b_frames = 0; enc_ctx->max_b_frames = 0; } } else { enc_ctx->sample_rate = dec_ctx->sample_rate; enc_ctx->channel_layout = dec_ctx->channel_layout; enc_ctx->channels = av_get_channel_layout_nb_channels(dec_ctx->channel_layout); enc_ctx->sample_fmt = encoder->sample_fmts[0]; enc_ctx->time_base = {1, enc_ctx->sample_rate}; enc_ctx->bit_rate = dec_ctx->bit_rate; enc_ctx->codec_type = dec_ctx->codec_type; } if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER) enc_ctx->flags |= AV_CODEC_FLAG_GLOBAL_HEADER; /**（编码 3.3）：*/ /* Third parameter can be used to pass settings to encoder */ if ((ret = avcodec_open2(enc_ctx, encoder, NULL)) codecpar, enc_ctx)) time_base = enc_ctx->time_base; //替换掉旧的 out_stream->codec = enc_ctx; } else if (dec_ctx->codec_type == AVMEDIA_TYPE_UNKNOWN) { fprintf(stderr, \"Elementary stream #%d is of unknown type, cannot proceed\\n\", i); ret = AVERROR_INVALIDDATA; goto end; } else { /* if this stream must be remuxed */ if ((ret = avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar)) time_base = in_stream->time_base; } } //打印输出日志 // fprintf(stderr, \"---------------------------ofmt_ctx----------------\\n\"); // av_dump_format(ofmt_ctx, 0, out_filename, 1); /**（封装 4.4）：初始化AVIOContext*/ if (!(ofmt_ctx->oformat->flags & AVFMT_NOFILE)) { ; if ((ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE)) streams[audio_index]->codec; AVCodecContext *enc_ctx_audio = ofmt_ctx->streams[audio_index]->codec; int ret = -1; /**（重采样 5.1）：申请内存*/ if (!(swr_ctx = swr_alloc())) { fprintf(stderr, \"Could not allocate resampler context\\n\"); return ret; } /**（重采样 5.2）：设置参数*/ av_opt_set_int(swr_ctx, \"in_channel_count\", dec_ctx_audio->channels, 0); av_opt_set_int(swr_ctx, \"in_sample_rate\", dec_ctx_audio->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"in_sample_fmt\", dec_ctx_audio->sample_fmt, 0); av_opt_set_int(swr_ctx, \"out_channel_count\", enc_ctx_audio->channels, 0); av_opt_set_int(swr_ctx, \"out_sample_rate\", enc_ctx_audio->sample_rate, 0); av_opt_set_sample_fmt(swr_ctx, \"out_sample_fmt\", enc_ctx_audio->sample_fmt, 0); /**（重采样 5.3）：初始化SwrContext*/ if ((ret = swr_init(swr_ctx)) codec_type == AVMEDIA_TYPE_VIDEO) { /**（滤镜 6.1）：获取输入和输出滤镜器【同音频】*/ buffersrc = avfilter_get_by_name(\"buffer\"); buffersink = avfilter_get_by_name(\"buffersink\"); if (!buffersrc || !buffersink) { fprintf(stderr, \"filtering source or sink element not found\\n\"); ret = AVERROR_UNKNOWN; goto end; } snprintf(args, sizeof(args), \"video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\", dec_ctx->width, dec_ctx->height, dec_ctx->pix_fmt, dec_ctx->time_base.num, dec_ctx->time_base.den, dec_ctx->sample_aspect_ratio.num, dec_ctx->sample_aspect_ratio.den); /**（滤镜 6.2）：创建和初始化输入和输出过滤器实例并将其添加到现有图形中*/ if ((ret = avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, \"in\", args, NULL, filter_graph)) pix_fmt, sizeof(enc_ctx->pix_fmt), AV_OPT_SEARCH_CHILDREN); if (ret codec_type == AVMEDIA_TYPE_AUDIO) { buffersrc = avfilter_get_by_name(\"abuffer\");//名称比视频的前面多一个a buffersink = avfilter_get_by_name(\"abuffersink\"); if (!buffersrc || !buffersink) { fprintf(stderr, \"filtering source or sink element not found\\n\"); ret = AVERROR_UNKNOWN; goto end; } if (!dec_ctx->channel_layout) dec_ctx->channel_layout = av_get_default_channel_layout(dec_ctx->channels); snprintf(args, sizeof(args), \"time_base=%d/%d:sample_rate=%d:sample_fmt=%s:channel_layout=0x%lld\", dec_ctx->time_base.num, dec_ctx->time_base.den, dec_ctx->sample_rate, av_get_sample_fmt_name(dec_ctx->sample_fmt), dec_ctx->channel_layout); if ((ret = avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, \"in\", args, NULL, filter_graph)) sample_fmt, sizeof(enc_ctx->sample_fmt), AV_OPT_SEARCH_CHILDREN)) channel_layout, sizeof(enc_ctx->channel_layout), AV_OPT_SEARCH_CHILDREN)) sample_rate, sizeof(enc_ctx->sample_rate), AV_OPT_SEARCH_CHILDREN)) buffersrc_ctx /* Endpoints for the filter graph. */ outputs->name = av_strdup(\"in\"); outputs->filter_ctx = buffersrc_ctx; outputs->pad_idx = 0; outputs->next = NULL; //绑定关系 out ——> buffersink_ctx inputs->name = av_strdup(\"out\"); inputs->filter_ctx = buffersink_ctx; inputs->pad_idx = 0; inputs->next = NULL; if (!outputs->name || !inputs->name) { ret = AVERROR(ENOMEM); fprintf(stderr, \"Filter av_strdup name failed\\n\"); goto end; } /**（滤镜 6.4）：将字符串描述的图形添加到图形中*/ if ((ret = avfilter_graph_parse_ptr(filter_graph, filter_spec, &inputs, &outputs, NULL)) buffersrc_ctx = buffersrc_ctx; fctx->buffersink_ctx = buffersink_ctx; fctx->filter_graph = filter_graph; end: avfilter_inout_free(&inputs); avfilter_inout_free(&outputs); return ret; } int init_filters() { const char *filter_spec; unsigned int i; int ret; filter_stm = (FilterStream *) (av_malloc_array(ifmt_ctx->nb_streams, sizeof(*filter_stm))); if (!filter_stm) return AVERROR(ENOMEM); //这里会根据音频和视频的stream_index创建对应的filter_stm组 for (i = 0; i nb_streams; i++) { filter_stm[i].buffersrc_ctx = NULL; filter_stm[i].buffersink_ctx = NULL; filter_stm[i].filter_graph = NULL; if (!(ifmt_ctx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_AUDIO || ifmt_ctx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO)) continue; if (ifmt_ctx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) filter_spec = \"null\"; /* passthrough (dummy) filter for video */ // filter_spec = \"scale=iw/2:ih/2[in_tmp];[in_tmp]split=4[in_1][in_2][in_3][in_4];[in_1]pad=iw*2:ih*2[a];[a][in_2]overlay=w[b];[b][in_3]overlay=0:h[d];[d][in_4]overlay=w:h[out]\"; /* passthrough (dummy) filter for video */ else filter_spec = \"anull\"; /* passthrough (dummy) filter for audio */ // filter_spec = \"aecho=0.8:0.88:60:0.4\"; /* passthrough (dummy) filter for audio */ ret = init_filter(&filter_stm[i], ifmt_ctx->streams[i]->codec, ofmt_ctx->streams[i]->codec, filter_spec); if (ret) return ret; } return 0; } int encode_write_frame(AVFrame *filt_frame, unsigned int stream_index) { int ret = -1; AVFrame *reasampling_frame; AVPacket *enc_pkt = av_packet_alloc(); AVCodecContext *enc_ctx = ofmt_ctx->streams[stream_index]->codec; /* encode filtered frame */ enc_pkt->data = NULL; enc_pkt->size = 0; av_init_packet(enc_pkt); if (!filt_frame) {//刷新缓存 fprintf(stderr, \"filt_frame is null , flush encode ?\\n\"); avcodec_send_frame(enc_ctx, NULL); ret = 0; goto end; } //音频的这种情况需要重采样再进行输出 if (stream_index == AVMEDIA_TYPE_AUDIO && filt_frame->nb_samples != enc_ctx->frame_size) { //输入的比输出的还小；参考官方例子：doc/examples/transcode_aac.c if (filt_frame->nb_samples frame_size) { /* Make the FIFO as large as it needs to be to hold both, * the old and the new samples. */ /*if ((ret = av_audio_fifo_realloc(fifo, av_audio_fifo_size(fifo) + filt_frame->nb_samples)) data, filt_frame->nb_samples); if (buf_size nb_samples) { fprintf(stderr, \"Could not write data to FIFO\\n\"); ret = AVERROR_EXIT; goto end; } //这一次还不够转换 int new_size = av_audio_fifo_size(fifo); if (new_size frame_size) { ret = 0; goto end; } } if (!(reasampling_frame = av_frame_alloc())) { fprintf(stderr, \"Could not allocate oframe_a\\n\"); ret = -1; goto end; } //把filt_frame参数复制给reasampling_frame if ((ret = av_frame_copy_props(reasampling_frame, filt_frame)) format = enc_ctx->sample_fmt; reasampling_frame->channel_layout = enc_ctx->channel_layout; reasampling_frame->channels = enc_ctx->channels; reasampling_frame->sample_rate = enc_ctx->sample_rate; reasampling_frame->nb_samples = enc_ctx->frame_size; if ((ret = av_frame_get_buffer(reasampling_frame, 0)) nb_samples frame_size) { if (av_audio_fifo_read(fifo, (void **) reasampling_frame->data, reasampling_frame->nb_samples) nb_samples) { fprintf(stderr, \"Could not read data from FIFO\\n\"); // av_frame_free(&output_frame); ret = AVERROR_EXIT; goto end; } fprintf(stdout, \"deal fifo\\n\"); } else { /**（重采样 5.4）：重新采样*/ if (swr_get_out_samples(swr_ctx, 0) >= enc_ctx->frame_size) {//当上次缓存足够时，取缓存 if ((ret = swr_convert(swr_ctx, reasampling_frame->data, enc_ctx->frame_size, NULL, 0)) data, enc_ctx->frame_size, (const uint8_t **) filt_frame->data, filt_frame->nb_samples)) nb_samples != enc_ctx->frame_size){ resample_pos += enc_ctx->frame_size; //编码前重新给pts和dts赋值 reasampling_frame->pts = resample_pos; reasampling_frame->pkt_dts = resample_pos; } /**（编码 3.5）：把滤镜处理后的AVFrame送去编码*/ ret = avcodec_send_frame(enc_ctx, reasampling_frame); } else { /**（编码 3.5）：把滤镜处理后的AVFrame送去编码*/ ret = avcodec_send_frame(enc_ctx, filt_frame); } if (ret = 0) { /**（编码 3.6）：从编码器中得到编码后数据，放入AVPacket中*/ ret = avcodec_receive_packet(enc_ctx, enc_pkt); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)//本次的ret只针对本循环，对外部为正常 break; else if (ret >>>>\" : \"v-----\", enc_pkt->size, enc_pkt->dts, enc_pkt->pts, enc_pkt->duration, ofmt_ctx->streams[stream_index]->cur_dts); //设置pts等信息 /* prepare packet for muxing */ enc_pkt->stream_index = stream_index; av_packet_rescale_ts(enc_pkt, ofmt_ctx->streams[stream_index]->codec->time_base, ofmt_ctx->streams[stream_index]->time_base); enc_pkt->pos = -1; printf(\"write2 %s Packet. size:%5d\\tdts:%5lld\\tpts:%5lld\\tduration:%5lld\\tcur_dts:%5lld\\n\", stream_index == AVMEDIA_TYPE_AUDIO ? \"a>>>>>\" : \"v-----\", enc_pkt->size, enc_pkt->dts, enc_pkt->pts, enc_pkt->duration, ofmt_ctx->streams[stream_index]->cur_dts); /**（封装 4.6）：将编码后的数据AVPacket进行封装，写入到文件*/ if ((ret = av_interleaved_write_frame(ofmt_ctx, enc_pkt)) nb_samples != enc_ctx->frame_size) { if (swr_get_out_samples(swr_ctx, 0) >= enc_ctx->frame_size) { encode_write_frame(filt_frame, stream_index); } } ret = 0; end: av_packet_free(&enc_pkt); return ret; } int filter_encode_write_frame(AVFrame *de_frame, unsigned int stream_index) { int ret; AVFrame *filt_frame = NULL; /**（滤镜 6.6）：将解码后的AVFrame送去filtergraph进行滤镜处理*/ if ((ret = av_buffersrc_add_frame_flags(filter_stm[stream_index].buffersrc_ctx, de_frame, AV_BUFFERSRC_FLAG_KEEP_REF)) pict_type = AV_PICTURE_TYPE_NONE; //然后把滤镜处理后的数据重新进行编码成你想要的格式，再封装输出 if ((ret = encode_write_frame(filt_frame, stream_index)) streams[audio_index]->codec->sample_fmt, ofmt_ctx->streams[audio_index]->codec->channels, ofmt_ctx->streams[audio_index]->codec->frame_size * 2))) { fprintf(stderr, \"Could not allocate FIFO\\n\"); ret = AVERROR(ENOMEM); goto end; } while (1) { /**（解封装 1.3）：读取解封装后数据到AVPacket中*/ if ((ret = av_read_frame(ifmt_ctx, de_pkt)) stream_index; if (!(de_frame = av_frame_alloc())) { ret = AVERROR(ENOMEM); fprintf(stderr, \"Error alloc frame\\n\"); goto end; } //注意：av_packet_rescale_ts 在解码前重新计算pts、dts和duration的值，需要与编码时对应起来，防止视频和音频出现不同步 // 注意传入的参数！AVStream和AVCodecContext的时间基。而我的AVCodecContext是通过生成的，并非原来的初始化时的。 av_packet_rescale_ts(de_pkt, ifmt_ctx->streams[stream_index]->time_base, ifmt_ctx->streams[stream_index]->codec->time_base); /**（解码 2.5）：把AVPacket送去解码*/ /* read all the output frames (in general there may be any number of them */ if ((ret = avcodec_send_packet(ifmt_ctx->streams[stream_index]->codec, de_pkt)) = 0) { /**（解码 2.6）：从解码器获取解码后的数据到AVFrame*/ ret = avcodec_receive_frame(ifmt_ctx->streams[stream_index]->codec, de_frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { break; } else if (ret pts = de_frame->best_effort_timestamp; //这是解码后的裸数据，如果可以对其进行滤镜处理 ret = filter_encode_write_frame(de_frame, stream_index); // ret = encode_write_frame(de_frame, stream_index); if (ret nb_streams; i++) { avcodec_send_packet(ifmt_ctx->streams[i]->codec, NULL); avcodec_send_frame(ofmt_ctx->streams[i]->codec, NULL); } fprintf(stdout, \"write the trailer\\n\"); /**（封装 4.7）：写入文件尾*/ av_write_trailer(ofmt_ctx); end: av_packet_free(&de_pkt); av_frame_free(&de_frame); for (i = 0; i nb_streams; i++) { avcodec_free_context(&ifmt_ctx->streams[i]->codec); if (ofmt_ctx && ofmt_ctx->nb_streams > i && ofmt_ctx->streams[i] && ofmt_ctx->streams[i]->codec) avcodec_free_context(&ofmt_ctx->streams[i]->codec); if (filter_stm && filter_stm[i].filter_graph) avfilter_graph_free(&filter_stm[i].filter_graph); } av_free(filter_stm); avformat_close_input(&ifmt_ctx); if (ofmt_ctx && !(ofmt_ctx->oformat->flags & AVFMT_NOFILE)) avio_closep(&ofmt_ctx->pb); avformat_free_context(ofmt_ctx); if (fifo) av_audio_fifo_free(fifo); if (ret 测试文件下载地址 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/17_sync.html":{"url":"FFmpeg/17_sync.html","title":"音视频同步处理","keywords":"","body":"音视频同步处理播放流程时钟同步处理时钟类型处理逻辑裸数据\"同步\"播放影响视频播放进度的因素影响音频播放进度的因素同时播放视频和音频编解码同步处理转码流程回顾1. 处理之前相关函数2. 解码前处理（红色上AVPacket）3. 裸数据处理（绿色AVFrame）4. 编码后处理（红色右AVPacket）后话音视频同步处理 播放流程 从图中我们得到一个重要的名称，叫做\"时钟\"。视频和音频是两个不相干的线程，各自进行播放，而各自播放的进度都需要参考这个\"时钟\"，才能使得播放同步。其实这里说的\"进度\"就是我们熟悉pts，所以下面主要扒编解码中pts的那些点。讲之前，先讲讲三种类型的时钟处理。 时钟同步处理 时钟类型 音频时钟：即以音频进度作为参考时钟，音频播放进度为准。由于耳朵感官上比眼睛更敏感，所以大多少采用这种形式。缺点是画面可能出现卡段或跳帧的情况。 视频时钟：即以视频进度作为参考时钟，视频播放进度为准。 外部时钟：即以外部进度（时间戳）作为参考时钟。 处理逻辑 当音频/视频帧播放的进度大于参考时钟的进度时，需要让该帧等待播放；当音频/视频帧播放的进度小于参考时钟的进度时，需要快速播放完或者丢弃该帧处理。 总结来说就三个词：\"等待\" 和 \"快速\"或\"丢弃\"。 裸数据\"同步\"播放 我们首先再去简单屡屡音视频的一些基本概念，因为分析音视频同步不同步，理论知识真的很重要！播放的都是解码后的裸数据（yuv、pcm），如果视频跟音频两个裸数据源本身播放就不同步，再怎么处理也是徒劳。 影响视频播放进度的因素 总数据大小 = 一帧大小 x 每一秒播放的帧数（fps）x 时间 当我们做视频编解码、转换格式或滤镜处理等等时，如果出现过重新封装之后播放发现时间对不上了，那很可能你对这几个基本概念还不熟悉，看看这两篇补补？直播推流全过程：视频数据源之YUV（1） 、视音频数据处理入门：RGB、YUV像素数据处理 一帧大小：根据视频数据格式计算，如rgb=width x height x 3; yuv420=width x height x 3 / 2 ; ..... 帧率: 每一秒播放帧的数量，单位：fps（frame per second）。 比特率(码率)： 就是： 一帧大小 x 每一秒播放的帧数（fps）。 影响音频播放进度的因素 数据大小（单位：字节）= 采样率 x (位宽 / 8) x 声道数 x 时间 采样率：每秒钟采样次数，如：44100。 位宽：常见有8位、16位、32位；即对应1个字节、2个字节、4个字节。 声道数：常用单声道、双声道；即对应1个声道数，2个声道数。 还不是很熟悉的朋友请往这补补？直播推流全过程：音频数据源之PCM（2） 、视音频数据处理入门：PCM音频采样数据处理 同时播放视频和音频 这里所说是\"同时\"播放，而非同步，是因为我们并没有做同步处理，只是根据理论知识让它们各自播放，效果上展示出来同步的效果。 来实践一下？ 把 Kobe.flv 这个视频解码后做些滤镜等处理后得到yuv、pcm的裸数据后进行播放。解码过程参看：FFmpeg 简单实现转码 和 FFmpeg Transcode(转码) 。 解码后的数据如下： qincji:VAFFmpeg mac$ ls -lsh source/ 109864 -rw-r--r-- 1 mac staff 53M 1 16 11:29 Kobe-384x216-15fps.yuv 20672 -rw-r--r--@ 1 mac staff 10M 1 10 11:06 Kobe-44100-2-32-33ks.pcm 进行播放 我这使用Audition播放音频，设置如下： 若使用ffplay，则使用命令： ffplay -ar 44100 -channels 2 -f f32le -i source/Kobe-44100-2-32-33ks.pcm 使用ffplay播放视频 ffplay -f rawvideo -framerate 15 -video_size 384x216 source/Kobe-384x216-15fps.yuv 播放显示的同步效果： 编解码同步处理 首先我们先了解一下FFmpeg中有关显示处理的几个重要知识点： pts：用来显示的时间戳，单位：AVStream->time_base。 dts：用来解码的时间戳，单位：AVStream->time_base。 duration：数据时长，单位：AVStream->time_base。 编解码同步处理是指：音视频转码的过程（如格式转换、视频合并等）。需要把视频解封装、解码后重新编码、封装成新的文件，这个过程需要精准的处理pts、dts和duration才能保证播放的过程同步。 转码流程回顾 上图中三个具有颜色的点，红色是转码过程中必须要处理的，而绿色则根据实际情况而定。 以下处理的代码位置在FFmpeg Transcode(转码) 中。 1. 处理之前相关函数 1) AVRational time_base ：时间基；我们只需设置编码的时间基，其他框架自动生成。设置方式： 视频：(应保持跟解码保持一致) time_base = {1, fps}; //如time_base = {1, 30} 音频： time_base = {1, sample}; //如time_base = {1, 44100} 2）AVCodecContext->frame_size：音频帧中每个通道的样本数，跟nb_samples的值一样。所以： 一帧大小(单位：字节) = frame_size x 位宽 x 声道数 这里需要特别注意的是，编码时frame_size的大小跟采样率（sample_rate）并没有关系，它是跟音频编码器类型相关的，如： avcodec_open2 -> avctx->codec->init(avctx) 这将会调用AVCodec(编码器)中的对应的init，如AV_CODEC_ID_AAC编码器： AVCodec ff_aac_encoder = { .name = \"aac\", .long_name = NULL_IF_CONFIG_SMALL(\"AAC (Advanced Audio Coding)\"), .type = AVMEDIA_TYPE_AUDIO, .id = AV_CODEC_ID_AAC, .priv_data_size = sizeof(AACEncContext), .init = aac_encode_init, .encode2 = aac_encode_frame, .close = aac_encode_end, .defaults = aac_encode_defaults, .supported_samplerates = mpeg4audio_sample_rates, .caps_internal = FF_CODEC_CAP_INIT_THREADSAFE, .capabilities = AV_CODEC_CAP_SMALL_LAST_FRAME | AV_CODEC_CAP_DELAY, .sample_fmts = (const enum AVSampleFormat[]){ AV_SAMPLE_FMT_FLTP, AV_SAMPLE_FMT_NONE }, .priv_class = &aacenc_class, }; 然后会调用：aac_encode_init -> avctx->frame_size = 1024 ，这也就是当指定编码器后，怎么也改变不了frame_size的值的原因。 3) av_packet_rescale_ts : 简单理解为，将pkt的pts、dts和duration值从原来的，以tb_dst为标准转换为现在的有效。 /** * Convert valid timing fields (timestamps / durations) in a packet from one * timebase to another. Timestamps with unknown values (AV_NOPTS_VALUE) will be * ignored. * * @param pkt packet on which the conversion will be performed * @param tb_src source timebase, in which the timing fields in pkt are * expressed * @param tb_dst destination timebase, to which the timing fields will be * converted */ void av_packet_rescale_ts(AVPacket *pkt, AVRational tb_src, AVRational tb_dst); 我们从源码上来看： void av_packet_rescale_ts(AVPacket *pkt, AVRational src_tb, AVRational dst_tb) { if (pkt->pts != AV_NOPTS_VALUE) pkt->pts = av_rescale_q(pkt->pts, src_tb, dst_tb); if (pkt->dts != AV_NOPTS_VALUE) pkt->dts = av_rescale_q(pkt->dts, src_tb, dst_tb); if (pkt->duration > 0) pkt->duration = av_rescale_q(pkt->duration, src_tb, dst_tb); //忽略以下... } 2. 解码前处理（红色上AVPacket） 解码之前需要AVPacket的pts、dts、duration的值重新转换，如下： av_packet_rescale_ts(de_pkt, ifmt_ctx->streams[stream_index]->time_base, ifmt_ctx->streams[stream_index]->codec->time_base); 编码之后这几个数据值会传递给AVFrame，进行下去。 3. 裸数据处理（绿色AVFrame） 解码之后得到的AVFrame中装的就是裸数据，当我们需要对该数据进行加工后，结果可能是出现多个AVFrame数据，当出现这种情况时，需要重新计算当前AVFrame.pkt_dts和AVFrame.pts的值后，才能送去编码(avcodec_send_frame)，否则会造成音视频的不同步。 具体要怎么设置呢？哈哈，我也在找答案中…… 不过我在某个音频编码的源码中找到一段是这么转换的pts的： int ff_af_queue_add(AudioFrameQueue *afq, const AVFrame *f) { AudioFrame *new = av_fast_realloc(afq->frames, &afq->frame_alloc, sizeof(*afq->frames)*(afq->frame_count+1)); if(!new) return AVERROR(ENOMEM); afq->frames = new; new += afq->frame_count; /* get frame parameters */ new->duration = f->nb_samples; new->duration += afq->remaining_delay; if (f->pts != AV_NOPTS_VALUE) { //转换新的pts new->pts = av_rescale_q(f->pts, afq->avctx->time_base, (AVRational){ 1, afq->avctx->sample_rate }); //然后再减去 avctx->initial_padding 的值 new->pts -= afq->remaining_delay; if(afq->frame_count && new[-1].pts >= new->pts) av_log(afq->avctx, AV_LOG_WARNING, \"Queue input is backward in time\\n\"); } else { new->pts = AV_NOPTS_VALUE; } afq->remaining_delay = 0; /* add frame sample count */ afq->remaining_samples += f->nb_samples; afq->frame_count++; return 0; } // remaining_delay的值初始化 av_cold void ff_af_queue_init(AVCodecContext *avctx, AudioFrameQueue *afq) { afq->avctx = avctx; afq->remaining_delay = avctx->initial_padding; afq->remaining_samples = avctx->initial_padding; afq->frame_count = 0; } 上面计算新的pts的方式，我测试验证了一下，确实如此。 总结一下上面的情况（音频）： 也就是说当音频处理后出现多帧（AVFrame）的情况下，在编码(avcodec_send_frame)前，需要根据当前设置编码的time_base和sample_rate重新设置pts才能保证编码后的音频才不会\"变形\"。这里给我的解决方案（代码源同上）： //当解码的帧数与送去编码的帧数有差别时，我们必须更改pts和dts的值，否者封装时av_packet_rescale_ts（计算）pts和dts有重复，会出问题 if (filt_frame->nb_samples != enc_ctx->frame_size) { resample_pos += enc_ctx->frame_size; //编码前重新给pts和dts赋值 reasampling_frame->pts = resample_pos; reasampling_frame->pkt_dts = resample_pos; } 注：上面处理方案没有针对视频流测试过，但思路是一样的，看编码前的裸数据在你给定的fps播放下，总的帧数播放完跟原来时长是否一致，再然后定义新的pts。 4. 编码后处理（红色右AVPacket） enc_pkt->stream_index = stream_index; av_packet_rescale_ts(enc_pkt, ofmt_ctx->streams[stream_index]->codec->time_base, ofmt_ctx->streams[stream_index]->time_base); enc_pkt->pos = -1; 后话 造成音视频不同步的原因有很多种，学会分析其中的原因才是我们想要的，其中最好的方式就是看别人怎么做的。如果你在做播放的过程想参考一下音视频同步处理不妨看一下fftools/ffplay.c 。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/18_command.html":{"url":"FFmpeg/18_command.html","title":"FFmpeg命令使用指南","keywords":"","body":"FFmpeg命令使用指南FFmpeg主要工具ffmpeg1）帮助文档2）指令格式3）打印某个协议的具体信息（如h264）4）滤镜命令5）协议命令6）把多个图片合并视频命令7）合并视频命令ffprobe1）帮助文档2）指令格式ffplay1）帮助文档2）指令格式3）播放pcm数据4）播放yuv数据5）播放网络资源6）播放的其他重要参数FFmpeg命令使用指南 本文目的主要为了探讨如何使用命令，以及该如何去查找我们需要的命令，最后抛砖引玉示范几个案例。 FFmpeg主要工具 FFmpeg主要工具有ffmpeg、ffprobe和ffplay，它们分别用作编解码、内容分析和播放器。它们均可通过编译或者直接下载安装获得的应用程序。 ffmpeg 官方文档 ，如果英文不太好，推荐从《FFmpeg从入门到精通》书中查找中文含义。 源码所在：fftools/ffmpeg.c 以下分析ffmpeg工具帮助文档，以便掌握使用命令的技巧 1）帮助文档 通常拿到工具的第一步看帮助文档： ffmpeg --help 当内容太多时，我们需要把打印的信息保存到文件中，再拿工具看： ffmpeg --help > ffmpeg_help.txt 内容如下： Hyper fast Audio and Video encoder usage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}... Getting help: -h -- print basic options -h long -- print more options -h full -- print all options (including all format and codec specific options, very long) -h type=name -- print all options for the named decoder/encoder/demuxer/muxer/filter/bsf See man ffmpeg for detailed description of the options. Print help / information / capabilities: -L show license -h topic show help -? topic show help -help topic show help --help topic show help -version show version -buildconf show build configuration -formats show available formats -muxers show available muxers -demuxers show available demuxers -devices show available devices -codecs show available codecs -decoders show available decoders -encoders show available encoders -bsfs show available bit stream filters -protocols show available protocols -filters show available filters -pix_fmts show available pixel formats -layouts show standard channel layouts -sample_fmts show available audio sample formats -colors show available color names -sources device list sources of the input device -sinks device list sinks of the output device -hwaccels show available HW acceleration methods Global options (affect whole program instead of just one file: -loglevel loglevel set logging level -v loglevel set logging level -report generate a report -max_alloc bytes set maximum size of a single allocated block -y overwrite output files -n never overwrite output files -ignore_unknown Ignore unknown stream types -filter_threads number of non-complex filter threads -filter_complex_threads number of threads for -filter_complex -stats print progress report during encoding -max_error_rate maximum error rate ratio of errors (0.0: no errors, 1.0: 100% errors) above which ffmpeg returns an error instead of success. -bits_per_raw_sample number set the number of bits per raw sample -vol volume change audio volume (256=normal) Per-file main options: -f fmt force format -c codec codec name -codec codec codec name -pre preset preset name -map_metadata outfile[,metadata]:infile[,metadata] set metadata information of outfile from infile -t duration record or transcode \"duration\" seconds of audio/video -to time_stop record or transcode stop time -fs limit_size set the limit file size in bytes -ss time_off set the start time offset -sseof time_off set the start time offset relative to EOF -seek_timestamp enable/disable seeking by timestamp with -ss -timestamp time set the recording timestamp ('now' to set the current time) -metadata string=string add metadata -program title=string:st=number... add program with specified streams -target type specify target file type (\"vcd\", \"svcd\", \"dvd\", \"dv\" or \"dv50\" with optional prefixes \"pal-\", \"ntsc-\" or \"film-\") -apad audio pad -frames number set the number of frames to output -filter filter_graph set stream filtergraph -filter_script filename read stream filtergraph description from a file -reinit_filter reinit filtergraph on input parameter changes -discard discard -disposition disposition Video options: -vframes number set the number of video frames to output -r rate set frame rate (Hz value, fraction or abbreviation) -s size set frame size (WxH or abbreviation) -aspect aspect set aspect ratio (4:3, 16:9 or 1.3333, 1.7777) -bits_per_raw_sample number set the number of bits per raw sample -vn disable video -vcodec codec force video codec ('copy' to copy stream) -timecode hh:mm:ss[:;.]ff set initial TimeCode value. -pass n select the pass number (1 to 3) -vf filter_graph set video filters -ab bitrate audio bitrate (please use -b:a) -b bitrate video bitrate (please use -b:v) -dn disable data Audio options: -aframes number set the number of audio frames to output -aq quality set audio quality (codec-specific) -ar rate set audio sampling rate (in Hz) -ac channels set number of audio channels -an disable audio -acodec codec force audio codec ('copy' to copy stream) -vol volume change audio volume (256=normal) -af filter_graph set audio filters Subtitle options: -s size set frame size (WxH or abbreviation) -sn disable subtitle -scodec codec force subtitle codec ('copy' to copy stream) -stag fourcc/tag force subtitle tag/fourcc -fix_sub_duration fix subtitles duration -canvas_size size set canvas size (WxH or abbreviation) -spre preset set the subtitle options to the indicated preset 2）指令格式 usage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}... 这就是ffmpeg指令格式，-i前面的options(选择)指定输入文件，[outfile options]指定输出文件。举个例子：（官方文档的）将输入文件的帧速率强制为1fps，将输出文件的帧速率强制为24fps，进行转换： ffmpeg -r 1 -i Kobe_in.flv -r 24 Kobe_out.avi 其中-r是作用转换帧率，描述：-r rate set frame rate (Hz value, fraction or abbreviation)。 输入和输出对比一下： 这是输入： Duration: 00:00:30.00, start: 0.000000, bitrate: 221 kb/s Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 182 kb/s, 15.07 fps, 15 tbr, 1k tbn, 30 tbc 这是输出： Input #0, avi, from 'Kobe_out.avi': Metadata: encoder : Lavf58.29.100 Duration: 00:07:30.04, start: 0.000000, bitrate: 62 kb/s Stream #0:0: Video: mpeg4 (Simple Profile) (FMP4 / 0x34504D46), yuv420p, 384x216 [SAR 1:1 DAR 16:9], 44 kb/s, 24 fps, 24 tbr, 24 tbn, 24 tbc 3）打印某个协议的具体信息（如h264） -h type=name -- print all options for the named decoder/encoder/demuxer/muxer/filter/bsf 就是这一行有点难理解，-h type=name，这是指定对类型decoder/encoder/...中某个类型的\"选项\"信息，举一个例子就知道，如： ffmpeg -h decoder=h264 打印信息如下： Decoder h264 [H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10]: General capabilities: dr1 delay threads Threading capabilities: frame and slice Supported hardware devices: videotoolbox H264 Decoder AVOptions: -enable_er .D.V..... Enable error resilience on damaged frames (unsafe) (default auto) -x264_build .D.V..... Assume this x264 version if no x264 version found in any SEI (from -1 to INT_MAX) (default -1) 而从哪里知道有哪些decoder呢？ffmpeg -help时显示出-decoders show available decoders，也就是可以通过这个命令打印的： ffmpeg -decoders 其他encoder、...、filter也是一样的操作，先通过找有哪些存在的，然后再打印，又如filter: ffmpeg -filters 找到有 acopy，然后查看它的具体用法： ffmpeg -h filter=acopy 4）滤镜命令 官方文档 1）我们从上面的帮助文档(ffmpeg -h)中找到有关滤镜输出的，分别是视频滤镜输出，以及音频滤镜输出： -vf filter_graph set video filters 和 -af filter_graph set audio filters ，而通过上一步ffmpeg -filters找到处理视频的boxblur（动态模糊）以及处理音频的aecho（回声处理）两条，然后分别打印两个的用法如下： ffmpeg -h filter=boxblur 输出如下： Filter boxblur Blur the input. Inputs: #0: default (video) Outputs: #0: default (video) boxblur AVOptions: luma_radius ..FV..... Radius of the luma blurring box (default \"2\") lr ..FV..... Radius of the luma blurring box (default \"2\") luma_power ..FV..... How many times should the boxblur be applied to luma (from 0 to INT_MAX) (default 2) lp ..FV..... How many times should the boxblur be applied to luma (from 0 to INT_MAX) (default 2) chroma_radius ..FV..... Radius of the chroma blurring box cr ..FV..... Radius of the chroma blurring box chroma_power ..FV..... How many times should the boxblur be applied to chroma (from -1 to INT_MAX) (default -1) cp ..FV..... How many times should the boxblur be applied to chroma (from -1 to INT_MAX) (default -1) alpha_radius ..FV..... Radius of the alpha blurring box ar ..FV..... Radius of the alpha blurring box alpha_power ..FV..... How many times should the boxblur be applied to alpha (from -1 to INT_MAX) (default -1) ap ..FV..... How many times should the boxblur be applied to alpha (from -1 to INT_MAX) (default -1) This filter has support for timeline through the 'enable' option. 通过以上说明，我们设置视频滤镜处理为：boxblur=luma_radius=3:luma_power=8。 以及音频： ffmpeg -h filter=aecho 输出如下： Filter aecho Add echoing to the audio. Inputs: #0: default (audio) Outputs: #0: default (audio) aecho AVOptions: in_gain ..F.A.... set signal input gain (from 0 to 1) (default 0.6) out_gain ..F.A.... set signal output gain (from 0 to 1) (default 0.3) delays ..F.A.... set list of signal delays (default \"1000\") decays ..F.A.... set list of signal decays (default \"0.5\") 通过上面说明我们设置音频滤镜处理为：aecho=0.8:0.88:60:0.4 或者 aecho=in_gain=0.8:out_gain=0.88:delays=60:decays=0.4 。 2）合并两条处理命令如下： ffmpeg -i Kobe_in.flv -vf boxblur=luma_radius=3:luma_power=8 -af aecho=0.8:0.88:60:0.4 Kobe_out.flv 3）如果使用多条滤镜指令则使用,进行分割，如添加视频水平翻转(hflip)如下： ffmpeg -i Kobe_in.flv -vf boxblur=luma_radius=1:luma_power=8,hflip Kobe_out.flv 4）其他：滤镜处理是非常非常丰富的，例如：高斯模糊、3D降噪、添加logo、视频截切、视频反转、滤镜以及各种各样的特性（近200种处理）。 5）协议命令 官方文档 首先我们得知道在FFmpeg中所有的输入源都是一种协议(protocol)，如file、http、rtmp、concat等等，这都是一种协议格式，所以网络资源等同于本地文件一样处理就好了。 通过开始时的帮助信息，我们使用ffmpeg -h full输出所有帮助信息，然后我们对一下进行测试： 1）将flv格式转换成fps为60的git格式进行输出，我们先找到git的封装信息： GIF muxer AVOptions: -loop E........ Number of times to loop the output: -1 - no loop, 0 - infinite loop (from -1 to 65535) (default 0) -final_delay E........ Force delay (in centiseconds) after the last frame (from -1 to 65535) (default -1) 注：AVOptions是针对该类型的\"选项\"参数，上面打印的帮助日志包含了非常多的AVOptions，自己多尝试两边就知道用法了。 2）通过上面AVOptions指定封装成GIF的\"选项\"进行输出，如下： ffmpeg -r 60 -i Kobe_in.flv -r 60 -loop 0 output.GIF 3）同样，当我们把网络数据进行转换到本地文件，便可以这样： ffmpeg -r 60 -i rtmp://58.200.131.2:1935/livetv/hunantv -r 60 -loop 0 output.GIF 4.1）推送道理也一样，只是将本地协议的格式转为远程协议的格式（将mp4转换成flv格式推送到rtmp服务器）： ffmpeg -r 60 -i lol.mp4 -r 60 -f flv rtmp://8.129.163.125:1935/myapp/testpush 注：这里特意说一下-f的使用，强制使用格式，如这里的flv是flv muxer AVOptions: 4.2）然后可以通过ffplay进行拉流播放： ffplay rtmp://8.129.163.125:1935/myapp/testpush 6）把多个图片合并视频命令 使用-f指定image2解封装格式，解封装-pattern_type\"选项\"的值glob_sequence、从下表为5开始，最多200张作为输入。输入fps为15，用x264进行编码输出mp4： ffmpeg -f image2 -pattern_type glob_sequence -start_number 5 -start_number_range 200 -framerate 12 -i 'loading/loading%d.png' -r 15 -vcodec libx264 out.mp4 7）合并视频命令 首先要明确合并的效果，以及合并的输入流。明确是否需要重新编解码、滤镜处理等等。这就是需要看输入的类型了。比如当两中尺寸不等，编码格式不同的两个输入文件，如果合并时，是否需要把尺寸统一、编码统一，然后才能进行合并。 _________ | | | input 0 |\\ |_________| \\ \\ _________ \\ | | _________ \\| decode? | _________ | | | | | | | input 1 |---->| filter? |--->| output | |_________| | | |_________| /| encode? | / | ... | _________ / |_________| | | / | input 2 |/ |_________| 指令使用看：FFMpeg无损合并视频的多种方法 （原文没找到），为了方便看，搬过来： 1）方法一：FFmpeg concat协议 对于 MPEG 格式的视频，可以直接连接： ffmpeg -i \"concat:input1.mpg|input2.mpg|input3.mpg\" -c copy output.mpg 对于非 MPEG 格式容器，但是是 MPEG 编码器（H.264、DivX、XviD、MPEG4、MPEG2、AAC、MP2、MP3 等），可以包装进 TS 格式的容器再合并。在新浪视频，有很多视频使用 H.264 编码器，可以采用这个方法 ffmpeg -i input1.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input1.ts ffmpeg -i input2.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input2.ts ffmpeg -i input3.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input3.ts ffmpeg -i \"concat:input1.ts|input2.ts|input3.ts\" -c copy -bsf:a aac_adtstoasc -movflags +faststart output.mp4 保存 QuickTime/MP4 格式容器的时候，建议加上 -movflags +faststart。这样分享文件给别人的时候可以边下边看。 2）方法二：FFmpeg concat 分离器 这种方法成功率很高，也是最好的，但是需要 FFmpeg 1.1 以上版本。先创建一个文本文件 filelist.txt： file 'input1.mkv' file 'input2.mkv' file 'input3.mkv' 然后： ffmpeg -f concat -i filelist.txt -c copy output.mkv 注意：使用 FFmpeg concat 分离器时，如果文件名有奇怪的字符，要在 filelist.txt 中转义。 3）方法三：Mencoder 连接文件并重建索引 这种方法只对很少的视频格式生效。幸运的是，新浪视频使用的 FLV 格式是可以这样连接的。对于没有使用 MPEG 编码器的视频（如 FLV1 编码器），可以尝试这种方法，或许能够成功。 mencoder -forceidx -of lavf -oac copy -ovc copy -o output.flv input1.flv input2.flv input3.flv 4）方法四：使用 FFmpeg concat 过滤器重新编码（有损） 语法有点复杂，但是其实不难。这个方法可以合并不同编码器的视频片段，也可以作为其他方法失效的后备措施。 ffmpeg -i input1.mp4 -i input2.webm -i input3.avi -filter_complex '[0:0] [0:1] [1:0] [1:1] [2:0] [2:1] concat=n=3:v=1:a=1 [v] [a]' -map '[v]' -map '[a]' output.mkv 如你所见，上面的命令合并了三种不同格式的文件，FFmpeg concat 过滤器会重新编码它们。 注意这是有损压缩。 [0:0] [0:1] [1:0] [1:1] [2:0] [2:1] 分别表示第一个输入文件的视频、音频、第二个输入文件的视频、音频、第三个输入文件的视频、音频。 concat=n=3:v=1:a=1 表示有三个输入文件，输出一条视频流和一条音频流。 [v] [a] 就是得到的视频流和音频流的名字，注意在 bash 等 shell 中需要用引号，防止通配符扩展。 提示: 以上三种方法，在可能的情况下，最好使用第二种。第一种次之，第三种更次。第四种是后备方案，尽量避免。规格不同的视频合并后可能会有无法预测的结果。有些媒体需要先分离视频和音频，合并完成后再封装回去。对于 Packed B-Frames 的视频，如果封装成 MKV 格式的时候提示 Can't write packet with unknown timestamp，尝试在 FFmpeg 命令的 ffmpeg 后面加上 -fflags +genpts 5）了解了上面情况下，我们合并两个尺寸不一样的视频，lol_640x360.mp4与Kobe_384x216.flv合并成out_640x360.avi。思路：我们通过把两个尺寸统一成一致（处理方案很多），然后转码成统一格式.ts，使用concat协议输出到一个文件。 a. 使用scale滤镜把Kobe_384x216.flv放大成lol_640x360.mp4尺寸，setsar(像素比)&setdar （宽高比），vcodec（h264编码），acodec（aac编码），不然默认其他编码（还应该制定码率什么的，这里从简了）： ffmpeg -i Kobe_384x216.flv -vf scale=w=640:h=360,setsar=sar=1/1,setdar=dar=16/9 -vcodec libx264 -acodec aac Kobe_640x360.flv 注：setsar等时通过使用ffprobe查看lol_640x360.mp4文件所知道的，如： Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 256 kb/s, 15 fps, 15 tbr, 15360 tbn, 30 tbc (default) b. 统一转码成.ts格式： ffmpeg -i Kobe_640x360.flv -c copy -bsf:v h264_mp4toannexb -f mpegts input1.ts ffmpeg -i lol_640x360.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts input2.ts c. 使用concat协议输出到一个文件： ffmpeg -i \"concat:input1.ts|input2.ts\" -c copy -bsf:a aac_adtstoasc -movflags +faststart out_640x360.avi ffprobe 官方文档 源码所在：fftools/ffprobe.c ffprobe作为FFmpeg的分析工具，内容非常强大，可以说是能分析输入源一切的信息。 1）帮助文档 同上操作，先看帮助文档： ffprobe --help 这里就不贴出来了，而且很多跟ffmpeg都是一样的。 2）指令格式 usage: ffprobe [OPTIONS] [INPUT_FILE] 最简单使用就是直接忽略[OPTIONS]，如： ffprobe Kobe.flv 输出结果如下： Input #0, flv, from 'Kobe.flv': Metadata: metadatacreator : iku hasKeyframes : true hasVideo : true hasAudio : true hasMetadata : true canSeekToEnd : false datasize : 828648 videosize : 690936 audiosize : 133316 lasttimestamp : 30 lastkeyframetimestamp: 25 lastkeyframelocation: 696106 Duration: 00:00:30.00, start: 0.000000, bitrate: 221 kb/s Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 182 kb/s, 15.07 fps, 15 tbr, 1k tbn, 30 tbc Stream #0:1: Audio: aac (HE-AAC), 44100 Hz, stereo, fltp, 33 kb/s 其他也非常简单明了，如查看packets信息-show_packets show packets info： ffprobe -show_packets Kobe.flv 输出将是所有packet信息，这里拿出一小段： [PACKET] codec_type=video stream_index=0 pts=29533 pts_time=29.533000 dts=29533 dts_time=29.533000 duration=66 duration_time=0.066000 convergence_duration=N/A convergence_duration_time=N/A size=777 pos=818689 flags=__ [/PACKET] ffplay 官方文档 源码所在：fftools/ffplay.c 这是一个非常非常强大的播放器，市面上的视频格式都支持播放，而且播放裸流数据一样是可以的，如pcm音频，yuv视频、rgb视频等。 1）帮助文档 同上操作，先看帮助文档： ffplay --help 这里就不贴出来了，而且很多跟ffmpeg都是一样的。 2）指令格式 usage: ffplay [options] input_file 最简单使用就是直接忽略[OPTIONS]，如： ffplay Kobe.flv 3）播放pcm数据 关于裸数据的播放，就必须要设置这几个参数：采样率、声道数、位宽、值的范围(fmt)和排序方式(大端/小端)。原因就是：一个存pcm裸流数据的文件是不包含任何参数的，只是按顺序存放一个个采样点的值，具体请看：直播推流全过程：音频数据源之PCM（2） 。 1）我们先从Kobe.flv解码出pcm裸流数据，存储到文件中，我们通过ffprobe Kobe.flv命令参看解码前的pcm信息如下： Stream #0:1: Audio: aac (HE-AAC), 44100 Hz, stereo, fltp, 33 kb/s 注：可以直接通过FFmpeg Decode（解码） 生成pcm裸流数据。 2）所以解码后得到的pcm数据信息为：采样率=44100，声道数=2，位宽=4，值的范围(fmt)=fltp（32位浮点类型），排序方式=小端。根据这几个英文意思，从帮助文档里面找到先关的几条\"选项\"如下： -ar ED..A.... set audio sampling rate (in Hz) (from 0 to INT_MAX) (default 0) -f fmt force format f32be demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) f32le demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) s32be demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) u32be demuxer AVOptions: -sample_rate .D....... (from 0 to INT_MAX) (default 44100) -channels .D....... (from 0 to INT_MAX) (default 1) 3）这里特意多取几个进行介绍一下： f32be=浮点类型32位大端排序；f32le=浮点类型32位小端排序；s32be=带符号整形32位大端排序；u32be=无符号整形32位大端排序。相信你一下子就能找到规律了。 第一个字母：f为浮点类型，s为带符号整形，u为无符号整形； 中间数字：表示占位，有8、16、24、32、64可选。 4）所以，最后播放pcm命令为： ffplay -ar 44100 -channels 2 -f f32le Kobe-44100-2-32-33ks.pcm 4）播放yuv数据 跟pcm一样，分析过程并无多大区别。而且yuv相关更少，只有跟像素、yuv类型以及fps相关。 1）我们先从Kobe.flv解码出yuv裸流数据，存储到文件中，我们通过ffprobe Kobe.flv命令参看解码前的yuv信息如下： Stream #0:0: Video: h264 (Main), yuv420p(progressive), 384x216 [SAR 1:1 DAR 16:9], 182 kb/s, 15.07 fps, 15 tbr, 1k tbn, 30 tbc 2）所以解码后得到的yuv数据信息为：像素=384x216，yuv类型=yuv420p，fps=15。根据这几个英文意思，从帮助文档里面找到先关的几条\"选项\"如下： rawvideo demuxer AVOptions: -video_size .D....... set frame size -pixel_format .D....... set pixel format (default \"yuv420p\") -framerate .D....... set frame rate (default \"25\") colorspace AVOptions: format ..FV..... Output pixel format (from -1 to 164) (default -1) yuv420p ..FV..... yuv420p10 ..FV..... yuv420p12 ..FV..... yuv422p ..FV..... yuv422p10 ..FV..... yuv422p12 ..FV..... yuv444p ..FV..... yuv444p10 ..FV..... yuv444p12 ..FV..... 3）所以，最后播放yuv命令为： ffplay -f rawvideo -video_size 384x216 -pixel_format yuv420p -framerate 15 Kobe-384x216-15fps.yuv 5）播放网络资源 上面我们叫的输入源都被称为一种协议格式，跟本地文件（如.flv等）处理是一样的，所以播放网络资源后面直接跟地址就行了，如： rtmp://58.200.131.2:1935/livetv/hunantv 6）播放的其他重要参数 1）打印日志，这有助于我们分析问题，或者阅读源码： -v loglevel set logging level 参数\"选项\"： \"quiet\" \"panic\" \"fatal\" \"error\" \"warning\" \"info\" \"verbose\" \"debug\" \"trace\" 使用： ffplay -v \"debug\" lol.mp4 2）生成日志 -report generate a report 使用： ffplay -v \"debug\" `-report lol.mp4 3）一些常规的播放设置相关 -x width force displayed width //指定宽度 -y height force displayed height //指定高度 -s size set frame size (WxH or abbreviation) //设置帧的大小（过时？），使用：video_size？ -fs force full screen //全屏播放 -an disable audio //禁用声音 -vn disable video //禁用视频 -sn disable subtitling //禁用字幕 -ss pos seek to a given position in seconds //从指定开始播放（跳过多少秒） -t duration play \"duration\" seconds of audio/video //播放多长时间（单位秒） -bytes val seek by bytes 0=off 1=on -1=auto -seek_interval seconds set seek interval for left/right keys, in seconds //设置按下左右键时快退/快进多少秒 -nodisp disable graphical display //播放时无显示（可用于调试） -noborder borderless window //无边框播放 -alwaysontop window always on top //窗口总在最上面 -volume volume set startup volume 0=min 100=max //设置音量（0——100），0为静音 -f fmt force format //强制播放的格式 -window_title window title set window title //播放器窗口添加标题显示 -af filter_graph set audio filters //播放前添加音频滤镜处理后再播放 -vf filter_graph set video filters //播放前添加视频滤镜处理后再播放 -showmode mode select show mode (0 = video, 1 = waves, 2 = RDFT) //画面显示的模式 -i input_file read specified file //指定输入文件（可以忽略） -codec decoder_name force decoder //指定解码器 其中播放前的滤镜处理跟ffmpeg使用一样，只用替换成ffplay即可，如： ffplay -vf boxblur=luma_radius=3:luma_power=8 -af aecho=0.8:0.88:60:0.4 Kobe.flv Kobi.flv下载 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/19_swscale.html":{"url":"FFmpeg/19_swscale.html","title":"Swscale（图像转换）","keywords":"","body":"Swscale（图像转换）FFmpeg图像转换流程libswscale库代码例子Swscale（图像转换） 本文对应官方例子：doc/examples/scaling_video.c。 FFmpeg图像转换流程 只有三个简单的函数就可以实现了，而在FFmpeg Transcode(转码) 中的应用就是对应着处理音频重采样的地方。重采样是对解码后的裸流数据（AVFrame中的data）进行格式转换等，而图像转换则是视频对解码后的裸流数据（AVFrame中的data）进行格式转换。 libswscale库 该库就是对图片转换等处理，提供了高级别的图像转换API，例如它允许进行图像缩放和像素格式转换等大量；根音频对应的处理库是swresample，它允许操作音频采样、音频通道布局转换与布局调整等。 我们平常也可以通过学习这些源码，来理解这些格式；也可以把里面的函数实现拷贝出来当做自己常规处理的工具。 代码例子 /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/18 * description: 来着官方例子：doc/examples/transcoding.c * */ #define __STDC_CONSTANT_MACROS extern \"C\" { #include #include #include } int main(int argc, char **argv) { uint8_t *src_data[4], *dst_data[4]; int src_linesize[4], dst_linesize[4]; //384x216 int src_w = 384, src_h = 216, dst_w = src_w, dst_h = src_h; enum AVPixelFormat src_pix_fmt = AV_PIX_FMT_YUV420P, dst_pix_fmt = AV_PIX_FMT_YUV444P; const char *dst_filename = NULL, *src_filename = NULL; FILE *dst_file, *src_file; struct SwsContext *sws_ctx; int i, ret; //输入文件从上几篇文章中的Kobe.flv解码而来的yuv420p数据 src_filename = \"source/Kobe-384x216-yuv420p.yuv\"; dst_filename = \"output/Kobe-384x216-yuv44p.yuv\"; remove(dst_filename); src_file = fopen(src_filename, \"rb+\"); dst_file = fopen(dst_filename, \"wb+\"); if (!dst_file) { fprintf(stderr, \"Could not open destination file %s\\n\", dst_filename); exit(1); } /**1. 获取SwsContext*/ /* create scaling context */ sws_ctx = sws_getContext(src_w, src_h, src_pix_fmt, dst_w, dst_h, dst_pix_fmt, SWS_BILINEAR, NULL, NULL, NULL); if (!sws_ctx) { fprintf(stderr, \"Impossible to create scale context for the conversion \" \"fmt:%s s:%dx%d -> fmt:%s s:%dx%d\\n\", av_get_pix_fmt_name(src_pix_fmt), src_w, src_h, av_get_pix_fmt_name(dst_pix_fmt), dst_w, dst_h); ret = AVERROR(EINVAL); goto end; } /**2.1 申请输入方内存，并初始化大小（对应AVFrame中的data和linesize）*/ /* allocate source and destination image buffers */ if ((ret = av_image_alloc(src_data, src_linesize, src_w, src_h, src_pix_fmt, 16)) Kobe.flv Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/20_subtitle_ffmpeg.html":{"url":"FFmpeg/20_subtitle_ffmpeg.html","title":"FFmpeg添加字幕的详细操作","keywords":"","body":"FFmpeg添加字幕的详细操作1. 硬字幕和软字幕的简介1.1. 硬字幕1.2. 软字幕2. SRT和ASS字幕格式的简介2.1. SRT（SubRip Subtitle）2.2. ASS（Advanced SubStation Alpha）3. 使用FFmpeg添加字幕的流程3.1. 编译带有libass的FFmpeg3.2. 生成字幕命令3.2.1 SRT硬字幕命令3.2.2 ASS硬字幕命令3.2.3 SRT和ASS软字幕命令3.2.4 SRT可以转ASS命令3.2.3 多通道(软)字幕命令3.2.4 多通道(软)字幕，中英字幕实现FFmpeg添加字幕的详细操作 在视频中添加字幕可以使视频更具可读性，并为观众提供更好的观看体验，这在多语种内容中尤为重要。FFmpeg是一个流行的开源视频处理工具，它可以被用来给视频添加字幕。本文将介绍FFmpeg集成libass的编译流程，介绍SRT和ASS字幕格式及其参数，如何使用FFmpeg向视频添加硬字幕和软字幕，并通过示例演示如何生成单一字幕和多项字幕。 而本文最后实现的效果为：默认中英文字幕，可选英文字幕的软字幕的实现： 1. 硬字幕和软字幕的简介 1.1. 硬字幕 将字幕渲染到视频的纹理上，然后将其编码成独立于视频格式的一个完整的视频。硬字幕不能更改或删除，因为它们与视频（通道）是一个整体。 1.2. 软字幕 在播放视频时实时渲染和读取。软字幕可以在播放过程中随时添加或删除。软字幕比硬字幕更加灵活，因为它们可以随时进行修改，但它们也需要高性能的播放器支持。 软字幕单独生成一个字幕通道，与视频 、音频一样，如以下Stream #0:2： Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 852x480, 397 kb/s, 25 fps, 25 tbr, 12800 tbn (default) Metadata: handler_name : VideoHandler vendor_id : [0][0][0][0] Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 127 kb/s (default) Metadata: handler_name : SoundHandler vendor_id : [0][0][0][0] Stream #0:2[0x3](eng): Subtitle: mov_text (tx3g / 0x67337874), 0 kb/s (default) Metadata: handler_name : SubtitleHandler 2. SRT和ASS字幕格式的简介 2.1. SRT（SubRip Subtitle） 是一种简单的字幕格式，主要由时间戳和文本组成。它通常用于简单的字幕文件，如电影聚会之类。SRT字幕格式的参数如下： 标题的计数器/索引 START和END：字幕开始和字幕结束的时间戳，格式为 \"小时：分钟：毫秒\"。 TEXT：在此时间戳范围内显示的字幕文本 一行空白表示一个结束 1 00:00:0,000 --> 00:00:2,000 这是0到2秒显示的字幕 2 00:00:2,000 --> 00:00:4,000 这是2到4秒显示的字幕 3 00:00:4,000 --> 00:00:7,000 这是4到7秒显示的字幕 2.2. ASS（Advanced SubStation Alpha） 是一个高级的字幕格式，它可以支持更多的样式和控制，比如，更改颜色、字体和大小，还可以通过几何变换来控制字幕的位置。ASS字幕格式包含以下参数： 2.2.1. 样式: Style: 序号, 字体名称, 字号, 颜色, 阴影, 边框, 描边, 阴影, 抗锯齿, 倾斜度, weight, underline 例子：Style: Top, Microsoft YaHei,40,&H00F5FF&,-1,2,0,0,1,0,0 2.2.2. 对齐: Dialogue: 0,0:00:03.42,0:00:04.91,Top,,0,0,0,,{\\an6}本字幕居右上 其中\\an6表示右上角对齐，默认是左下角对齐。 2.2.3. 触发器: Dialogue: 0,0:00:03.42,0:00:04.91,Top,,0,0,0,,{\\t(0,300,\\fade(400,400))}三秒内渐入渐出 其中\\fade(400,400)表示透明度从0渐变到400再从400渐变到0。 2.2.4. 动画: Dialogue: 0,0:00:03.42,0:00:04.91,Top,,0,0,0,,{\\move(0,0,100,100)}右下角移动 其中\\move(0,0,100,100)表示从(0,0)移动到(100,100)。 2.2.5. 特殊效果： Dialogue: 0,0:00:03.42,0:00:05.62,Top,,0,0,0,,{\\fad(500,500)\\blur3}左右淡入淡出，模糊度3 其中\\blur3表示模糊度为3。 2.2.6. 合的ASS字幕案例： [Script Info] ; Script generated by FFmpeg/Lavc59.18.100 Title: 某电影 Original Script: 某人 ScriptType: v4.00+ WrapStyle: 0 Collisions: Normal PlayResX: 1920 PlayResY: 1080 Timer: 100.0000 [V4 Styles] Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding Style: Default,Arial,30,&Hffffff,&Hffffff,&H0,&H0,0,0,0,0,100,100,0,0,1,1,0,2,10,10,50,0 [Events] Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text Dialogue: 0,0:00:00.00,0:00:02.00,Default,,0,0,0,,这是0到2秒显示的{\\b1}{\\i1}字幕{\\i0}{\\b0} Dialogue: 0,0:00:02.00,0:00:04.00,Default,,0,0,0,,这是2到4秒显示的字幕 Dialogue: 0,0:00:04.00,0:00:07.00,Default,,0,0,0,,这是4到7秒显示的字幕 Dialogue: 0,0:00:07.00,0:00:10.00,Default,,0,0,0,,这是7到10秒显示的字幕 Dialogue: 0,0:00:10.00,0:00:30.00,Default,,0,0,0,,这是10到30秒显示的字幕{\\an5\\move(960,700,960,900)\\t(\\fscx200\\fscy200\\frx360\\fry360\\fad(200,200))\\fs60\\p5\\t(\\fscx100\\fscy100\\frx0\\fry0\\fad(200,200))\\c&HFFFFFF&\\3c&HA000&}高级操作. 在这个案例中，我们可以看到SCRIPT INFO中定义了脚本的一些基本信息，V4 Styles中定义了字幕样式，Events中定义了具体的字幕内容和效果。 在字幕内容中，使用了\\an5表示居中上对齐，\\move(960,700,960,900)表示在x=960，y从700移动到900，\\t()表示特效，\\fscx表示X轴缩放，\\fscy表示Y轴缩放，\\frx表示X轴旋转，\\fry表示Y轴旋转，\\fs表示字号大小，\\p表示描边的点数，\\c表示字体颜色，\\3c表示描边的颜色。 此外，在这个案例中还使用了透明度、渐变、模糊等各种效果，可以看出ASS字幕的灵活性和功能性非常强大。 3. 使用FFmpeg添加字幕的流程 3.1. 编译带有libass的FFmpeg 下载FFmpeg6.0源码 下载libass，解压到FFmpeg源码目录，并且重命名为libass 在FFmpeg根目录编写编译脚本，并且执行 function build() { ./configure \\ --prefix=$PREFIX \\ --target-os=$PLATFORM \\ --disable-static \\ --enable-libass \\ --enable-shared || exit 0 make clean make install } PREFIX=/usr/local/ffmpeg #PLATFORM=linux PLATFORM=darwin build 编译完成后，查看版本信息：/usr/local/ffmpeg/bin/ffmpeg -version，可见启用了--enable-libass MacBook-Pro:FFmpeg-n6.0 mac$ /usr/local/ffmpeg/bin/ffmpeg -version ffmpeg version 6.0 Copyright (c) 2000-2023 the FFmpeg developers built with Apple clang version 14.0.0 (clang-1400.0.29.202) configuration: --prefix=/usr/local/ffmpeg --target-os=darwin --disable-static --enable-libass --enable-libass --enable-gpl --extra-cflags=-I/usr/local/ffmpeg/libx264/include --extra-ldflags=-L/usr/local/ffmpeg/libx264/lib --enable-libx264 --enable-shared libavutil 58. 2.100 / 58. 2.100 libavcodec 60. 3.100 / 60. 3.100 libavformat 60. 3.100 / 60. 3.100 libavdevice 60. 1.100 / 60. 1.100 libavfilter 9. 3.100 / 9. 3.100 libswscale 7. 1.100 / 7. 1.100 libswresample 4. 10.100 / 4. 10.100 libpostproc 57. 1.100 / 57. 1.100 可以配置环境变量，方便使用 3.2. 生成字幕命令 3.2.1 SRT硬字幕命令 ffmpeg -i input.mp4 -vf subtitles=subtitle.srt output_srt.mp4 3.2.2 ASS硬字幕命令 ffmpeg -i input.mp4 -vf ass=subtitle.ass output_ass.mp4 3.2.3 SRT和ASS软字幕命令 ffmpeg -i input.mp4 -i subtitle.srt -c copy -c:s mov_text -metadata:s:s:0 language=chi ouptut_chi.mp4 or ffmpeg -i input.mp4 -i subtitle.ass -c copy -c:s mov_text -metadata:s:s:0 language=chi ouptut_chi.mp4 3.2.4 SRT可以转ASS命令 ffmpeg -i subtitle.srt subtitle.ass 3.2.3 多通道(软)字幕命令 1. 准备字幕文件：假设有中文字幕文件为ch.srt，英文字幕文件为en.srt。 中文字幕文件：ch.srt ``` 1 00:00:0,000 --> 00:00:5,000 这里是视频的标题 1 2 00:00:5,000 --> 00:00:10,000 这里是视频的标题 2 - 英文字幕文件：`en.srt` 1 00:00:0,000 --> 00:00:5,000 Here's the video's title 1 2 00:00:5,000 --> 00:00:10,000 Here's the video's title 2 **2. 命令** ```shell ffmpeg -i input.mp4 -i ch.srt -i en.srt -map 0 -map 1 -map 2 -c copy -c:s mov_text -metadata:s:s:0 language=chi -metadata:s:s:1 language=eng output_chi_eng.mp4 注意：ass格式同样的操作 ffmpeg -i input.mp4 -i ch.ass -i en.ass -map 0 -map 1 -map 2 -c copy -c:s mov_text -metadata:s:s:0 language=chi -metadata:s:s:1 language=eng output_chi_eng_ass.mp4 3.2.4 多通道(软)字幕，中英字幕实现 1. 准备字幕文件：假设有中英文字幕文件为ch_en.srt，英文字幕文件为en.srt。 中文字幕文件：ch_en.srt ``` 1 00:00:0,000 --> 00:00:5,000 这里是视频的标题 1 Here's the video's title 1 2 00:00:5,000 --> 00:00:10,000 这里是视频的标题 2 Here's the video's title 2 - 英文字幕文件：`en.srt` 1 00:00:0,000 --> 00:00:5,000 Here's the video's title 1 2 00:00:5,000 --> 00:00:10,000 Here's the video's title 2 **2. 命令** ```shell ffmpeg -i input.mp4 -i ch_en.srt -i en.srt -map 0 -map 1 -map 2 -c copy -c:s mov_text -metadata:s:s:0 language=chi_eng -metadata:s:s:1 language=eng output_chi_eng_eng.mp4 3. 最后使用ffprobe查看一下生成文件的内容 Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'output_chi_eng_eng.mp4': Metadata: major_brand : isom minor_version : 512 compatible_brands: isomiso2avc1mp41 encoder : Lavf60.3.100 description : Bilibili VXCode Swarm Transcoder v0.6.11 Duration: 00:05:24.20, start: 0.000000, bitrate: 531 kb/s Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 852x480, 397 kb/s, 25 fps, 25 tbr, 12800 tbn (default) Metadata: handler_name : VideoHandler vendor_id : [0][0][0][0] Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 127 kb/s (default) Metadata: handler_name : SoundHandler vendor_id : [0][0][0][0] Stream #0:2[0x3](chi): Subtitle: mov_text (tx3g / 0x67337874), 0 kb/s (default) Metadata: handler_name : SubtitleHandler Stream #0:3[0x4](eng): Subtitle: mov_text (tx3g / 0x67337874), 0 kb/s Metadata: handler_name : SubtitleHandler 参考： https://www.bannerbear.com/blog/how-to-add-subtitles-to-a-video-file-using-ffmpeg/ Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"FFmpeg/21_deep_demuxing.html":{"url":"FFmpeg/21_deep_demuxing.html","title":"如何深入学习解封装？","keywords":"","body":"如何深入学习解封装？1.查看协议2.搭建调试环境3.在调试中学习解封装3.1.未打开文件时，尝试探索匹配封装格式3.2.打开文件时，再次尝试探索匹配封装格式3.3.根据不同的封装协议，在该协议内，每次读取一个AVPacket3.4.把读取的AVPacket保存到FFFormatContext->packet_buffer链表中3.5.当我们调用av_read_frame读取AVPacket时，这时候是从packet_buffer中获取如何深入学习解封装？ 毫无疑问，通过调试源码是最好的学习的方式！ 1.查看协议 要学习封装的具体细节，首先，我们得知道规则，不然看源码实现也是很难明白为什么要这样的？。我们也许是看MP4、AVI或者RTMP、HLS等流媒体，本人比较建议拿分析工具对应着看每个字节或bit所表示的内容，直接看官方网点分析协议很是费劲的。 在我看来，这些协议不用记，也记不住。知道怎么分析出其所表示的意思即可。 2.搭建调试环境 参考FFmpeg原理——FFmpeg调试环境搭建 。 我这里是在Mac上用Clion演示分析。 3.在调试中学习解封装 调用API实现解封装很简单，但是想要深入学习却是一件枯槁艰难的事情。 3.1.未打开文件时，尝试探索匹配封装格式 在这里，你可以看read_probe对应的协议所匹配的逻辑。 3.2.打开文件时，再次尝试探索匹配封装格式 3.3.根据不同的封装协议，在该协议内，每次读取一个AVPacket 可具体看对应的协议，具体读取AVPacket的逻辑，根据协议，进行解封装操作。如：根据flv协议读取出每一个AVPacket，然后解析出音频、视频、解码器等信息。 3.4.把读取的AVPacket保存到FFFormatContext->packet_buffer链表中 3.5.当我们调用av_read_frame读取AVPacket时，这时候是从packet_buffer中获取 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-25 11:34:33 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"OpenGL/":{"url":"OpenGL/","title":"OpenGL","keywords":"","body":"OpenGL简述推荐文章OpenGL 简述 OpenGL使用GPU渲染视频，释放宝贵的CPU资源，学习它是必不可少的。但是，正如雷神所说 ：作为一个搞视频技术的人研究OpenGL，需要耗费大量时间和精力，这样学习不是很经济。所以推荐只学习有关视频渲染相关知识。 推荐文章 OpenGL介绍，和相关程序库 纹理有关的基础知识 、OpenGL播放RGB/YUV 、OpenGL播放YUV420P（通过Texture，使用Shader） Android OpenGL ES官方文档 LearnOpenGL-CN OpenGL电子书下载 OpenGL基础知识 AFPlayer：OpenGL ES播放RGB OpenGLES基本使用 ：点、线、三角形、四边形、矩阵、纹理（贴纸）、摄像头显示、FBO、EGL、滤镜叠加、视频流编码输出。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-16 20:31:08 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"OpenGL/01_opengl.html":{"url":"OpenGL/01_opengl.html","title":"OpenGL基础知识","keywords":"","body":"OpenGL基础知识OpenGL简述OpenGL ESOpenGL播放YUV学习资料OpenGL基础知识 OpenGL简述 OpenGL（英语：Open Graphics Library，译名：开放图形库或者“开放式图形库”）是用于渲染2D、3D矢量图形的跨语言、跨平台的应用程序编程接口（API）。对于我们音视频开发最大的好处是可以使用OpenGL操控显卡（GPU）播放（渲染）视频，实现硬件加速，并且释放了宝贵的CPU资源。 OpenGL只提供了渲染的功能，核心API没有窗口系统、音频、打印、键盘／鼠标或其他输入设备的概念。然而，有些集成于原生窗口系统的东西需要允许和宿主系统交互，并且为了减轻繁重的编程工作，封装了OpenGL函数，为开发者提供相对简单的用法，实现一些较为复杂的操作。 1）以下包可以用来创建并管理 OpenGL 窗口，也可以管理输入，但几乎没有除此以外的其它功能： GLFW ——跨平台窗口和键盘、鼠标、手柄处理；偏向游戏。 freeglut ——跨平台窗口和键盘、鼠标处理；API 是 GLUT API 的超集，同时也比 GLUT 更新、更稳定。 GLUT ——早期的窗口处理库，已不再维护。 2）支持创建 OpenGL 窗口的还有一些“多媒体库”，同时还支持输入、声音等类似游戏的程序所需要的功能： Allegro 5——跨平台多媒体库，提供针对游戏开发的 C API SDL——跨平台多媒体库，提供 C API SFML——跨平台多媒体库，提供 C++ API；同时也提供 C#、Java、Haskell、Go 等语言的绑定 3）窗口包： FLTK——小型的跨平台 C++ 窗口组件库 Qt——跨平台 C++ 窗口组件库，提供许多OpenGL辅助对象 wxWidgets——跨平台 C++ 窗口组件库 OpenGL ES OpenGL ES （OpenGL for Embedded Systems）是三维图形应用程序接口OpenGL的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，如Android 。 OpenGL播放YUV /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/21 * description: MacOS使用OpenGL播放YUV * */ #define GLFW_INCLUDE_GLU #define NDEBUG extern \"C\" { #include //MacOS上引入 } //set '1' to choose a type of file to play #define LOAD_RGB24 0 #define LOAD_BGR24 0 #define LOAD_BGRA 0 #define LOAD_YUV420P 1 int screen_w = 384, screen_h = 216; const int pixel_w = 384, pixel_h = 216; //Bit per Pixel #if LOAD_BGRA const int bpp=32; #elif LOAD_RGB24 | LOAD_BGR24 const int bpp = 24; #elif LOAD_YUV420P const int bpp = 12; #endif //YUV file FILE *fp = NULL; unsigned char buffer[pixel_w * pixel_h * bpp / 8]; unsigned char buffer_convert[pixel_w * pixel_h * 3]; inline unsigned char CONVERT_ADJUST(double tmp) { return (unsigned char) ((tmp >= 0 && tmp > 1); int ypSize = nWidth * nHeight; int upSize = (ypSize >> 2); int offSet = 0; y_planar = yuv_src; u_planar = yuv_src + ypSize; v_planar = u_planar + upSize; for (int i = 0; i > 1) * (u_width) + (j >> 1); V = *(u_planar + offSet); // Get the U value from the v planar U = *(v_planar + offSet); // Cacular the R,G,B values // Method 1 R = CONVERT_ADJUST((Y + (1.4075 * (V - 128)))); G = CONVERT_ADJUST((Y - (0.3455 * (U - 128) - 0.7169 * (V - 128)))); B = CONVERT_ADJUST((Y + (1.7790 * (U - 128)))); offSet = rgb_width * i + j * 3; rgb_dst[offSet] = B; rgb_dst[offSet + 1] = G; rgb_dst[offSet + 2] = R; } } free(tmpbuf); } void display(void) { if (fread(buffer, 1, pixel_w * pixel_h * bpp / 8, fp) != pixel_w * pixel_h * bpp / 8) { // Loop fseek(fp, 0, SEEK_SET); fread(buffer, 1, pixel_w * pixel_h * bpp / 8, fp); } //Make picture full of window //Move to(-1.0,1.0) glRasterPos3f(-1.0f, 1.0f, 0); //Zoom, Flip glPixelZoom((float) screen_w / (float) pixel_w, -(float) screen_h / (float) pixel_h); #if LOAD_BGRA glDrawPixels(pixel_w, pixel_h,GL_BGRA, GL_UNSIGNED_BYTE, buffer); #elif LOAD_RGB24 glDrawPixels(pixel_w, pixel_h, GL_RGB, GL_UNSIGNED_BYTE, buffer); #elif LOAD_BGR24 glDrawPixels(pixel_w, pixel_h,GL_BGR_EXT, GL_UNSIGNED_BYTE, buffer); #elif LOAD_YUV420P CONVERT_YUV420PtoRGB24(buffer, buffer_convert, pixel_w, pixel_h); glDrawPixels(pixel_w, pixel_h, GL_RGB, GL_UNSIGNED_BYTE, buffer_convert); #endif //GLUT_DOUBLE glutSwapBuffers(); //GLUT_SINGLE //glFlush(); } void timeFunc(int value) { display(); // Present frame every 40 ms glutTimerFunc(40, timeFunc, 0); } void processSpecialKeys(int key, int x, int y) { printf(\"OpenGL processSpecialKeys: %d\\n\", key); switch (key) { case GLUT_KEY_UP: break; case GLUT_KEY_DOWN: break; case GLUT_KEY_LEFT: break; case GLUT_KEY_RIGHT: break; } } int main(int argc, char **argv) { const char *src_filename = \"source/Kobe-384x216-yuv420p.yuv\"; fp = fopen(src_filename, \"rb+\"); // GLUT init glutInit(&argc, argv); //Double, Use glutSwapBuffers() to show glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB); //Single, Use glFlush() to show // glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB ); //在屏幕中显示的位置 glutInitWindowPosition(0, 0); //窗口的大小 glutInitWindowSize(screen_w, screen_h); //创建窗口 glutCreateWindow(\"Simplest Video Play OpenGL\"); printf(\"OpenGL Version: %s\\n\", glGetString(GL_VERSION)); //显示的函数 glutDisplayFunc(&display); //轮询函数 glutTimerFunc(40, timeFunc, 0); //监听按键 glutSpecialFunc(processSpecialKeys); // Start! glutMainLoop(); return 0; } 学习资料 OpenGL介绍，和相关程序库 纹理有关的基础知识 。 最简单的视音频播放示例5：OpenGL播放RGB/YUV 最简单的视音频播放示例6：OpenGL播放YUV420P（通过Texture，使用Shader） Android OpenGL ES官方文档 LearnOpenGL-CN 电子书 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"OpenGL/02_glsl.html":{"url":"OpenGL/02_glsl.html","title":"GLSL（着色器语言）中文手册","keywords":"","body":"GLSL（着色器语言）中文手册[原文]基本类型:基本结构和数组:向量的分量访问:运算符:基础类型间的运算:变量限定符:函数参数限定符:glsl的函数:构造函数:类型转换:精度限定:预编译指令:内置的特殊变量内置的常量流控制内置函数库官方的shader范例:GLSL（着色器语言）中文手册[原文] 着色器语言 GLSL (opengl-shader-language)入门大全 基本类型: 类型 说明 void 空类型,即不返回任何值 bool 布尔类型 true,false int 带符号的整数 signed integer float 带符号的浮点数 floating scalar vec2, vec3, vec4 n维浮点数向量 n-component floating point vector bvec2, bvec3, bvec4 n维布尔向量 Boolean vector ivec2, ivec3, ivec4 n维整数向量 signed integer vector mat2, mat3, mat4 2x2, 3x3, 4x4 浮点数矩阵 float matrix sampler2D 2D纹理 a 2D texture samplerCube 盒纹理 cube mapped texture 基本结构和数组: 类型 说明 结构 struct type-name{} 类似c语言中的 结构体 数组 float foo[3] glsl只支持1维数组,数组可以是结构体的成员 向量的分量访问: glsl中的向量(vec2,vec3,vec4)往往有特殊的含义,比如可能代表了一个空间坐标(x,y,z,w),或者代表了一个颜色(r,g,b,a),再或者代表一个纹理坐标(s,t,p,q) 所以glsl提供了一些更人性化的分量访问方式. vector.xyzw 其中xyzw 可以任意组合 vector.rgba 其中rgba 可以任意组合 vector.stpq 其中rgba 可以任意组合 vec4 v=vec4(1.0,2.0,3.0,1.0); float x = v.x; //1.0 float x1 = v.r; //1.0 float x2 = v[0]; //1.0 vec3 xyz = v.xyz; //vec3(1.0,2.0,3.0) vec3 xyz1 = vec(v[0],v[1],v[2]); //vec3(1.0,2.0,3.0) vec3 rgb = v.rgb; //vec3(1.0,2.0,3.0) vec2 xyzw = v.xyzw; //vec4(1.0,2.0,3.0,1.0); vec2 rgba = v.rgba; //vec4(1.0,2.0,3.0,1.0); 运算符: 优先级(越小越高) 运算符 说明 结合性 1 () 聚组:a*(b+c) N/A 2 [] () . ++ -- 数组下标[],方法参数fun(arg1,arg2,arg3),属性访问a.b,自增/减后缀a++ a-- L - R 3 ++ -- + - ! 自增/减前缀++a --a,正负号(一般正号不写)a ,-a,取反!false R - L 4 * / 乘除数学运算 L - R 5 + - 加减数学运算 L - R 7 = 关系运算符 L - R 8 == != 相等性运算符 L - R 12 && 逻辑与 L - R 13 ^^ 逻辑排他或(用处基本等于!=) L - R 14 __\\ \\ __ 逻辑或 L - R 15 ? : 三目运算符 L - R 16 = += -= *= /= 赋值与复合赋值 L - R 17 , 顺序分配运算 L - R ps 左值与右值: 左值:表示一个储存位置,可以是变量,也可以是表达式,但表达式最后的结果必须是一个储存位置. 右值:表示一个值, 可以是一个变量或者表达式再或者纯粹的值. 操作符的优先级：决定含有多个操作符的表达式的求值顺序，每个操作的优先级不同. 操作符的结合性：决定相同优先级的操作符是从左到右计算，还是从右到左计算。 基础类型间的运算: glsl中,没有隐式类型转换,原则上glsl要求任何表达式左右两侧(l-value),(r-value)的类型必须一致 也就是说以下表达式都是错误的: int a =2.0; //错误,r-value为float 而 lvalue 为int. int a =1.0+2; float a =2; float a =2.0+1; bool a = 0; vec3 a = vec3(1.0, 2.0, 3.0) * 2; 下面来分别说说可能遇到的情况: 1.float 与 int: float与float , int与int之间是可以直接运算的,但float与int不行.它们需要进行一次显示转换.即要么把float转成int: int(1.0) ,要么把int转成float: float(1) ,以下表达式都是正确的: int a=int(2.0); float a= float(2); int a=int(2.0)*2 + 1; float a= float(2)*6.0+2.3; 2.float 与 vec(向量) mat(矩阵): vec,mat这些类型其实是由float复合而成的,当它们与float运算时,其实就是在每一个分量上分别与float进行运算,这就是所谓的逐分量运算.glsl里 大部分涉及vec,mat的运算都是逐分量运算,但也并不全是. 下文中就会讲到特例. 逐分量运算是线性的,这就是说 vec 与 float 的运算结果是还是 vec. int 与 vec,mat之间是不可运算的, 因为vec和mat中的每一个分量都是 float 类型的. 无法与int进行逐分量计算. 下面枚举了几种 float 与 vec,mat 运算的情况 vec3 a = vec3(1.0, 2.0, 3.0); mat3 m = mat3(1.0); float s = 10.0; vec3 b = s * a; // vec3(10.0, 20.0, 30.0) vec3 c = a * s; // vec3(10.0, 20.0, 30.0) mat3 m2 = s * m; // = mat3(10.0) mat3 m3 = m * s; // = mat3(10.0) 3. vec(向量) 与 vec(向量): 两向量间的运算首先要保证操作数的阶数都相同.否则不能计算.例如: vec3*vec2 vec4+vec3 等等都是不行的. 它们的计算方式是两操作数在同位置上的分量分别进行运算,其本质还是逐分量进行的,这和上面所说的float类型的 逐分量运算可能有一点点差异,相同的是 vec 与 vec 运算结果还是 vec, 且阶数不变. vec3 a = vec3(1.0, 2.0, 3.0); vec3 b = vec3(0.1, 0.2, 0.3); vec3 c = a + b; // = vec3(1.1, 2.2, 3.3) vec3 d = a * b; // = vec3(0.1, 0.4, 0.9) 3. vec(向量) 与 mat(矩阵): 要保证操作数的阶数相同,且vec与mat间只存在乘法运算. 它们的计算方式和线性代数中的矩阵乘法相同,不是逐分量运算. vec2 v = vec2(10., 20.); mat2 m = mat2(1., 2., 3., 4.); vec2 w = m * v; // = vec2(1. * 10. + 3. * 20., 2. * 10. + 4. * 20.) ... vec2 v = vec2(10., 20.); mat2 m = mat2(1., 2., 3., 4.); vec2 w = v * m; // = vec2(1. * 10. + 2. * 20., 3. * 10. + 4. * 20.) 向量与矩阵的乘法规则如下: 4. mat(矩阵) 与 mat(矩阵): 要保证操作数的阶数相同. 在mat与mat的运算中, 除了乘法是线性代数中的矩阵乘法外.其余的运算任为逐分量运算.简单说就是只有乘法是特殊的,其余都和vec与vec运算类似. mat2 a = mat2(1., 2., 3., 4.); mat2 b = mat2(10., 20., 30., 40.); mat2 c = a * b; //mat2(1.*10.+3.*20.,2.*10.+4.*20.,1.* 30.+3.*40.,2.* 30.+4.*40.); mat2 d = a+b;//mat2(1.+10.,2.+20.,3.+30.,4.+40); 矩阵乘法规则如下: 变量限定符: 修饰符 说明 none (默认的可省略)本地变量,可读可写,函数的输入参数既是这种类型 const 声明变量或函数的参数为只读类型 attribute 只能存在于vertex shader中,一般用于保存顶点或法线数据,它可以在数据缓冲区中读取数据 uniform 在运行时shader无法改变uniform变量, 一般用来放置程序传递给shader的变换矩阵，材质，光照参数等等. varying 主要负责在vertex 和 fragment 之间传递变量 const: 和C语言类似,被const限定符修饰的变量初始化后不可变,除了局部变量,函数参数也可以使用const修饰符.但要注意的是结构变量可以用const修饰, 但结构中的字段不行. const变量必须在声明时就初始化 const vec3 v3 = vec3(0.,0.,0.) 局部变量只能使用const限定符. 函数参数只能使用const限定符. struct light { vec4 color; vec3 pos; //const vec3 pos1; //结构中的字段不可用const修饰会报错. }; const light lgt = light(vec4(1.0), vec3(0.0)); //结构变量可以用const修饰 attribute: attribute变量是全局且只读的,它只能在vertex shader中使用,只能与浮点数,向量或矩阵变量组合, 一般attribute变量用来放置程序传递来的模型顶点,法线,颜色,纹理等数据它可以访问数据缓冲区 (还记得gl.vertexAttribPointer这个函数吧) attribute vec4 a_Position; uniform: uniform变量是全局且只读的,在整个shader执行完毕前其值不会改变,他可以和任意基本类型变量组合, 一般我们使用uniform变量来放置外部程序传递来的环境数据(如点光源位置,模型的变换矩阵等等) 这些数据在运行中显然是不需要被改变的. uniform vec4 lightPosition; varying: varying类型变量是 vertex shader 与 fragment shader 之间的信使,一般我们在 vertex shader 中修改它然后在fragment shader使用它,但不能在 fragment shader中修改它. //顶点着色器 varying vec4 v_Color; void main(){ ... v_Color = vec4(1.,1.,1.,1); } //片元着色器 ... varying vec4 v_Color; void main() { gl_FragColor = v_Color; } ... 要注意全局变量限制符只能为 const、attribute、uniform和varying中的一个.不可复合. 函数参数限定符: 函数的参数默认是以拷贝的形式传递的,也就是值传递,任何传递给函数参数的变量,其值都会被复制一份,然后再交给函数内部进行处理. 我们可以为参数添加限定符来达到传递引用的目的,glsl中提供的参数限定符如下: 限定符 说明 默认使用 in 限定符 in 复制到函数中在函数中可读写 out 返回时从函数中复制出来 inout 复制到函数中并在返回时复制出来 in 是函数参数的默认限定符,最终真正传入函数形参的其实是实参的一份拷贝.在函数中,修改in修饰的形参不会影响到实参变量本身. out 它的作用是向函数外部传递新值,out模式下传递进来的参数是write-only的(可写不可读).就像是一个\"坑位\",坑位中的值需要函数给他赋予. 在函数中,修改out修饰的形参会影响到实参本身. inout inout下,形参可以被理解为是一个带值的\"坑位\",及可读也可写,在函数中,修改inout修饰的形参会影响到实参本身. glsl的函数: glsl允许在程序的最外部声明函数.函数不能嵌套,不能递归调用,且必须声明返回值类型(无返回值时声明为void) 在其他方面glsl函数与c函数非常类似. vec4 getPosition(){ vec4 v4 = vec4(0.,0.,0.,1.); return v4; } void doubleSize(inout float size){ size= size*2.0 ; } void main() { float psize= 10.0; doubleSize(psize); gl_Position = getPosition(); gl_PointSize = psize; } 构造函数: glsl中变量可以在声明的时候初始化,float pSize = 10.0 也可以先声明然后等需要的时候在进行赋值. 聚合类型对象如(向量,矩阵,数组,结构) 需要使用其构造函数来进行初始化. vec4 color = vec4(0.0, 1.0, 0.0, 1.0); //一般类型 float pSize = 10.0; float pSize1; pSize1=10.0; ... //复合类型 vec4 color = vec4(0.0, 1.0, 0.0, 1.0); vec4 color1; color1 =vec4(0.0, 1.0, 0.0, 1.0); ... //结构 struct light { float intensity; vec3 position; }; light lightVar = light(3.0, vec3(1.0, 2.0, 3.0)); //数组 const float c[3] = float[3](5.0, 7.2, 1.1); 类型转换: glsl可以使用构造函数进行显式类型转换,各值如下: bool t= true; bool f = false; int a = int(t); //true转换为1或1.0 int a1 = int(f);//false转换为0或0.0 float b = float(t); float b1 = float(f); bool c = bool(0);//0或0.0转换为false bool c1 = bool(1);//非0转换为true bool d = bool(0.0); bool d1 = bool(1.0); 精度限定: glsl在进行光栅化着色的时候,会产生大量的浮点数运算,这些运算可能是当前设备所不能承受的,所以glsl提供了3种浮点数精度,我们可以根据不同的设备来使用合适的精度. 在变量前面加上 highp mediump lowp 即可完成对该变量的精度声明. lowp float color; varying mediump vec2 Coord; lowp ivec2 foo(lowp mat3); highp mat4 m; 我们一般在片元着色器(fragment shader)最开始的地方加上 precision mediump float; 便设定了默认的精度.这样所有没有显式表明精度的变量 都会按照设定好的默认精度来处理. 如何确定精度: 变量的精度首先是由精度限定符决定的,如果没有精度限定符,则要寻找其右侧表达式中,已经确定精度的变量,一旦找到,那么整个表达式都将在该精度下运行.如果找到多个, 则选择精度较高的那种,如果一个都找不到,则使用默认或更大的精度类型. uniform highp float h1; highp float h2 = 2.3 * 4.7; //运算过程和结果都 是高精度 mediump float m; m = 3.7 * h1 * h2; //运算过程 是高精度 h2 = m * h1; //运算过程 是高精度 m = h2 – h1; //运算过程 是高精度 h2 = m + m; //运算过程和结果都 是中等精度 void f(highp float p); // 形参 p 是高精度 f(3.3); //传入的 3.3是高精度 invariant关键字: 由于shader在编译时会进行一些内部优化,可能会导致同样的运算在不同shader里结果不一定精确相等.这会引起一些问题,尤其是vertx shader向fragmeng shader传值的时候. 所以我们需要使用invariant 关键字来显式要求计算结果必须精确一致. 当然我们也可使用 #pragma STDGL invariant(all)来命令所有输出变量必须精确一致, 但这样会限制编译器优化程度,降低性能. #pragma STDGL invariant(all) //所有输出变量为 invariant invariant varying texCoord; //varying在传递数据的时候声明为invariant 限定符的顺序: 当需要用到多个限定符的时候要遵循以下顺序: 1.在一般变量中: invariant > storage > precision 2.在参数中: storage > parameter > precision 我们来举例说明: invariant varying lowp float color; // invariant > storage > precision void doubleSize(const in lowp float s){ //storage > parameter > precision float s1=s; } 预编译指令: 以 # 开头的是预编译指令,常用的有: #define #undef #if #ifdef #ifndef #else #elif #endif #error #pragma #extension #version #line 比如 #version 100 他的意思是规定当前shader使用 GLSL ES 1.00标准进行编译,如果使用这条预编译指令,则他必须出现在程序的最开始位置. 内置的宏: __LINE__ : 当前源码中的行号. __VERSION__ : 一个整数,指示当前的glsl版本 比如 100 ps: 100 = v1.00 GL_ES : 如果当前是在 OPGL ES 环境中运行则 GL_ES 被设置成1,一般用来检查当前环境是不是 OPENGL ES. GL_FRAGMENT_PRECISION_HIGH : 如果当前系统glsl的片元着色器支持高浮点精度,则设置为1.一般用于检查着色器精度. 实例: 1.如何通过判断系统环境,来选择合适的精度: #ifdef GL_ES // #ifdef GL_FRAGMENT_PRECISION_HIGH precision highp float; #else precision mediump float; #endif #endif 2.自定义宏: #define NUM 100 #if NUM==100 #endif 内置的特殊变量 glsl程序使用一些特殊的内置变量与硬件进行沟通.他们大致分成两种 一种是 input类型,他负责向硬件(渲染管线)发送数据. 另一种是output类型,负责向程序回传数据,以便编程时需要. 在 vertex Shader 中: output 类型的内置变量: 变量 说明 单位 highp vec4 gl_Position; gl_Position 放置顶点坐标信息 vec4 mediump float gl_PointSize; gl_PointSize 需要绘制点的大小,(只在gl.POINTS模式下有效) float 在 fragment Shader 中: input 类型的内置变量: 变量 说明 单位 mediump vec4 gl_FragCoord; 片元在framebuffer画面的相对位置 vec4 bool gl_FrontFacing; 标志当前图元是不是正面图元的一部分 bool mediump vec2 gl_PointCoord; 经过插值计算后的纹理坐标,点的范围是0.0到1.0 vec2 output 类型的内置变量: 变量 说明 单位 mediump vec4 gl_FragColor; 设置当前片点的颜色 vec4 RGBA color mediump vec4 gl_FragData[n] 设置当前片点的颜色,使用glDrawBuffers数据数组 vec4 RGBA color 内置的常量 glsl提供了一些内置的常量,用来说明当前系统的一些特性. 有时我们需要针对这些特性,对shader程序进行优化,让程序兼容度更好. 在 vertex Shader 中: 1.const mediump int gl_MaxVertexAttribs>=8 gl_MaxVertexAttribs 表示在vertex shader(顶点着色器)中可用的最大attributes数.这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 不过最低不能小于 8 个. 2.const mediump int gl_MaxVertexUniformVectors >= 128 gl_MaxVertexUniformVectors 表示在vertex shader(顶点着色器)中可用的最大uniform vectors数. 这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 不过最低不能小于 128 个. 3.const mediump int gl_MaxVaryingVectors >= 8 gl_MaxVaryingVectors 表示在vertex shader(顶点着色器)中可用的最大varying vectors数. 这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 不过最低不能小于 8 个. 4.const mediump int gl_MaxVertexTextureImageUnits >= 0 gl_MaxVaryingVectors 表示在vertex shader(顶点着色器)中可用的最大纹理单元数(贴图). 这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 甚至可以一个都没有(无法获取顶点纹理) 5.const mediump int gl_MaxCombinedTextureImageUnits >= 8 gl_MaxVaryingVectors 表示在 vertex Shader和fragment Shader总共最多支持多少个纹理单元. 这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 不过最低不能小于 8 个. 在 fragment Shader 中: 1.const mediump int gl_MaxTextureImageUnits >= 8 gl_MaxVaryingVectors 表示在 fragment Shader(片元着色器)中能访问的最大纹理单元数,这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 不过最低不能小于 8 个. 2.const mediump int gl_MaxFragmentUniformVectors >= 16 gl_MaxFragmentUniformVectors 表示在 fragment Shader(片元着色器)中可用的最大uniform vectors数,这个值的大小取决于 OpenGL ES 在某设备上的具体实现, 不过最低不能小于 16 个. 3.const mediump int gl_MaxDrawBuffers = 1 gl_MaxDrawBuffers 表示可用的drawBuffers数,在OpenGL ES 2.0中这个值为1, 在将来的版本可能会有所变化. glsl中还有一种内置的uniform状态变量, gl_DepthRange 它用来表明全局深度范围. 结构如下: struct gl_DepthRangeParameters { highp float near; // n highp float far; // f highp float diff; // f - n }; uniform gl_DepthRangeParameters gl_DepthRange; 除了 gl_DepthRange 外的所有uniform状态常量都已在glsl 1.30 中废弃. 流控制 glsl的流控制和c语言非常相似,这里不必再做过多说明,唯一不同的是片段着色器中有一种特殊的控制流discard. 使用discard会退出片段着色器，不执行后面的片段着色操作。片段也不会写入帧缓冲区。 for (l = 0; l 0) ... if (true) discard; 内置函数库 glsl提供了非常丰富的函数库,供我们使用,这些功能都是非常有用且会经常用到的. 这些函数按功能区分大改可以分成7类: 通用函数: 下文中的 类型 T可以是 float, vec2, vec3, vec4,且可以逐分量操作. 方法 说明 T abs(T x) 返回x的绝对值 T sign(T x) 比较x与0的值,大于,等于,小于 分别返回 1.0 ,0.0,-1.0 T floor(T x) 返回 T ceil(T x) 返回>=等于x的最小整数 T fract(T x) 获取x的小数部分 T mod(T x, T y) T mod(T x, float y) 取x,y的余数 T min(T x, T y) T min(T x, float y) 取x,y的最小值 T max(T x, T y) T max(T x, float y) 取x,y的最大值 T clamp(T x, T minVal, T maxVal) T clamp(T x, float minVal,float maxVal) min(max(x, minVal), maxVal),返回值被限定在 minVal,maxVal之间 T mix(T x, T y, T a) T mix(T x, T y, float a) 取x,y的线性混合,x(1-a)+y\\a T step(T edge, T x) T step(float edge, T x) 如果 x T smoothstep(T edge0, T edge1, T x) T smoothstep(float edge0,float edge1, T x) 如果xedge1返回1.0, 否则返回Hermite插值 角度&三角函数: 下文中的 类型 T可以是 float, vec2, vec3, vec4,且可以逐分量操作. 方法 说明 T radians(T degrees) 角度转弧度 T degrees(T radians) 弧度转角度 T sin(T angle) 正弦函数,角度是弧度 T cos(T angle) 余弦函数,角度是弧度 T tan(T angle) 正切函数,角度是弧度 T asin(T x) 反正弦函数,返回值是弧度 T acos(T x) 反余弦函数,返回值是弧度 T atan(T y, T x) T atan(T y_over_x) 反正切函数,返回值是弧度 指数函数: 下文中的 类型 T可以是 float, vec2, vec3, vec4,且可以逐分量操作. 方法 说明 T pow(T x, T y) 返回x的y次幂 xy T exp(T x) 返回x的自然指数幂 ex T log(T x) 返回x的自然对数 ln T exp2(T x) 返回2的x次幂 2x T log2(T x) 返回2为底的对数 log2 T sqrt(T x) 开根号 √x T inversesqrt(T x) 先开根号,在取倒数,就是 1/√x 几何函数: 下文中的 类型 T可以是 float, vec2, vec3, vec4,且可以逐分量操作. 方法 说明 float length(T x) 返回矢量x的长度 float distance(T p0, T p1) 返回p0 p1两点的距离 float dot(T x, T y) 返回x y的点积 vec3 cross(vec3 x, vec3 y) 返回x y的叉积 T normalize(T x) 对x进行归一化,保持向量方向不变但长度变为1 T faceforward(T N, T I, T Nref) 根据 矢量 N 与Nref 调整法向量 T reflect(T I, T N) 返回 I - 2 dot(N,I) N, 结果是入射矢量 I 关于法向量N的 镜面反射矢量 T refract(T I, T N, float eta) 返回入射矢量I关于法向量N的折射矢量,折射率为eta 矩阵函数: mat可以为任意类型矩阵. 方法 说明 mat matrixCompMult(mat x, mat y) 将矩阵 x 和 y的元素逐分量相乘 向量函数: 下文中的 类型 T可以是 vec2, vec3, vec4, 且可以逐分量操作. bvec指的是由bool类型组成的一个向量: vec3 v3= vec3(0.,0.,0.); vec3 v3_1= vec3(1.,1.,1.); bvec3 aa= lessThan(v3,v3_1); //bvec3(true,true,true) 方法 说明 bvec lessThan(T x, T y) 逐分量比较x bvec lessThanEqual(T x, T y) 逐分量比较 x bvec greaterThan(T x, T y) 逐分量比较 x > y,将结果写入bvec对应位置 bvec greaterThanEqual(T x, T y) 逐分量比较 x >= y,将结果写入bvec对应位置 bvec equal(T x, T y) bvec equal(bvec x, bvec y) 逐分量比较 x == y,将结果写入bvec对应位置 bvec notEqual(T x, T y) bvec notEqual(bvec x, bvec y) 逐分量比较 x!= y,将结果写入bvec对应位置 bool any(bvec x) 如果x的任意一个分量是true,则结果为true bool all(bvec x) 如果x的所有分量是true,则结果为true bvec not(bvec x) bool矢量的逐分量取反 纹理查询函数: 图像纹理有两种 一种是平面2d纹理,另一种是盒纹理,针对不同的纹理类型有不同访问方法. 纹理查询的最终目的是从sampler中提取指定坐标的颜色信息. 函数中带有Cube字样的是指 需要传入盒状纹理. 带有Proj字样的是指带投影的版本. 以下函数只在vertex shader中可用: vec4 texture2DLod(sampler2D sampler, vec2 coord, float lod); vec4 texture2DProjLod(sampler2D sampler, vec3 coord, float lod); vec4 texture2DProjLod(sampler2D sampler, vec4 coord, float lod); vec4 textureCubeLod(samplerCube sampler, vec3 coord, float lod); 以下函数只在fragment shader中可用: vec4 texture2D(sampler2D sampler, vec2 coord, float bias); vec4 texture2DProj(sampler2D sampler, vec3 coord, float bias); vec4 texture2DProj(sampler2D sampler, vec4 coord, float bias); vec4 textureCube(samplerCube sampler, vec3 coord, float bias); 在 vertex shader 与 fragment shader 中都可用: vec4 texture2D(sampler2D sampler, vec2 coord); vec4 texture2DProj(sampler2D sampler, vec3 coord); vec4 texture2DProj(sampler2D sampler, vec4 coord); vec4 textureCube(samplerCube sampler, vec3 coord); 官方的shader范例: 下面的shader如果你可以一眼看懂,说明你已经对glsl语言基本掌握了. Vertex Shader: uniform mat4 mvp_matrix; //透视矩阵 * 视图矩阵 * 模型变换矩阵 uniform mat3 normal_matrix; //法线变换矩阵(用于物体变换后法线跟着变换) uniform vec3 ec_light_dir; //光照方向 attribute vec4 a_vertex; // 顶点坐标 attribute vec3 a_normal; //顶点法线 attribute vec2 a_texcoord; //纹理坐标 varying float v_diffuse; //法线与入射光的夹角 varying vec2 v_texcoord; //2d纹理坐标 void main(void) { //归一化法线 vec3 ec_normal = normalize(normal_matrix * a_normal); //v_diffuse 是法线与光照的夹角.根据向量点乘法则,当两向量长度为1是 乘积即cosθ值 v_diffuse = max(dot(ec_light_dir, ec_normal), 0.0); v_texcoord = a_texcoord; gl_Position = mvp_matrix * a_vertex; } Fragment Shader: precision mediump float; uniform sampler2D t_reflectance; uniform vec4 i_ambient; varying float v_diffuse; varying vec2 v_texcoord; void main (void) { vec4 color = texture2D(t_reflectance, v_texcoord); //这里分解开来是 color*vec3(1,1,1)*v_diffuse + color*i_ambient //色*光*夹角cos + 色*环境光 gl_FragColor = color*(vec4(v_diffuse) + i_ambient); } Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/":{"url":"MustRead/","title":"重点技术","keywords":"","body":"重点技术重点技术 手撕FLV协议 基于H.264看H.265 面试题整理 常见的音视频传输协议对比 常见的音视频编解码器对比 常见的音视频封装格式对比 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-28 12:56:08 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/01_flv.html":{"url":"MustRead/01_flv.html","title":"手撕FLV协议","keywords":"","body":"手撕FLV协议实现效果FLV协议实现代码使用ffplay查看输出文件测试文件下载地址手撕FLV协议 实现效果 纯代码实现分离FLV音视频流，并组装成AAC和H264文件，最后能正常播放。注：本文只对有AAC和H264格式音视频流组成的flv进行分离。 FLV协议 注：脚本部分本文未使用，可前往这查看。 实现代码 代码注释已经很详细了，其实就是对照协议表进行数据解析，其中： (1) H264协议请看直播推流全过程：视频编码之H.264（3）; (2) AAC协议请看直播推流全过程：音频编码之AAC（4）; #include /** * @author 秦城季 * @email xhunmon@126.com * @Blog https://qincji.gitee.io * @date 2021/01/01 * description: */ #include #include using namespace std; typedef struct Tag { int TagType; unsigned DataSize; unsigned Timestamp; unsigned TimestampExtended; int StreamID; unsigned char *Data; } Tag; //从flv中第一帧获取的数据 typedef struct FLV_AAC_HEAD { unsigned char audioObjectType: 5; unsigned char samplingFrequencyIndex: 4; unsigned char channelConfiguration: 4; unsigned char other: 3;//占位；这个值不需要的 000 } FLV_AAC_HEAD; //aac封装 每个帧头结构设定恒为7Byte typedef struct AAC_ADST_HEAD { //1. adts_fixed_header unsigned short syncword: 12; /* 恒为 1111 1111 1111*/ unsigned char ID: 1; /* 0：MPEG-4，1：MPEG-2*/ unsigned char layer: 2; /* 00*/ unsigned char protection_absent: 1; /* 是否使用error_check()。0：使用，1：不使用。*/ unsigned char profile: 2; /* AAC类型*/ unsigned char sampling_frequency_index: 4;/* 采样率的数组下标，即：sampling frequeny[sampling_frequency_index] */ unsigned char private_bit: 1; /* 0*/ unsigned char channel_configuration: 3; /* 声道数*/ unsigned char original_copy: 1; /* 0*/ unsigned char home: 1; /* 0*/ //2. adts_variable_header unsigned char copyright_identification_bit: 1; /* 0*/ unsigned char copyright_identification_start: 1; /* 0*/ unsigned short frame_length: 13; /* 帧的长度，包括head*/ unsigned short adts_buffer_fullness: 11; /* 固定0x7FF*/ unsigned char number_of_raw_data_blocks_in_frame: 2; /*在ADTS帧中有number_of_raw_data_blocks_in_frame + 1个AAC原始数据块。number_of_raw_data_blocks_in_frame == 0 表示说ADTS帧中有一个AAC原始数据块。*/ } AAC_ADST_HEAD; int checkFLV(FILE *fp_in) { int ret = 1; char *head = (char *) malloc(9); fread(head, 1, 9, fp_in); //Signature(FLV) if (head[0] != 'F' || head[1] != 'L' || head[2] != 'V') { printf(\"Not FLV File !\\n\"); ret = 0; } //第5个字节为Flags(流信息)，视频标志位：.... ...1；音频标志位：.... .1.. int vFlags = head[4] & 0x01; int aFlags = (head[4] & 0x04) >> 1; if (!vFlags) { printf(\"Not Video Data !\\n\"); ret = 0; } if (!aFlags) { printf(\"Not Audio Data !\\n\"); ret = 0; } free(head); return ret; } int getTagAndTagSize(FILE *fp_in, Tag *tag) { int length = -1; //Tag表中除了Data之外，其他所有的字节 unsigned char *buf = (unsigned char *) malloc(11); //读取Tag前面11个字节 fread(buf, 1, 11, fp_in); tag->TagType = buf[0]; tag->DataSize = (buf[1] Timestamp = (buf[4] TimestampExtended = buf[7]; tag->StreamID = (buf[8] Data = (unsigned char *) calloc(tag->DataSize, sizeof(char)); //读取Data fread(tag->Data, 1, tag->DataSize, fp_in); //读取PreviousTagSize的4个字节 fread(buf, 1, 4, fp_in); //这个长度就是 Tag(n)+PreviousTagSize(n)的长度 length = 11 + tag->DataSize + 4; unsigned PreviousTagSize = (buf[0] TagType, tag->DataSize, tag->Timestamp, tag->TimestampExtended, tag->StreamID, PreviousTagSize); return length; } unsigned char *sps_nalu = NULL; unsigned char *pps_nalu = NULL; char *buf_start = NULL; unsigned sps_nalu_size = 0; unsigned pps_nalu_size = 0; int videoToH264(FILE *fp_video, Tag *tag) { int ret = 1; //tag->Data = FrameType + CodecID + VideoData ; //VideoData = AVCPacketType + CompositionTime + 【Data（注：视频数据）】 unsigned FrameType = (tag->Data[0] & 0xF0) >> 4;//Data表(视频)：高4bit unsigned CodecID = tag->Data[0] & 0x0F; //Data表(视频)：低4bit unsigned AVCPacketType = tag->Data[1]; //VideoData表( AVCVIDEOPACKE) unsigned CompositionTime = (tag->Data[2] Data[3] Data[4];//VideoData表( AVCVIDEOPACKE) if (CodecID != 7) {//7: AVC(高级视频编码) ret = 0; printf(\"Not AVC Error\\n\"); goto end; } printf(\"FrameType=%d\\t|CodecID=%d\\t|AVCPacketType=%d\\t|CompositionTime=%d\\t\\n\", FrameType, CodecID, AVCPacketType, CompositionTime); //从tag->Data[5]开始至结束为【Data（注：视频数据）】 //一般FLV第一个视频Tag为sps和pps数据，需要根据AVCDecoderConfigurationRecord进行解析，单独抽出来封装成NALU数据 if (AVCPacketType == 0) {//0: AVC sequence header printf(\"0: AVC sequence header\\n\"); if (sps_nalu) { free(sps_nalu); } if (pps_nalu) { free(pps_nalu); } unsigned Version = tag->Data[5];//sps：版本 unsigned AVCProfileIndication = tag->Data[6];//sps：编码规格 unsigned profile_compatibility = tag->Data[7];//sps：编码规格 unsigned AVCLevelIndication = tag->Data[8];//sps：编码规格 unsigned reserved_And_lengthSizeMinusOne = tag->Data[9];//sps：应恒为0xFF unsigned numOfSequenceParameterSets = tag->Data[10] & 0x1F;//sps：sps个数，取低5位；tag->Data[10]一般为0xE1 unsigned sps_len = (tag->Data[11] Data[12];//sps：sps长度 sps_nalu_size = sps_len + 4; sps_nalu = (unsigned char *) malloc(sps_nalu_size);//加4个为起始位 memcpy(sps_nalu, buf_start, 4); memcpy(sps_nalu + 4, tag->Data + 13, sps_len); printf(\"sps_len=%d\\nsps_data=\", sps_len); for (int i = 4; i Data[ppsIndex];//pps：pps个数，1个字节； unsigned pps_len = (tag->Data[ppsIndex + 1] Data[ppsIndex + 2];//pps：pps长度 pps_nalu_size = pps_len + 4; pps_nalu = (unsigned char *) malloc(pps_nalu_size);//加4个为起始位 memcpy(pps_nalu, buf_start, 4); memcpy(pps_nalu + 4, tag->Data + ppsIndex + 3, pps_len); printf(\"pps_len=%d\\npps_data=\", pps_len); for (int i = 4; i = tag->DataSize) { printf(\"----break-----, pic_start + 4 >= tag->DataSize\\n\"); break; } printf(\"pic_size=0x%.2x%.2x%.2x%.2x\\n\", tag->Data[pic_start], tag->Data[pic_start + 1], tag->Data[pic_start + 2], tag->Data[pic_start + 3]); pic_size = (tag->Data[pic_start] Data[pic_start + 1] Data[pic_start + 2] Data[pic_start + 3]; pic_next_index = pic_start + 4 + pic_size; if (pic_size DataSize DataSize); break; } printf(\"write pic data,pic_start=%d\\tpic_size=%d\\tpic_next_index=%d\\tDataSize=%d \\n\", pic_start, pic_size, pic_next_index, tag->DataSize); //当前帧数据范围 pic_start + 4 ~ pic_next_index fwrite(buf_start, 1, 4, fp_video); fwrite(tag->Data + pic_start + 4, 1, pic_size, fp_video); pic_start = pic_next_index; } } else if (AVCPacketType == 2) {//2: AVC end of sequence printf(\"2: AVC end of sequence\\n\"); } end: free(tag->Data); return ret; } AAC_ADST_HEAD *aacAdstHead = NULL; int getAACHeadBuf(unsigned char *buf, AAC_ADST_HEAD *aacAdstHead, const unsigned int frame_size) { int ret = 0; aacAdstHead->frame_length = frame_size; //取syncword高位1Byte buf[0] = aacAdstHead->syncword >> 4; //syncword(低4位)+ID(1)+layer(2)+protection_absent(1) buf[1] = ((aacAdstHead->syncword & 0x00F) ID >> 3) + (aacAdstHead->layer >> 1) + aacAdstHead->protection_absent; //profile(2)+sampling_frequency_index(4)+private_bit(1)+channel_configuration(3bit取最高1bit) buf[2] = (aacAdstHead->profile sampling_frequency_index private_bit channel_configuration >> 2); // ...00000 00000000 //channel_configuration(3bit取低2bit)+original_copy(1)+home(1)+copyright_identification_bit(1)+copyright_identification_start(1)+frame_length(13bit取最高2bit) buf[3] = (aacAdstHead->channel_configuration original_copy home copyright_identification_bit copyright_identification_start frame_length >> 11); //frame_length(13bit取：去掉最高2bit开始8位) buf[4] = (aacAdstHead->frame_length >> 3) & 0x0FF; //frame_length(13bit取最低3bit)+adts_buffer_fullness(11bit取高5bit) buf[5] = (aacAdstHead->frame_length adts_buffer_fullness >> 6); //adts_buffer_fullness(11bit取低6bit) + number_of_raw_data_blocks_in_frame(2) buf[6] = (aacAdstHead->adts_buffer_fullness number_of_raw_data_blocks_in_frame; return ret; } int audioToAac(FILE *fp_audio, Tag *tag) { int ret = 1; unsigned SoundFormat = tag->Data[0] >> 4; unsigned SoundRate = (tag->Data[0] & 0x0F) >> 2; unsigned SoundSize = (tag->Data[0] & 0x02) >> 1; unsigned SoundType = tag->Data[0] & 0x01; //SoundData = AACPacketType + Data unsigned AACPacketType = tag->Data[1]; if (SoundFormat != 10) {//10 = AAC printf(\"Not AAC Error\\n\"); ret = 0; goto end; } if (AACPacketType == 0) {//0: AAC sequence header(参考14496-3 AudioSpecificConfig https://wiki.multimedia.cx/index.php/MPEG-4_Audio#Audio_Specific_Config) printf(\"0: AAC sequence header\\n\"); //读取flv文件中的头信息 FLV_AAC_HEAD *flvAacHead = (FLV_AAC_HEAD *) malloc(sizeof(FLV_AAC_HEAD)); //只要前面两个字节就行，如0x1390=00010011 10010000 flvAacHead->audioObjectType = tag->Data[2] >> 3;//UB[5]=00010... ........ flvAacHead->samplingFrequencyIndex = ((tag->Data[2] & 0x07) Data[3] >> 7);//UB[4]=.....011 1....... flvAacHead->channelConfiguration = (tag->Data[3] & 0x7F) >> 3;//UB[4]=........ .0010... flvAacHead->other = tag->Data[3] & 0x07;//两个字节剩余的bit，为0也行。 //构造AAC_ADST_HEAD if (aacAdstHead) { free(aacAdstHead); } aacAdstHead = (AAC_ADST_HEAD *) malloc(sizeof(AAC_ADST_HEAD)); aacAdstHead->syncword = 0xFFF; aacAdstHead->ID = 1; aacAdstHead->layer = 0; aacAdstHead->protection_absent = 1; aacAdstHead->profile = flvAacHead->audioObjectType; // aacAdstHead->profile = 1; aacAdstHead->sampling_frequency_index = flvAacHead->samplingFrequencyIndex; aacAdstHead->private_bit = 0; aacAdstHead->channel_configuration = flvAacHead->channelConfiguration; aacAdstHead->original_copy = 0; aacAdstHead->home = 0; aacAdstHead->copyright_identification_bit = 0; aacAdstHead->copyright_identification_start = 0; // aacAdstHead->frame_length = xx; //帧的长度动态计算的 aacAdstHead->adts_buffer_fullness = 0x7FF; aacAdstHead->number_of_raw_data_blocks_in_frame = 0; printf(\"audioObjectType=%d\\tsamplingFrequencyIndex=%d\\t channelConfiguration=%d\\n \", flvAacHead->audioObjectType, flvAacHead->samplingFrequencyIndex, flvAacHead->channelConfiguration); free(flvAacHead); } else if (AACPacketType == 1) {//1: AAC raw printf(\"1: AAC raw\\n\"); unsigned char *buf = (unsigned char *) malloc(7);; if (getAACHeadBuf(buf, aacAdstHead, tag->DataSize - 2 + 7) == -1) { printf(\"get AAC Head Buf Error\\n\"); ret = 0; goto end; } fwrite(buf, 1, 7, fp_audio); fwrite(tag->Data + 2, 1, tag->DataSize - 2, fp_audio); free(buf); } end: free(tag->Data); return ret; } int test() { return 0; } int main() { if (test()) {//非0位true exit(1); } printf(\"delete cache video file ? %d\\n\", remove(\"output/Kobe.h264\")); printf(\"delete cache audio file ? %d\\n\", remove(\"output/Kobe.aac\")); FILE *fp_in = fopen(\"assets/Kobe.flv\", \"rb+\"); FILE *fp_video = fopen(\"output/Kobe.h264\", \"wb+\"); FILE *fp_audio = fopen(\"output/Kobe.aac\", \"wb+\"); Tag *tag; if (!fp_in) { cout TagType == 0x12) {//脚本数据 } if (tag->TagType == 0x08) {//音频数据 ret = audioToAac(fp_audio, tag); } if (tag->TagType == 0x09) {//视频数据 ret = videoToH264(fp_video, tag); } } if (sps_nalu) { free(sps_nalu); } if (pps_nalu) { free(pps_nalu); } if (buf_start) { free(buf_start); } fclose(fp_video); return 0; } 使用ffplay查看输出文件 h264文件播放： ffplay xxx.h264 aac文件播放： ffplay xxx.aac 测试文件下载地址 参考 https://www.cnblogs.com/chyingp/p/flv-getting-started.html ISO/IEC_14496-3 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/02_h265.html":{"url":"MustRead/02_h265.html","title":"基于H.264看H.265","keywords":"","body":"基于H.264看H.265简述H265结构解析1. NALU type看回H264-视频编码之H.264H2652.2 实例分析基于H.264看H.265 简述 相对于H264来说，H265结构变化看H.265与H.264的差异详解 。但对于解析来说，最主要是多了视频参数集VPS。编码压缩方面相对于H264的宏块单位划分，增加最大为64x64（虽然名字不叫宏块）（压缩率更大了），预测方向也更多了（细节也更清晰了），以树状结构算法存储（可能照成I帧内压缩率高，因为要存这些树状信息），P帧得到很大的压缩率。 H265结构解析 1. NALU type 看回H264-视频编码之H.264 在H264中，每一个前缀码后面跟随的第一个字节即为NALU的语法元素，主要有三部分组成： forbidden_bit(1bit)，nal_reference_bit(2bits)（优先级），nal_unit_type(5bits)（类型） 所以，在H264中，我们如果需要获取NALU的类型，则可以通过以下方式进行解析： nalu_type = first_byte_in_nal & 0x1F 1 H265 而在H265中，每一个前缀码后面跟随的前两个字节为NALU的语法元素，主要有四部分组成： forbidden_zero_bit(1):nal_unit_type(6):nuh_layer_id(6):nuh_temporal_id_plus1(3) 1 在文档中定义如下： 可以看到，NALU的语法元素由H264的一个字节变为两个字节，而nal_unit_type则为NALU的类型，因此我们可以通过以下获取NALU的类型： int type = (code & 0x7E)>>1; 1 type的定义值如下： 上图，即为H265的NALU的TYPE,这里可以将上面的type简单的理解为如下我们需要的类型： VPS=32 SPS=33 PPS=34 IDR=19 P=1 B=0 2.2 实例分析 如下，为下载的视频文件surfing.265的头部信息 如上我们看到了四个NALU包，每个NALU的头部信息为： ① 00 00 00 01 40 01 ---> (0x40 & 0x7E)>>1 = 32 ---> VPS ② 00 00 00 01 42 01 ---> (0x42 & 0x7E)>>1 = 33 ---> SPS ③ 00 00 00 01 44 01 ---> (0x44 & 0x7E)>>1 = 34 ---> PPS ④ 00 00 00 01 26 01 ---> (0x26 & 0x7E)>>1 = 19 ---> IDR 通过以上头结构也可以看到，NALU的与语法元素中，forbidden_zero_bit通常为0，nuh_layer_id通常为0，nuh_temporal_id_plus1通常为1。 参考： H265码流结构分析 H.265与H.264的差异详解 H265码流结构文档参考：《T-REC-H.265-201504-I!!PDF-E.pdf》 高效率视频编码 首页资料 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/03_fst.html":{"url":"MustRead/03_fst.html","title":"面试题整理","keywords":"","body":"面试题整理面试题整理 1.什么是封装格式，视频的压缩数据是什么？ 封装格式是指对音频、视频、字幕等通道进行封装成一个文件，比如：mp4、flv、rmvb等等；视频压缩的数据是指码流。 2.视频为什么需要进行编码，编码的意义是什么(那些内容何以压缩和编码)？ 原始视频太大，不能有效的进行保存和传输，需要对原始数据进行压缩编码处理。编码能把数据进行很大力度的压缩处理，针对于视频中裸流数据YUV和音频的裸流数据PCM进行压缩和编码。 3.硬编码与软编码有什么区别？ 硬编码是指使用DSP芯片等硬件设备进行编码，软编码是指使用CPU进行编码，而我们程序员中的CPU是进行算法运行的CPU，因为从物理上来说CPU会集成其他芯片比如手机高通CPU会把DSP芯片集成到CPU里面去。硬编码优点是速度快、不占用CPU资源，但是确实各大厂商的设备质量好坏不一，存在很大的兼容性问题。软编码是指使用CPU进行编码，优点是介绍兼容性问题，缺点是速度慢，影响CPU做其他运行。 4.I帧 B帧 P帧的区别是什么？ I帧是关键帧，包含完整的图像信息，解码出来就能完全显示。码流数据很大。 5.B帧是双向预测帧，相似度在95%以内。需要依赖于I帧和P帧进行编解码。码流数据很小。 P帧是向前预测帧，相似度在70%以内。需要依赖I帧或者前一个P帧进行编解码。码流数据一般。 6.什么是GOP序列？ Group Of Picture，GOP是指图像组，同一个场景的数据，音视频编码中的一种重要的压缩技术，能够对连续的图像进行预测和压缩，在提高压缩率的同时保持视频质量，同时也方便视频的时间编辑。GOP包含一个关键帧和若干个非关键帧，关键帧是最重要的一帧视频，而非关键帧是通过关键帧和前一帧之间的预测得到的。 7.解码顺序和播放顺序是一样的吗? 不一样。因为视频的编解码顺序中B帧是以P帧作为参考帧，而播放时间顺序中B是在I帧和P之间。 8.宏块是什么，为什么有宏块？ 宏块是指在视频编码中一帧图片划分出来64x64、16x16、16x8、4x4等一个个小区域，为了在帧内加大压缩力度。因为划分的宏块中，宏块里面的内容都很相近，使用视频信源编码器进行编码过程中，只会获取宏块顶部和左侧的像素点，以及颜色变化的预测方向。而宏块越大，压缩的比例越大。 9.如何从码流片段中解析出完整一帧？ 从起始码（00 00 00 01）中截切后面的一个NAL单元，判断第一个字节后5bit是否是IDR类型就是一个完整的I帧了。 10.讲讲短视频和直播参数应该如何调节的？ 两者都是网络传输要求码流都比较小。 短视频中：是一个完整的文件，对细节有要求，所以fps(帧率)一般比较高，像抖音的可达到60帧。既然fps大了，那要求每个GOF（两个I帧间隔）比较长，即B帧的比较多。有些视频的GOF有十多秒。 直播：为了让新加入的的观众能尽快的看到画面，每个GOF则要求短，一般两秒一个I帧。但直播对fps感知并不高，15帧就好了。 11.即时通话如何保持音视频同步呢？ 实时，把最新的推流过去就是最好的同步，也就是编码时把当前的时间戳作为音频和视频的pts即可。 12.webrtc难点在哪？ P2P打洞技术。回音消除技术。 13.网络抖动时，如何优化使得播放更平滑？ 使用缓存，就是先加载后面数据一段时间，如：网站视频播放。但是这跟实时通讯相冲突了。所以是一把双刃剑。 14.RTMP和RTSP之间的应用场景？ RTMP用于直播；RTSP用于安防，需要保证每一帧画面不丢失。 15.直播会有B帧吗？为什么？ 直播一般没有B帧，B帧需要参考I帧和P帧才能解码出来，有拿到B帧时还没收到P帧，照成无法解码，照成显示延迟。 16.直播连麦是怎么实现的？ 直播 + 音视频通话 17.简述音视频开发中的编码和解码的过程 音视频编码过程：采集视频或音频流，经过编码器编码为相应格式，例如H.264、AAC等。音视频解码过程：读取编码后的音视频数据，经过解码器解码为原始音视频数据。 18.描述音视频码率的概念和在音视频开发中的应用。 音视频码率是用来衡量在一定时间内媒体数据量的大小，通常以kbps为单位表示。在音视频开发中，可以通过调整码率的大小来控制音视频的质量和传输速度。 19.请解释音视频同步的概念及其原理。 音视频同步表示音频和视频的播放速度、时间轴的同步，能够使得听众和观众看到和听到的画面和声音是一致的。音视频同步的原理是播放器管理音频和视频的缓冲器，通过缓冲大小的调整，使得播放两种信号时的时差尽量减小，从而实现同步。而调整缓冲大小的依据是看基础时钟，可以作为时钟的有3中，视频、音频或者系统时间，通常是以音频时钟为基，因为耳朵对声音更敏感。 20.描述音视频采集和播放的流程和过程。 音视频采集流程是采集音视频的数据，采集的过程中会对信号进行采样和预处理，并保存为特定的格式。播放流程是将采集的音视频数据解码，渲染并输出的过程。这个过程由播放器控制，经过缓冲、解码等处理后，将音视频输出给特定的设备。 21.请说明声音的数字化过程。 声音的数字化过程是指将声音信号转化为数字信号的过程。首先将声音信号进行采样以获取一定数量的信号点，然后对每个采样点进行量化，将其转化成离散的数字数值，最后将离散的数字数值转换成二进制数值。 22.什么是音视频流？请描述音视频流的分类。 音视频流是指音频或视频数据在网络环境下流动的格式化数据流。音视频流的分类包括实时流、非实时流、点播流、直播流等。 23.简述音视频原理中的YUV颜色模型。 YUV是一种颜色格式，在色彩空间中使用亮度Y和两个色度信号U和V来描述颜色。Y表示图像的亮度信息，U和V则表示颜色差信息。YUV广泛应用于视频编码和图像处理中，其中的Y分量可以在彩色和黑白之间转换，而U和V分量度差（Chrominance Difference），是确定颜色的信息。 24.解释色度子采样和亮度子采样的概念。 色度子采样是指在编码过程中对色度信息的缩减，比如将24位RGB图像转换为16、12、8位YUV。亮度子采样是指在采样时采用具有更低采样率的信号。采用亮度子采样技术可以帮助降低视频数据量，从而减少存储和传输成本。 25.详细描述音视频的帧率和分辨率的概念，并说明两者之间的关系。 帧率是指播放器每秒显示的图像帧数，通常以frames per second(fps)来表示。分辨率是指图像的精细度，表示图像的水平和垂直方向的像素点数。两者之间的关系是：每秒播放的图像帧数越多，视频数据的流畅度就越高；而分辨率越大，则视频的清晰度越高。 26.描述音视频网络传输和处理中的延迟和丢帧的概念和对音视频质量的影响。 音视频网络传输和处理中的延迟是指音视频信号从源到接收端的传输时间，丢帧是指音视频信息或部分视频数据在传输过程中出现了丢失。这些问题会对音视频质量产生不同的影响，如延迟过大会使音视频不同步，影响播放 体验；而丢帧则会导致视频画面的花屏和卡顿现象。 27.分析音视频编解码中的压缩和解压缩的过程。 音视频编解码中的压缩过程是指将原始的音视频数据转换为各种压缩格式，通常是有损或无损压缩，以实现数据的压缩和传输。而解压缩是指将压缩后的音视频数据还原为原始的音视频数据，通常包括数据解码、解封装、还 原等步骤。 28.什么是音频的音调和音量？ 音调指的是声音的基本频率，也称为音高。一般来说，频率越高，音调越高，反之亦然。人耳可以听到大约20Hz到20kHz之间的频率。 音量指的是声音的强度或者说是响度。音量与声音的振幅有关，振幅越大，音量越大。音量通常用分贝（dB）表示，0 dB代表听力的门限，即人耳可以听到的最小强度的声音。 29.什么是音频编码？ 音频编码是将原始的音频信号转换为具有压缩性和可传输性的数字信号的过程。音频编码包括有损编码和无损编码两种方式。有损编码会牺牲一些音频信号内容，并使用一些失真技术来压缩音频信号的数据，使得压缩后的音频信号比原始数据更小。常见的有损编码格式包括MP3、AAC等。无损编码则使用较少的压 降低压缩带来的数据损失，保持音频信号的准确性。无损编码格式通常包括FLAC、APE、ALAC等 30.音频编码一般使用哪些技术？ 基本概念: 1.采样：将模拟信号转换为数字信号的过程。 2.低通滤波：滤除高频部分无用信息，减少冗余数据，降低信号带宽。 3.量化：将采样后的数字信号映射到一定量级，得到相对粗略的数字信号。通过选取合适的量化级别可以实现压缩。 4.压缩算法：采取压缩算法将数据压缩至更小的尺寸，采用压缩算法可以达到更好的数据压缩效果，从而更好地实现带宽管控和存储管理。 常见技术和算法： 1.基于快速傅里叶变换（FFT）的算法，可以高效的从频域角度对信号进行编码压缩。 2.ADPCM算法，这种算法能够采用比采样率更小的比特率运行，并在此基础上提供更高的音频质量。 3.MP3算法，这种算法通常采用带控制的比特率，以达到更好的音频保真度。 4.AAC算法，是一种相对较新的音频编码技术，支持各种采样率和数据传输速率，目前较为流行。 31.AAC编码相对其他音频编码有哪些优势？ 1.更高的压缩比，相较于MP3和WMA，AAC可以获得更高的压缩比，这意味着采用AAC编码处理同样长度的音频文件，其体积通常更小。 2.更好的音质，AAC编码相比于同等比特率的MP3编码，可以达到更好的音频质量，表现在听觉上有更低的音频失真、更低的噪音水平和更多的高频成分等。 3.更高的采样率，AAC支持更高的采样率，目前最高可达到96kHz，相对于MP3的44.1kHz，AAC具有更广泛的音频处理能力。 4.更多的信道数，AAC可以支持更多的信道数量，最多可以支持48个信道（7.1声道），且在前向纠错和通道元素等方面提供了更高的灵活性和可控性 32.如何选取合适的音频采样率? 音频采样率是指音频信号在一秒内采样的次数，。一般来说，通常会选择44.1kHz或48kHz的采样率，这已经可以满足大多数音频需求。如果是高 的音频，建议选择96kHz及以上的高采样率，这可以带来更好的音频体验，同时数据量也会变得更大。具体需要看业务场景。 33.音频编码中是如何处理多个声道的？ 在音频编码中，多声道参考通常分为两种，包括立体声（Stereo）和环绕声（Surround Sound）。多声道参考的处理方法是在原始音频信号上增加了音频信号的通道数，比如从单声道（Mono）到双声道（Stereo），或者从立体声到环绕声。在多声道编码中需要对每个声道进行独立处理，包括采样、压缩、编码等等。 常见的处理方法包括： 1.通道复制法（Channel Duplication）：对于简单的双声道编码，采用通道复制法可以实现音频信号的双声道扩展，将单声道信号直接复制到两个声道上即可。这种方法虽然简单，但仍然可以满足很多应用需求。 2.空间编码法（Spatial Coding）：空间编码法是一种处理立体声的方法。它可以根据音像定位的原理将音频信号拆分到左右两个声道，并通过比较频率，相位和延迟等多个方面，进行复杂的处理，从而实现音乐与更好的环境融合。 3.矩阵编码法（Matrix Encoding）：矩阵编码法是一种处理环绕声的方法。这种编码可以通过比较信号的幅度、相位和延迟等方面，在更多的声道之间建立复杂的关系，以实现更好的音频空间效果。常用的有Dolby Surround和DTSSurround。 34.什么是音频的采样精度? 音频的采样精度指的是对采样后的模拟信号的量化精度，或者说是采样率所组成的二进制数据的位数。采样精度越高，音质同样也会越好，但产生的数据量也会越大。采样精度通常用位数据表示，例如16位、24位、32位等。 35.什么是PTS和DTS？ 在音视频编码中，PTS和DTS是两个重要的时间戳概念。PTS（Presentation Time Stamp）用于确保视频和音频播放的时间正确，DTS（Decode Time Stamp）用于确保解码器解码的顺序正确。 36.什么是音视频的容器格式？ 音视频的容器格式指的是把音频、视频、字幕等多种媒体文件数据流打包成一个单独的文件格式。容器格式像是一个集装箱，可以把各种编码、格式的音视频数据流装进去，方便存储和传输。容器格式通常包括文件头、媒体数据和文 件尾等部分。如：AVI、MP4、MKV、MOV、WMV、FLV、3GP等。 37.H.265相对于H.264，编码和解码的变化有哪些？ 1.块大小：H.264一般使用16x16像素的方块进行压缩，而H.265引入更小的块（8x8）和更大的块（64x64），以适应各种像素密度的视频压缩。 2.预测算法：H.265采用更复杂的运动估计预测算法和更多的预测方向。它可以利用多层和不同类型的预测探测，如双向预测和三向预测，以提高编码效率和图像质量。 3.内部滤波：H.264和H.265都使用内部滤波来优化压缩质量。其中，H.265使用更强大的内部滤波算法，以提高视频质量和编码效率。 4.位深度：H.265支持更高的位深度，包括8位、10位、12位和14位深度的视频。它不仅可以提供更高的图像质量，还可以在更广的颜色空间中保存色彩细节。 5.编码质量与需求：在相同的比特率下，H.265可以实现更好的视频质量，特别是对于高清视频和4K视频。虽然H.265的编码效率和压缩率较高，但编码和解码复杂度更高，需要更强大的硬件压缩和解码处理器来实现。 总体而言，H.265相对于H.264进行了更多的算法创新和标准改进，使得图像质量、压缩率、位深度和色彩准确性等方面都得到了提高。然而，H.265的编码需要更多的计算资源，对硬件要求更高，因此需要更先进的硬件和软件解 决方案来支持新的编码格式。 38.如何减小视频的码率？ 视频的码率（Bitrate）是指视频文件中每秒钟所拥有的数据量。码率越高，视频画质越好，但同时也意味着视频文件占用的空间越大，传输和分享视频的成本就会越高。因此，如果需要减小视频的码率以缩小视频文件大小，可以执行以下一些方法： 1 降低视频分辨率：视频分辨率指的是视频画面的宽和高像素数。一般来说，视频的分辨率越高，画面就越清晰，也就需要更高的码率来支持。因此，降低视频分辨率是一种常见的减小视频码率的方法。通过降低分辨率，可以减少需要编码的数据量，从 而减小视频文件的大小。 2 压缩视频编码格式：视频编码格式的种类很多，有些编码格式可以在保持视频质量的情况下使用更少的码率。例如，H.265比H.264提供更高的视频压缩率，因此可以将视频格式从H.264转换为H.265，以降低视频文件的码率。 3 降低视频帧率：视频帧率指每秒钟播放的视频帧数。通常视频的帧率都在24-30帧之间，降低视频帧率可以减少视频文件的大小。例如，将视频帧率从30fps降低到15fps，可以将视频文件的大小减半。 4 降低视频质量： 降低视频质量可以减小视频文件的大小，但也会影响视频的观看体验。可以通过调整视频码率和视频像素率等参数来控制视频质量。一般来说，视频质量越高，码率就越高，文件大小也就越大。因此可以考虑选择一个适当的视频质量 作为视频输出的标准。 总的来说，减小视频码率的方法有很多种，可以通过降低分辨率、帧率、视频质量、采用更好的视频压缩算法等方法来实现。不过，需要同时平衡视频质量和缩小文件大小之间的关系来实现自己的需求。 39.如何提高视频的压缩比？ 1 降低视频的分辨率和帧率。减小视频的分辨率和帧率可以大大减小视频的文件大小，从而提高压缩比。 2 选择更高效的视频压缩算法。比如可以采用 H.265 编码器而不是 H.264 编码器，因为 H.265 可以在相同画质下获得更小的文件大小。 3 调整视频的码率。码率是视频压缩中非常重要的参数，它是视频数据传输的速率。降低码率可以大幅降低视频的文件大小，但可能会降低画质。 4 优化视频的编码设置。例如，在编码设置中启用“两次压缩”选项可以显著减小视频的文件大小。 40.什么是颜色空间（Color Space）？ 颜色空间（Color Space）是指在一定条件下，能够呈现出一种颜色的某种特定组合的三个基本颜色通道的集合。通常用于确定数字图像或视频的颜色表示方法。颜色空间描述了颜色在数学上的表示方式以及如何将它们映射到设备或媒体上。常见的颜色空间有 RGB、CMYK、HSV、HSL 等。 41.常用的音视频采集设备有哪些？ 1 摄像机：摄像机是一种常见的录制视频的设备，主要用于电影拍摄、电视广播等领域。现在，各种类型的摄像机已经开始广泛应用于教育、企业、医疗等领域。 2 麦克风：麦克风用于捕捉录音，是一种非常实用的音频采集设备。根据不同场合和需求，麦克风的种类也不同，有动态麦克风、电容麦克风、无线麦克风等。 3 采集卡（Capture Card）：采集卡是一种能够将模拟信号转换为数字信号的电路板，通常用于捕获模拟视频和音频信号。采集卡通常具有高带宽、低延迟和广泛的操作系统支持，可以应用于实时视频、网络流媒体等场景。 4 视频采集设备：视频采集设备是一种能够将模拟视频信号转换为数字信号的设备，通常用于将模拟视频信号转换为数字信号。它们通常具有多种接口，包括USB、HDMI、DVI等，可以应用于监控、直播、视频会议等场景。 5 录音笔：录音笔是一种小巧便携的录音设备，通常用于个人采集音频、面试录音、会议记录等场景。随着技术的不断发展，一些录音笔还支持 MP3 录制、TTS 文字转语音等功能。 42.什么是音视频的处理？ 音视频处理（Audio Video Processing）是指对数字音频和视频信号进行处理和操作的技术过程，它可以涉及音视频采集、编码、解码、编辑、转码、传输、合成等多种技术领域。 音视频处理的目的通常包括以下几个方面： 1 提高音视频质量：音视频处理技术可以通过去噪、降噪、降噪、增益、均衡器等技术手段对音视频信号进行优化和改善，提高音视频质量。 2 实现音视频数据的压缩：通过压缩可以将音视频信号的数据量减小到一个更合理的范围，从而降低音视频的传输成本，并且可以提高实时传输的效率。 3 实现音视频格式转换：通过转码和封装等技术手段，可以将音视频数据转换成不同的格式，使其适合于不同的播放设备、嵌入式设备、网络传输等应用场景。 4 适应不同的网络传输环境：音视频处理技术可以根据不同的网络环境和传输带宽自适应地调整音视频播放速度和清晰度，保证音视频播放的流畅度和稳定性。 5 实现多媒体数据的编辑和合成：音视频处理技术可以通过对音视频信号进行裁剪、剪辑、合成、加密等操作，实现多媒体数据的编辑和制作。 43.什么是音视频的流媒体？ 音视频的流媒体称为“流式媒体”（StreamingMedia），是指音视频数据在网络上传输的过程中，通过网络的传输技术将数据“流式”地传送到播放软件或硬件中，使得用户可以边下载、边播放，无须等待全部数据下载完毕。实现流式媒体技术的主要原理是将大的音视频文件分成许多小的数据块，通过网络一段一段地传输到客户端缓存，播放软件播放时再从缓存中提取并播放，播放软件能够实时解码、播放数据块，能够实时播放出来，达到了在线收听收看的效果。 44.常见的音视频流协议有哪些？ 1 RTMP（Real Time Messaging Protocol）实时消息传输协议，是一种实时数据传输协议，可用于音视频流推送和播放等。它基于TCP连接，使用HTTP格式的消息，适合于流媒体数据的实时传输和动态流媒体交互。 2 RTSP（Real Time Streaming Protocol）实时流协议，是一种基于轻量级的协议，使用 UDP 传输方式，是一种基于时间的协议，主要用于 IP 网络中的媒体播放和流媒体数据传输等。 3 HLS（HTTP Live Streaming）是苹果公司提出的一种流媒体传输协议，基于 HTTP（HyperText Transfer Protocol）协议，具有多码率适应、可靠性好、可缓存、容易部署等特点。HLS 是苹果公司为 iOS 系统和 Safari浏览器开发的，但目前许多主流浏览器和设备也支持该协议。 4 HTTP-FLV 是一种基于 HTTP 协议传输的 FLV 文件格式，当前主要在国内使用。它的原理是将视频流封装在 FLV 文件中，使用 HTTP 协议传输数据。 5 WebRTC（Web Real-Time Communication）是 Google 公司提出的技术，主要应用于核心部分的数据的实时传输和转发方面，包括音视频等实时数据传输，目前被广泛应用于网页视频会议、在线教育、远程医疗等领域。 45.音频降噪原理? 音频降噪（Noise reduction）技术是指对语音信号中的噪声进行去除或者削弱的过程，让信噪比更高，使得人们的语音通信更加清晰。 音频降噪的原理可以简单概括为四步： 1 对语音信号进行采集，通过麦克风将语音信号转化为电信号。 2 对采集到的语音信号进行数字信号处理（DSP）,提取出与人类语音重要的语音频率特征。 3 通过运用滤波器、自适应滤波器、时域方法等技术，在保留语音特征的同时消除噪声，提高信噪比。 4 对降噪后的信号进行信号再生（equalization）, 使得音量、音质与原始信号尽可能地一致。 具体的降噪技术有基于频域的消噪技术，如FFT（Fast Fourier Transform）FFT算法和Spectral subtraction算法等；基于时域的消噪技术，如Wiener滤波、自适应滤波器、FIR滤波器、Kalman滤波器等等。此外，还可以使用混合噪声模型进行降噪。 需要注意的是，音频降噪虽然可以明显的提高音质与语音信号的质量，但是并不能完全消除噪音，因此降噪算法的目标是尽量最小化对语音信号的影响。 46.视频压缩压缩的是什么信息? 帧内压缩与帧间压缩原理？ 视频压缩技术压缩的是视频中的像素信息。 视频压缩技术中，有两种常见的压缩方法，即帧内压缩和帧间压缩。 帧内压缩是指单独对一帧（Frame）图像进行压缩，最常用的格式有MJPEG和H.264 AVI格式。它包含的压缩技术主要有以下几点： 1 去除冗余信息：对于一幅图像（每秒播放24至30张），在不同区域的同一颜色需要抽取其中一个进行编码，这样就可以大大降低码流。 2 量化：在图像处理过程中，会将图像中的一些区域作为一个宏块来处理，宏块中的像素在编码时是采用分组含义的，分组过程中经过适当的采样调整亮度和色度。 3 DCT编码：把宏块分成若干个8x8的图像块，每个 8x8 的图像块通过DCT（Discrete Cosine Transform）变换得到一个 8x8 的频域系数矩阵，对这个矩阵进行压缩。 4 颜色空间转换：通常，视频压缩器采用的是色度子采样技术，即在编码时减少对色度信号采样而对亮度信号进行全采样，这样也可以大幅度减少码流。 5 熵编码：熵编码可以说是视频压缩的关键，常用的熵编码方式有Huffman编码和Arithmetic编码，Huffman编码是一种按照数据的出现频率来确定数据的编码方式，而 而帧间压缩则是将一组连续的帧（从视频的播放角度看，通常为多个连续图像）进行压缩。在帧间压缩中，通常会采用一些特殊的技术来提取出视频编码中的空间和时间冗余信息，如基于运动估计的技术（Motion Estimation）和DCT变换等。 具体来说，帧间压缩将视频分解为连续的图片帧，每帧都进行图片压缩后存储。由于相邻帧中大部分像素点是相同的，所以也可以采用一些算法来压缩帧与帧之间的差异，例如P帧和B帧的概念。其中，P帧（Predicted Picture）会根据之前出现的一张参考帧来预测当前帧，将预测出的残差信息进行压缩保存；而B帧（Bidirectional predicted picture）则是根据前后两张参考帧来预测当前帧的压缩方式。这样压缩后的视频体积更小，播放速度更流畅。 总的来说，帧内压缩适用于静态图像的压缩，而帧间压缩适用于动态图像的压缩。通过这些方法，视频能够在保证一定画质和可视度的情况下，将数据压缩到较小的体积，提高了存储和传输效率。 47.直播互动是如何实现的？ 直播互动是指在直播平台中观众可以通过不同方式与主播进行互动，包括评论、点赞、送礼物、打赏、参与抽奖等，而主播则可以与观众进行沟通、回应、抽奖等。这种互动能够提升直播的人气和粘性，也可以带来一定的营收。 实现直播互动的技术和平台： 1 实时通信技术：直播互动需要实时通信，观众和主播之间的消息需要通过网络传输并实时显示在直播页面上。为了保证实时性，需要采用实时通信技术，例如 WebSockets、Socket.IO、MQTT等。这些技术可以实现双向通信、较低的延迟和高可靠性，能够满足直播互动的需求。 2 礼物打赏系统：礼物打赏是观众向主播表达喜爱和支持的一种方式，也是直播平台的主要收入来源之一。实现礼物打赏系统需要涉及支付接口、虚拟货币系统、礼物商城、贡献榜等。直播平台需要提供可靠的支付接口，实现虚拟货币兑换和礼物购买 功能，并可以记录观众的贡献度和消费记录。 3 弹幕和评论系统：弹幕和评论是观众与主播之间获取反馈和互动的主要形式之一。弹幕是观众以文字形式发送消息，通过滚动屏幕展示有时还会带有不同颜色、动画等特效。评论系统则是观众自由发送消息进行交流的区域。实现这些系统需要采用实时 通信技术，同时还需要具备实现页面上弹幕或评论浮层效果等前端技术。 4 抽奖和活动系统：抽奖和活动是直播互动的一种推广方式，能够带来用户活跃度和粘性。例如在进行特殊的节目、活动、主持等内容时，主播通过为观众发放抽奖码来吸引观众参与。抽奖和活动在技术实现上需要后端存储和管理活动的规则、奖品的种 类、参与人数等信息。同时还要具备互动规则，例如抽奖码的生成、开奖时间、中奖规则、奖品发放等。 48.音视频通话如何实现？视频会议原理？ 音视频通话主要是通过 VoIP 技术实现的，即基于 Internet Protocol (IP) 的语音和视频通话。它将语音和视频信号数字化，通过互联网传输，让两个或多个人可以实现远程通话，包括一对一通话和多人视频会议。 音视频通话的实现原理： 1 音视频采集：使用麦克风和摄像头采集声音和视频信号，并将它们转换成数字信号。 2 音视频编解码：将数字信号进行编码和解码，以进行传输和播放。编解码过程可以压缩声音和视频信号，从而降低带宽和网络延迟要求，并提高数据传输的效率。 3 传输协议选择：选择合适的传输协议，例如 UDP、TCP 或 HTTP(S)。UDP 传输速度快、延时低，适用于视频通话。TCP 方便进行连接管理和数据重传，适用于音频通话。 4 数据传输：采用 IP 协议将数字信号传输到另一端，传输时需要考虑网络带宽、网络拥塞、延迟等因素。音视频可以分别传输，也可以同时传输。 5 音视频解码：接收端进行音视频的解码，支持多种解码方式，如H.264、VP8、VP9等。 6 声音和视频输出：将解码后的声音和视频信号通过喇叭和显示器输出。通常有多种输出方式，例如扬声器、耳机和外置音响等。 视频会议的实现原理： 1 多人视音频编码：与一对一通话类似，多人的视频也要进行编码。 2 多人视音频混合：混合多个来自不同端的音视频流，并进行媒体同步，保证使用的是相同的音频和视频数据同步。 3 QoS管理：网络实时性服务质量（QoS）机制管理在多方视频中更重要。QoS可以保障视频、音频的传输速度，同时还可以防止其它应用程序抢占网络资源，提高用户体验。 4 视频会议控制：会议控制层负责用户之间的协商与交流。例如，设置视频会议主持人角色，通过视频会议平台共享屏幕，允许分享PPT等。 5 呼叫控制：与一对一通话类似，在开始一个视频会议之前需要进行呼叫控制和连接建立，用于协商各参与方的连接通路。这个过程中，要确保会议的连接以及必要的信息安全。 49.如何完成两个视频流之间的音视频同步？ 在实际的视频直播、视频会议等场景中，经常需要对多个视频流进行同步播放。对于两个视频流之间的音视频同步，通常有以下几个步骤： 1 确定时间戳： 每个音视频流中都会包含时间戳信息，以确定每一帧的播放时间。通常视频帧的时间戳是基于采集时间的相对时间戳，而音频帧的时间戳是基于采样时间的绝对时间戳。在进行音视频同步时，需要将两种时间戳进行转换，以确保它们 在同一时刻播放。 2 采用缓冲方式： 由于音频和视频的数据大小、帧率不同，会导致两个流之间的数据到达时间存在差异，为了进行同步处理，需要采用缓冲方式进行处理。对于视频流，可以用一个缓冲区来存储视频帧，在缓存中等待音频流的到达，等音频流接收到了 对应的时间戳数据时，将相应视频帧提取出来，进行播放。对于音频流，也可以采用一个缓冲区来存储音频帧，等待视频流的数据到达时再进行播放。 3 确定时间偏移量： 由于网络原因和硬件原因等，音视频数据在传输和播放过程中可能存在时间偏移的问题。为了解决这个问题，可以通过计算播放过程中视频和音频的时间差，以此来调整两个流之间的播放耗时，使之能够同步。 4 添加时间戳同步信息： 为了保证不同平台或端点之间的兼容性，在对音视频进行同步处理时，还需要在视频流和音频流中添加时间戳同步信息，以便后续的传输和播放处理。 50.如果要进行远程监控，你会如何处理视频数据的加密和解密？ 对于远程监控场景，需要考虑视频数据在传输和存储过程中的安全性，保障用户的隐私和版权等利益。因此，需要对视频数据进行加密处理。下面是处理视频数据加密和解密的一般方式： 1 视频加密： 对于视频加密，可以使用对称加密算法，例如 AES等算法。在视频数据采集后，对其进行加密处理，以确保视频数据不被非法访问或窃取。需要注意的是加密过程需要消耗一定的计算资源，应该保证加密的效率，以免影响视频传输的实时性。 2 加密存储： 如果视频需要存储在远程服务器中，可以采用加密存储的方式，例如使用对称加密算法加密视频数据，然后将其存储在服务器中。需要注意的是，在对视频数据进行加密存储时，需要与密钥管理系统配合使用，以确保密钥的安全和管理。 3 视频解密： 在远程客户端（如手机、PC等）需要查看加密视频时，需要对视频数据进行解密，以还原视频画面。在解密过程中，需要使用相同的密钥进行解密，如果密钥的安全性无法保障，则整个解密过程也有可能被攻击者攻击。因此，密钥管理是加密视频解密的关键。 总之，在远程监控场景下，视频数据的安全保障至关重要。采用视频加密处理和加密存储，结合密钥管理系统，可以有效保障视频数据的安全性，有效防止非法访问和窃取，保障用户的隐私。 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-28 15:36:54 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/04_transport_protocols.html":{"url":"MustRead/04_transport_protocols.html","title":"常见的音视频传输协议对比","keywords":"","body":"常见的音视频传输协议对比常见的音视频传输协议对比 协议名称 作用 发布时间 拥有者 优点 缺点 RTP 实时传输协议（Real-time Transport Protocol） 1996 IETF 实时性好、支持任意编码格式、建立连接速度快 无法保证可靠性，需要结合RTCP进行控制 RTSP 实时流协议（Real Time Streaming Protocol） 1998 IETF 实现点播和直播功能、支持播放器拖动等 数据完整性无法得到保证，不稳定 RTMP 实时消息传输协议（Real-Time Messaging Protocol） 2002 Adobe 实时性好、支持流媒体数据和交互式协议通信 兼容性受限、安全性问题 HTTP 超文本传输协议（HyperText Transfer Protocol） 1991 IETF 兼容性好、易于开发、支持多种媒体格式 实时性差、延迟高 SIP 会话初始化协议（Session Initiation Protocol） 1999 IETF 易于扩展、支持多种传输方式（TCP、UDP、TLS等） 安全性存在问题 HLS HTTP直播流媒体协议（HTTP Live Streaming） 2009 Apple 兼容性好、支持多种容器和编码格式 实现过程复杂、HLS直播端播放器有延迟 WebRTC Web实时通信协议（Web Real-Time Communication） 2011 W3C 集成于浏览器内、无需安装插件 不支持所有浏览器，需要服务器支持 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-28 11:57:45 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/05_codec.html":{"url":"MustRead/05_codec.html","title":"常见的音视频编解码器对比","keywords":"","body":"常见的音视频编解码器对比常见的音视频编解码器对比 名称 作用 发布时间 拥有者 优点 缺点 H.264 视频编解码器 2003 ITU-T 高压缩比、广泛使用、支持多种分辨率 需要较高的计算能力 H.265 视频编解码器 2013 ITU-T 压缩性能更优秀、支持更高分辨率、比特率更低 编码过程更复杂 VP8 视频编解码器 2010 Google 开源、图像质量好、支持多种分辨率 编码效率较低 VP9 视频编解码器 2013 Google 具有极高的压缩比、兼容性强、支持多种分辨率 编解码复杂度大、处理开销高 AAC 音频编解码器 1997 Fraunhofer IIS 压缩率高、音质好、支持多种采样率 无法处理低比特率的音频 MP3 音频编解码器 1993 Fraunhofer IIS 具有广泛的兼容性、压缩率高 无法处理高达96kHz以上的高采样率 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-30 07:02:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"MustRead/06_muxer.html":{"url":"MustRead/06_muxer.html","title":"常见的音视频封装格式对比","keywords":"","body":"常见的音视频封装格式对比常见的音视频封装格式对比 封装格式 作用 发布时间 拥有者 优点 缺点 AVI 音视频封装 1992年 微软 可以支持多种音视频编解码器，广泛兼容 不支持流式媒体和比较大的文件 QuickTime 音视频封装 1991年 苹果 支持多种编码格式和特殊效果，适合用于视频制作 在某些平台上可能没有很好的兼容性 MPEG-2 TS 流媒体传输 1995年 国际标准化组织 能够在不同平台上传输和播放音视频数据 不适合在本地存储大文件 MP4 音视频封装 2001年 MPEG组织 支持多种音视频编解码器和数字版权管理，适合互联网传播 不支持逐帧编辑，容易损坏 FLV 视频封装 2005年 Adobe 文件比较小，适合视频的互联网传播 不常用于电视和电影等大型传统媒体 MKV 音视频封装 2002年 Matroska组织 允许包含多个音频、视频和字幕轨道，支持流式传输和多语言标签 编码格式有限制，不支持数字版权保护 MOV 音视频封装 1991年 苹果公司 容器格式规格开放，可以支持多种编码格式和字幕轨道 性能较低，压缩率较低 WebM 视频封装 2010年 谷歌 支持VP9和VP8编码，适合在网络上传播，免费和开源 流媒体传输相对困难 HLS 音视频封装 2009年 苹果 支持流式传输，适合直播，可多码率适应网络环境 需要服务器支持，不通用 DASH 音视频封装 2012年 MPEG组织 支持流式传输，可多码率适应网络环境，可对音视频流进行个性化处理 需要服务器支持，不通用 Ogg 音视频封装 2002年 Xiph.Org基金会 免费和开源，支持多种音视频编解码器 兼容性相对较低，不支持数字版权管理 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-28 12:56:08 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"Other/":{"url":"Other/","title":"Null","keywords":"","body":"随手笔记~随手笔记~ 初步认识c/c++编译 网络基础知识 ADB的使用 JNI Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-03-29 17:23:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"Other/01_c_compile.html":{"url":"Other/01_c_compile.html","title":"初步认识c/c++编译","keywords":"","body":"初步认识c/c++编译程序的生命周期编译过程与编译器gcc/g++编译器执行过程静态库与动态库(Linux)或者重要工具gcc/g++重要参数初步认识c/c++编译 程序的生命周期 通过了解程序的生命周期，而直到编译所在生命周期的哪一个部分。 编译过程与编译器 编译过程是指编写的源代码通过编译器进行编译，最后生成cpu所能识别的二进制形式存在的源代码的过程。而编译器则是指能够使源代码编译生成二进制形式的工具，根据平台不同，工具也不同，如window是XXX.exe可执行程序，unix系统侧不定后缀名，系统根据文件的头部信息来判断。 例如：在屏幕上输出“VIP会员”，C语言的写法为： puts(\"VIP会员\"); 二进制的写法为： gcc/g++编译器执行过程 gcc/g++编译器能把一个源文件生成一个执行文件，这是因为该编译器是集成了各种程序（预处理器、汇编器等），这个过程中的工作如下： 例子：编写 test.c 如下： #include int main() { printf(\"hello world\\n\"); return 0; } 在控制台执行命令(-o：为指定输出文件)： qincji:build mac$ ls test.c qincji:build mac$ gcc -E test.c -o test.i qincji:build mac$ ls test.c test.i qincji:build mac$ gcc -S test.i -o test.s qincji:build mac$ ls test.c test.i test.s qincji:build mac$ gcc -c test.s -o test.o qincji:build mac$ ls test.c test.i test.o test.s qincji:build mac$ gcc test.o -o test qincji:build mac$ ls test test.c test.i test.o test.s qincji:build mac$ ./test hello world qincji:build mac$ 静态库与动态库(Linux) 动态库与静态库统称为函数库，根据系统不一样，后缀名标识也不一定，如图： 静态库 静态库是指编译链接时，把库文件的代码全部加入到可执行文件中，因此生成的文件比较大，但在运行时也就不再需要库文件了。优点：静态库节省时间，不需要再进行动态链接，需要调用的代码直接就在代码内部。 生成静态库 首先用gcc编绎该文件，生成.o文件，然后ar(archive)工具生成静态库。-fPIC生成与位置无关代码，不用此选项的话，编译后的代码是位置相关的，所以动态载入时，是通过代码拷贝的方式来满足不同进程的需要，而不能达到真正代码段共享的目的。 gcc -fPIC -c test.c -o test.o ar r libtest.a test.o 动态库 动态库与之相反，在编译链接时并没有把库文件的代码加入到可执行文件中，而是在程序执行时由运行时链接文件加载库。优点：动态库节省空间，如果一个动态库被两个程序调用,那么这个动态库只需要在内存中。 首先用gcc编绎该文件，生成.o文件，然后再用gcc指定-shared生成动态库。 gcc -fPIC -c test.c -o test.o gcc -shared test.o -o libtest.so 或者 gcc -fPIC -shared test.c -o libtest.so 简单使用 编辑test.c文件如下： #include void output() { printf(\"hello world\\n\"); } 编辑main.c文件如下： #include \"test.c\" int main(){ output(); return 0; } (1)使用动态库 首页将test.c编译生成动态库libtest.so，然后执行一下命令生成可执行文件： gcc main.c -L. -ltest -o main 当编译main.c时，需要链接代码中引入的test.c文件，其中，-L.：是指编译的时候，在当前目录搜索库的路径；-ltest：指编译的时候使用的库（详见gcc/g++重要参数）。 在控制台中演示： qincji:build mac$ ls libtest.so main.c test.c test.o qincji:build mac$ gcc main.c -L. -ltest -o main qincji:build mac$ ls libtest.a libtest.so main main.c test.c test.o qincji:build mac$ ./main hello world qincji:build mac$ (2)强制使用静态库 gcc main.c -L. -Wl,-Bstatic -ltest -Wl,-Bdynamic -o main 其中：-Wl 表示传递给 ld 链接器的参数，通过ld --help查看 ld 帮助文档时。 注意：我使用MacOs系统时，在查看ld帮助文档(man ld)发现系统并不支持链接动态库，描述如下： -static Produces a mach-o file that does not use the dyld. Only used building the kernel. (3)将静态库打包到动态库中 gcc -shared -o libtest.so -Wl,--whole-archive libtest.a -Wl,--no-whole-archive 其中：--whole-archive: 将未使用的静态库符号(函数实现)也链接进动态库；--no-whole-archive: 默认，未使用不链接进入动态库。 重要工具 ar : 创建或修改archive文件，或从存档文件中提取。如生成静态库，查看详细参数命令： ar --help。 nm : 列出目标文件中的符号（方法），查看动态库方法等，查看详细参数命令： nm --help。 objcopy : 复制和翻译目标文件，查看详细参数命令： objcopy --help。 objdump : 显示目标文件中的信息，查看详细参数命令： objdump --help。 readelf : 显示可执行和可链接格式的文件的信息，查看详细参数命令： readelf --help。 gcc/g++重要参数 通过gcc --help命令查看更多。 -E 只激活预处理,这个不生成文件, 你需要把它重定向到一个输出文件里面。 -S 只激活预处理和编译，就是指把文件编译成为汇编代码。 -c 只激活预处理,编译,和汇编,也就是他只把程序做成obj文件 -o 制定目标名称, 默认的时候, gcc 编译出来的文件是 a.out。 -fPIC 生成与位置无关代码。 -shared 生成动态库，使用例子： gcc -fPIC -shared test.c -o libTest.so -include file 包含某个代码,简单来说,就是便以某个文件,需要另一个文件的时候,就可以用它设定,功能就相当于在代码中使用 #include。例子： gcc test.c -include xxx.h -Dmacro 相当于 C 语言中的 #define macro -Dmacro=defn 相当于 C 语言中的 #define macro=defn -Umacro 相当于 C 语言中的 #undef macro -undef 取消对任何非标准宏的定义 -Idir 在你是用 #include \"file\" 的时候, gcc/g++ 会先在当前目录查找你所制定的头文件, 如果没有找到, 他回到默认的头文件目录找, 如果使用 -I 制定了目录,他会先在你所制定的目录查找, 然后再按常规的顺序去找。 对于 #include, gcc/g++ 会到 -I 制定的目录查找, 查找不到, 然后将到系统的默认的头文件目录查找 。 -I- 就是取消前一个参数的功能, 所以一般在 -Idir 之后使用。 -idirafter dir 在 -I 的目录里面查找失败, 讲到这个目录里面查找。 -iprefix prefix 、-iwithprefix dir 一般一起使用, 当 -I 的目录查找失败, 会到 prefix+dir 下查找 -M 生成文件关联的信息。包含目标文件所依赖的所有源代码你可以用 gcc -M test.c 来测试一下，很简单。 -MM 和上面的那个一样，但是它将忽略由 #include 造成的依赖关系。 　　 -MD 和-M相同，但是输出将导入到.d的文件里面 　　 -MMD 和 -MM 相同，但是输出将导入到 .d 的文件里面。 -Wa,option 此选项传递 option 给汇编程序; 如果 option 中间有逗号, 就将 option 分成多个选项, 然 后传递给会汇编程序。 -Wl.option 此选项传递 option 给连接程序; 如果 option 中间有逗号, 就将 option 分成多个选项, 然 后传递给会连接程序。 -llibrary 指编译的时候使用的库，library是指动态库或静态库的名称。如：liblibrary.a或liblibrary.so，系统会自动加上lib前缀和.a(.so)后缀。 -Ldir 指编译的时候，搜索库的路径。比如你自己的库，可以用它指定目录，不然编译器将只在标准库的目录找。这个dir就是目录的名称。 -O0 、-O1 、-O2 、-O3 编译器的优化选项的 4 个级别，-O0 表示没有优化, -O1 为默认值，-O3 优化级别最高。 -g 只是编译器，在编译的时候，产生调试信息。 　　 -gstabs 此选项以 stabs 格式声称调试信息, 但是不包括 gdb 调试信息。 　　 -gstabs+ 此选项以 stabs 格式声称调试信息, 并且包含仅供 gdb 使用的额外调试信息。 　　 -ggdb 此选项将尽可能的生成 gdb 的可以使用的调试信息。 -static 此选项将禁止使用动态库，所以，编译出来的东西，一般都很大，也不需要什么动态连接库，就可以运行。 -share 此选项将尽量使用动态库，所以生成文件比较小，但是需要系统由动态库。 参考 http://c.biancheng.net/view/450.html https://www.runoob.com/w3cnote/gcc-parameter-detail.html https://cloud.tencent.com/developer/article/1343895 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"Other/02_net_base.html":{"url":"Other/02_net_base.html","title":"网络基础知识","keywords":"","body":"网络基础知识通信简单流程IP⼦⽹掩码端口网关UDPTCP其他网络基础知识 通信简单流程 上图简单的描述了一下网络通信的过程，通过上面的知识点来描述一下网络基础知识，补补这方面的基础知识。 IP Internet上每一台计算机都有唯一的地址来标识它的身份，即IP地址，使用域名其实也是要转化为IP地址的。IP地址的结构为x.x.x.x，每个x的值0~255（1Btye的范围）。IP地址分类如下： 由上图可知，把IP地址转换为二进制时，根据最高的位第一个0开始区分什么类型，所以IP的组成结构为： IP = 网络地址(含类型标识) + 主机地址 上图所示的结构是固定的，但是实际上除了类型标识固定之外，网络地址和主机地址占位却是不固定的，如B类地址： IP地址：180.210.242.131； 即：10110100.11010010.11110010.10000011 同时掩码：255.255.248.0； 即：11111111.11111111.11111000.00000000 网络地址：两者进行与运算； 即：10110100.11010010.11110000.00000000（180.210.240.0） 主机地址：掩码取反再和IP做与运算，即：00000000.00000000.00000010.10000011（0.0.2.131） B类的网络地址应该为：xx*.**.xxxxxxxx.xxxxxxxx 但真实的网络地址却是：xx110100.11010010.11110xxx.xxxxxxxx 所以本来的网络号是16位，但它实际网络号是21位，就是借了5位网络位，所以可以划分252^52​5​​个子网，即32个，实际使用30个，这个网段可以容纳主机2^11^个，即2048个，有效2046个一头一尾分别做网络号和广播。 ⼦⽹掩码 ⼦⽹掩码只有⼀个作⽤，就是将某个IP地址划分成⽹络地址和主机地址两部分⼦⽹掩码的设定必须遵循⼀定的规则。与IP地址相同，⼦⽹掩码的⻓度也是32位： 左边是⽹络位，⽤⼆进制数字“1”表示； 右边是主机位，⽤⼆进制数字“0”表示。 端口 通信过程中通过IP地址找到了该台电脑后，需要再跟进端口号来识别该通过那个应用程序。端⼝号只有整数，范围是从0到65535。 端口号有知名端口和动态端口。知名端⼝是众所周知的端⼝号，范围从0到1023（类似电话中的110、119等）。⼀般情况下，如果⼀个程序需要使⽤知名端⼝的需要有root权限。 80端⼝分配给HTTP服务 21端⼝分配给FTP服务 网关 网关(Gateway)又称网间连接器、协议转换器，实质上是一个网络通向其他网络的IP地址。这一设备成为网关设备，路由器就是了。根据\"路由\"协议进行不同网段的信息传递，如图： UDP ⽤户数据报协议，是⼀个⽆连接的简单的⾯向数据报的运输层协议。UDP不提供可靠性，它只是把应⽤程序传给IP层的数据报发送出去，但是并不能保证它们能到达⽬的地。由于UDP在传输数据报前不⽤在客户和服务器之间建⽴⼀个连接，且没有超时重发等机制，故⽽传输速度很快。UDP是⼀种⾯向⽆连接的协议，每个数据报都是⼀个独⽴的信息，包括完整的源地址或⽬的地址，它在⽹络上以任何可能的路径传往⽬的地，因此能否到达⽬的地，到达⽬的地的时间以及内容的正确性都是不能被保证的。 UDP特点： UDP是⾯向⽆连接的通讯协议，UDP数据包括⽬的端⼝号和源端⼝号信息，由于通讯不需要连接，所以可以实现⼴播发送。 UDP传输数据时有⼤⼩限制，每个被传输的数据报必须限定在64KB之内。 UDP是⼀个不可靠的协议，发送⽅所发送的数据报并不⼀定以相同的次序到达接收⽅。 UDP是⾯向消息的协议，通信时不需要建⽴连接，数据的传输⾃然是不可靠的，UDP⼀般⽤于多点通信和实时的数据业务，⽐如： 语⾳⼴播 视频 QQ TFTP(简单⽂件传送） SNMP（简单⽹络管理协议） RIP（路由信息协议，如报告股票市场，航空信息） DNS(域名解释） UDP操作简单，⽽且仅需要较少的监护，因此通常⽤于局域⽹⾼可靠性的分散系统中client/server应⽤程序。例如视频会议系统，并不要求⾳频视频数据绝对的正确，只要保证连贯性就可以了，这种情况下显然使⽤UDP会更合理⼀些。 TCP TCP长/短连接优缺点 ⻓连接可以省去较多的TCP建⽴和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适⽤⻓连接。但是，client与server之间的连接如⼀直不关闭的话，会存在⼀个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取⼀些策略，如关闭⼀些⻓时间没有读写事件发⽣的连接，这样可以避免⼀些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最⼤⻓连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。短连接对于服务器来说管理较为简单，存在的连接都是有⽤的连接，不需要额外的控制⼿段。但如果客户请求频繁，将在TCP的建⽴和关闭操作上浪费时间和带宽。 其他 抓包工具推荐：wireshark 模拟网络环境：Packet Tracer 参考 https://blog.csdn.net/zhangbaoanhadoop/article/details/82745769 Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"Other/03_adb_dy.html":{"url":"Other/03_adb_dy.html","title":"ADB的使用","keywords":"","body":"ADB的使用ADB的命令使用大全其他简单实用打开应用，在命令行输入该命令来查看当前在前台的activity：编写shell脚本，然后执行：ADB的使用 ADB的命令使用大全 awesome-adb 其他简单实用 打开应用，在命令行输入该命令来查看当前在前台的activity： qincji:Downloads mac$ adb shell dumpsys activity activities | grep mResumedActivity mResumedActivity: ActivityRecord{e3c2d05 u0 com.ss.android.ugc.aweme.lite/com.ss.android.ugc.aweme.splash.SplashActivity t122} 编写shell脚本，然后执行： #!/bin/bash adb shell am start -n com.ss.android.ugc.aweme.lite/com.ss.android.ugc.aweme.splash.SplashActivity sleep 10s a=1 while (($a Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-01-13 16:59:51 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"Other/04_jni.html":{"url":"Other/04_jni.html","title":"JNI","keywords":"","body":"JNI简述数据类型映射JNI的使用静态注册案例动态注册案例相比于静态注册，动态注册有以下优点：Java对象传递给NativeNative对象传递给Java（Native回调Java）多线程中，native如何回调java？其他重点内容JNI 简述 JNI全称：Java Native Interface（Java本地接口）。JNI是提供一种Java字节码调用C/C++的解决方案，描述的是一种技术。简单来说，Java是通过JNI调用C/C++的。 数据类型映射 Java类型 Native类型 描述符 boolean jboolean Z（特殊） byte jbyte B char jchar C short jshort S int jint I long jlong J（特殊） float jfloat F double jdouble D void void V Object jobject LClassName; 其中ClassName为具体的类名，以分号结尾。注意，Java中的基本数据类型除了void都有对应的j开头的Native类型，而Obj ect类型被映射为jobject类型。如，java中的字符串：Ljava/lang/String; 在涉及到数组时，Java中的数组与指向数组元素的指针在JNI中等价。数组类型转换也非常简单：取数组元素类型的描述符 面加上[即可。例如Java中的int[]对应的Native类型为[I。 下表为Java数组类型与对应的Native类型及描述符的映射表： Java数组类型 Native类型 描述符 boolean[] jbooleanArray [Z byte[] jbyteArray [B char[] jcharArray [C short[] jshortArray [S int[] jintArray [I long[] jlongArray [J float[] jfloatArray [F double[] jdoubleArray [D Object[] jobjectArray [LClassName; 其中ClassName为具体的类名，以分号结尾。注意，数组类型的描述符要在前面加上[来表示。例如，int[][]对应的描述符 为[[I。 JNI的使用 JNI的使用大概类似如下图所示： 静态注册案例 1、首先，定义一个Java类，在其中声明native方法： public class MyUtils { static { System.loadLibrary(\"native-lib\"); } public static native int add(int a, int b); public static native String concatenate(String str1, String str2); public static native void arrayTest(int[] arr); public static native int[] sumArrays(int[] arr1, int[] arr2); } 其中，我们定义了四个native方法，分别是： • add()方法，接收两个整型参数并返回它们的和。• concatenate()方法，接收两个字符串并将它们拼接在一起返回。• arrayTest()方法，接收一个整型数组。• sumArrays()方法，接收两个整型数组，将它们对应的元素相加，返回一个新的整型数组。 2、然后，我们需要在C/C++中实现这些方法。下面是一个示例实现： #include \"MyUtils.h\" #include extern \"C\" JNIEXPORT jint JNICALL Java_com_example_MyUtils_add(JNIEnv *env, jclass clazz, jint a, jint b) { return a + b; } extern \"C\" JNIEXPORT jstring JNICALL Java_com_example_MyUtils_concatenate(JNIEnv *env, jclass clazz, jstring str1, jstring str2) { const char *cStr1 = env->GetStringUTFChars(str1, nullptr); const char *cStr2 = env->GetStringUTFChars(str2, nullptr); char buffer[256]; strcpy(buffer, cStr1); strcat(buffer, cStr2); env->ReleaseStringUTFChars(str1, cStr1); env->ReleaseStringUTFChars(str2, cStr2); return env->NewStringUTF(buffer); } extern \"C\" JNIEXPORT void JNICALL Java_com_example_MyUtils_arrayTest(JNIEnv *env, jclass clazz, jintArray arr) { jint *cArr = env->GetIntArrayElements(arr, nullptr); jsize len = env->GetArrayLength(arr); for (int i = 0; i ReleaseIntArrayElements(arr, cArr, JNI_COMMIT); } extern \"C\" JNIEXPORT jintArray JNICALL Java_com_example_MyUtils_sumArrays(JNIEnv *env, jclass clazz, jintArray arr1, jintArray arr2) { jint *cArr1 = env->GetIntArrayElements(arr1, nullptr); jint *cArr2 = env->GetIntArrayElements(arr2, nullptr); jsize len = env->GetArrayLength(arr1); jintArray result = env->NewIntArray(len); jint *cResult = env->GetIntArrayElements(result, nullptr); for (int i = 0; i ReleaseIntArrayElements(arr1, cArr1, JNI_ABORT); env->ReleaseIntArrayElements(arr2, cArr2, JNI_ABORT); env->ReleaseIntArrayElements(result, cResult, JNI_COMMIT); return result; } 在C/C++代码中，每个native方法都需要使用externC进行声明，并以stdcall方式实现。在实现每个方法时，需要根据参数类型在JNI API中进行数据类型转换操作 3、完成上述代码之后，我们需要编译生成动态链接库文件。以Android Studio为例，可以在build.gradle中添加以下配置： android { // ... defaultConfig { // ... externalNativeBuild { cmake { cppFlags \"\" // 指定C/C++文件所在的目录 arguments \"-DCMAKE_CXX_FLAGS=-I${projectDir}/src/main/cpp/include\" } } } // ... externalNativeBuild { cmake { // 指定CMakeLists.txt所在的目录 path \"src/main/cpp/CMakeLists.txt\" } } } 然后，我们在CMakeLists.txt中添加以下内容： cmake_minimum_required(VERSION 3.4.1) # 添加头文件目录 include_directories(${CMAKE_SOURCE_DIR}/include) # 添加源文件 add_library(native-lib SHARED MyUtils.cpp ) # 搜索并链接静态库 find_library(log-lib log) target_link_libraries(native-lib ${log-lib} ) 其中，include_directories指令用于添加头文件目录，add_library指令用于添加源文件，find_library指令用于搜索并 接静态库，target_link_libraries指令用于链接库。 4、在应用程序运行时，加载动态链接库文件时，静态注册的native方法就可以使用了。例如可以通过以下方式调用： // 调用add方法 int result = MyUtils.add(1, 2); // 调用concatenate方法 String s = MyUtils.concatenate(\"hello\", \"world\"); // 调用arrayTest方法 int[] arr = {1, 2, 3}; MyUtils.arrayTest(arr); // 调用sumArrays方法 int[] arr1 = {1, 2, 3}; int[] arr2 = {4, 5, 6}; int[] result = MyUtils.sumArrays(arr1, arr2); 动态注册案例 将上面静态注册案例的第2步改成以下即可： #include \"jni.h\" #include jint my_add(JNIEnv *env, jobject obj, jint a, jint b) { return a + b; } jstring my_concatenate(JNIEnv *env, jobject obj, jstring str1, jstring str2) { const char *cStr1 = env->GetStringUTFChars(str1, nullptr); const char *cStr2 = env->GetStringUTFChars(str2, nullptr); char buffer[256]; strcpy(buffer, cStr1); strcat(buffer, cStr2); env->ReleaseStringUTFChars(str1, cStr1); env->ReleaseStringUTFChars(str2, cStr2); return env->NewStringUTF(buffer); } void my_arrayTest(JNIEnv *env, jobject obj, jintArray arr) { jint *cArr = env->GetIntArrayElements(arr, nullptr); jsize len = env->GetArrayLength(arr); for (int i = 0; i ReleaseIntArrayElements(arr, cArr, JNI_COMMIT); } jintArray my_sumArrays(JNIEnv *env, jobject obj, jintArray arr1, jintArray arr2) { jint *cArr1 = env->GetIntArrayElements(arr1, nullptr); jint *cArr2 = env->GetIntArrayElements(arr2, nullptr); jsize len = env->GetArrayLength(arr1); jintArray result = env->NewIntArray(len); jint *cResult = env->GetIntArrayElements(result, nullptr); for (int i = 0; i ReleaseIntArrayElements(arr1, cArr1, JNI_ABORT); env->ReleaseIntArrayElements(arr2, cArr2, JNI_ABORT); env->ReleaseIntArrayElements(result, cResult, JNI_COMMIT); return result; } // 定义native方法数组 static JNINativeMethod nativeMethods[] = { {\"add\", \"(II)I\", (void *) my_add}, {\"concatenate\", \"(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;\", (void *) my_concatenate}, {\"arrayTest\", \"([I)V\", (void *) my_arrayTest}, {\"sumArrays\", \"([I[I)[I\", (void *) my_sumArrays}, }; // 动态注册native方法 JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM *vm, void *reserved) { JNIEnv *env; if (vm->GetEnv((void **)&env, JNI_VERSION_1_6) != JNI_OK) { return -1; } jclass clazz = env->FindClass(\"com/example/MyUtils\"); env->RegisterNatives(clazz, nativeMethods, sizeof(nativeMethods) / sizeof(nativeMethods[0])); return JNI_VERSION_1_6; } 相比于静态注册，动态注册有以下优点： 1 灵活性更高：动态注册的方式可以让开发者在运行时根据需要动态绑定native方法，灵活性更高。 2 代码更加简洁：相比于静态注册，动态注册不需要在Java类中声明native方法，因此Java代码更加简洁。 3 更好的代码可读性和可维护性：所有native方法的实现都集中在对应的C/C++代码中，代码的可读性和可维护性更高。 4 减少编译时间：相比于静态注册，在动态注册的情况下，每次修改native方法的实现不需要重新编译Java类，仅需要重native的实现即可，可以大大减少编译时间。 Java对象传递给Native 1、Java代码如下所示： public class Person { public String name; public int age; public boolean gender; } 2、C++代码如下所示： struct PersonInfo { char *name; int age; bool gender; }; extern \"C\" JNIEXPORT void JNICALL Java_com_example_MyUtils_processPerson(JNIEnv *env, jclass clazz, jobject obj) { // 将Java对象转换为PersonInfo结构体 jfieldID nameFieldID = env->GetFieldID(clazz, \"name\", \"Ljava/lang/String;\"); jfieldID ageFieldID = env->GetFieldID(clazz, \"age\", \"I\"); jfieldID genderFieldID = env->GetFieldID(clazz, \"gender\", \"Z\"); jstring nameObj = (jstring) env->GetObjectField(obj, nameFieldID); jint age = env->GetIntField(obj, ageFieldID); jboolean gender = env->GetBooleanField(obj, genderFieldID); const char *nameChars = env->GetStringUTFChars(nameObj, nullptr); PersonInfo personInfo; personInfo.name = const_cast(nameChars); personInfo.age = age; personInfo.gender = gender; // ... } 在C++代码中，我们需要使用JNI API来获取Java对象的Field ID，之后通过GetObjectField()、GetIntField()和GetBooleanField()等方法来获取Java对象的内容。最后，我们将获取到的信息填充到C++中的结构体PersonInfo中，即可在native代码中使用该对象。 注意，在实现过程中需要注意使用ReleaseXX()方法释放对Java对象的引用计数。 Native对象传递给Java（Native回调Java） 1 在 Java 层中定义 Native 方法。例如： public native void nativeMethod(); 2 在 Native 层中实现该方法，并通过 JNI 找到对应的 Java 方法。 JNIEXPORT void JNICALL Java_com_example_MyClass_nativeMethod(JNIEnv *env, jobject) { // 处理数据 ` ... // 通过 JNI 找到 Java 方法，并将数据传递回去 jclass cls = env->GetObjectClass(obj); // obj 为 Java 层的对象，可以通过参数传递过来 jmethodID methodID = env->GetMethodID(cls, \"setData\", \"(Ljava/lang/String;)V\"); jstring data = env->NewStringUTF(\"Hello Java\"); env->CallVoidMethod(obj, setMethodID, data); } 多线程中，native如何回调java？ 1 在 Java 层中定义 Native 方法和回调方法。Native 方法使用 synchronized关键字进行同步，保证多线程安全。例如： public class MyClass { public synchronized native void nativeMethod(); // 该方法将被 Native 层回调 public void callback(String data) { // 处理 Native 层返回的数据 } } 2 在 Native 层中实现该方法，并通过 JNI 找到对应的 Java方法，并使用全局引用。全局引用的对象能够在多个线程间共享。 // 定义全局引用 static jclass g_cls = NULL; static jobject g_obj = NULL; static jmethodID g_callbackMethodID = NULL; static JavaVM *g_jvm = NULL; JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM *vm, void *reserved) { JNIEnv *env; if (vm->GetEnv((void **)&env, JNI_VERSION_1_6) != JNI_OK) { return JNI_ERR; } g_jvm = vm; return JNI_VERSION_1_6; } JNIEnv *GetJNIEnv() { JNIEnv *env = NULL; if (g_jvm->GetEnv((void **)&env, JNI_VERSION_1_6) != JNI_OK) { return NULL; } return env; } JNIEXPORT void JNICALL Java_com_example_MyClass_nativeMethod(JNIEnv *env, jobject obj) { // 将 Java 层对象转成全局引用 if (g_cls == NULL) { g_cls = env->GetObjectClass(obj); g_callbackMethodID = env->GetMethodID(g_cls, \"callback\", \"(Ljava/lang/String;)V\"); g_obj = env->NewGlobalRef(obj); } // 处理数据 ... // 回调 Java 方法 std::thread([=](){ JNIEnv *env; if (g_VM->GetEnv((void **)&env, JNI_VERSION_1_6) == JNI_OK) { jstring data = env->NewStringUTF(\"Hello Java\"); env->CallVoidMethod(g_obj, g_callbackMethodID, data); env->DeleteLocalRef(data); } }).detach(); } JNIEXPORT void JNICALL Java_com_example_MyClass_nativeRelease(JNIEnv *env, jobject obj) { // 删除全局引用 env->DeleteGlobalRef(obj); } 这个实现中，还提供了释放全局引用的方法 nativeRelease，以避免内存泄漏。 3 在 Java 层创建对象，并调用 Native 方法。例如： public class Main { public void run() { // 创建 Native 对象，并调用 Native 方法 MyClass obj = new MyClass(); obj.nativeMethod(); } // 该方法将被 Native 层回调 public void callback(String data) { // 处理 Native 层返回的数据 } } 其他重点内容 JNI中JavaVM（重要） 一个进程只有一个 JavaVM。 所有的线程共用一个 JavaVM。 JNIEnv JNIEnv表示Java调用native语言的环境，封装了几乎全部 JNI 方法的指针。 JNIEnv只在创建它的线程生效，不能跨线程传递，不同线程的JNIEnv彼此独立。（重要！） 在 native 环境下创建的线程，要想和 java 通信，即需要获取一个 JNIEnv 对象。如下： 1 GetEnv 函数：在某个线程中获取与 JavaVM 绑定的 JNIEnv 对象，以便进行 Java 对象的操作。2 AttachCurrentThread 函数：将另一个线程附加到 JavaVM 中，并返回与之绑定的 JNIEnv 对象。3 DetachCurrentThread 函数：将一个线程从 JavaVM 中分离并释放与之绑定的 JNIEnv 对象。4 GetJavaVM 函数：从 JNI 掉用中获取 JavaVM 对象。 ```c++ static JavaVM *g_jvm = NULL; JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM vm, void reserved) { if (vm->GetEnv((void **)&g_jni_env, JNI_VERSION_1_6) != JNI_OK) { return JNI_ERR; } g_jvm = vm; return JNI_VERSION_1_6;} JNIEnv GetJNIEnv() { JNIEnv env = NULL; if (g_jvm->GetEnv((void **)&env, JNI_VERSION_1_6) != JNI_OK) { return NULL; } return env;}``` Copyright © xhunmon 2022 all right reserved，powered by GitbookUpdate: 2023-04-05 20:03:04 new Valine({el: \"#vcomments\",appId: 'xMemPde6Lax4ffLfLBkuEi9t-gzGzoHsz',appKey: 'jIVEp2lShgotSTy0GalhgIzm',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "}}